{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bad7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import docker\n",
    "import logging\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635545e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the monitoring results data\n",
    "results = \"/usr/local/bin/results\"\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "start_containers = \"/usr/local/bin/results/started_nextflow_containers.csv\"\n",
    "\n",
    "for root, dirs, files in os.walk(results):\n",
    "    # print(i)\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data = pd.read_csv(file_path, index_col=0)\n",
    "            print(f\"Found CSV file: {file_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ffbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of missing containers: 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check 1: Compare started vs. finished containers\n",
    "fin_df = pd.read_csv(fin_containers, index_col=0)\n",
    "start_df = pd.read_csv(start_containers, index_col=0)\n",
    "missing_containers = []\n",
    "# print(fin_df.columns)\n",
    "for container in fin_df['ContainerID']:\n",
    "    if container not in start_df['ContainerID'].values:\n",
    "        missing_containers.append(container)\n",
    "    # print(f\"Container {container} is present in finished containers.\")\n",
    "if missing_containers:\n",
    "    print(\"The following containers are missing from the started containers list:\")\n",
    "    for container in missing_containers:\n",
    "        print(container)\n",
    "print(\"Amount of missing containers:\",len(missing_containers))\n",
    "\n",
    "# Check 2: Compare finished containers with task cpu data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6eeb86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container nxf-bfJFtRFRw0GIbtJCiIuzsBYd has work directory /storage/nf-core/exec/work/74/d4bd0fff2b47205292a504c29ebe8c\n",
      "Container nxf-8L2S9NgKP0YAIrhQ1YAfxlhT has work directory /storage/nf-core/exec/work/b7/9463339793f7be0528f2908d5cf85c\n",
      "Container nxf-7YmtU0huW8HeHSt09MlA9RgN has work directory /storage/nf-core/exec/work/03/f64cb99f85b92bb2cc40c4188ae895\n",
      "Container nxf-Iy0bbDZ1tASrFdK36P54Y7Ez has work directory /storage/nf-core/exec/work/8c/cb319d859a0b685c21bebbf768a47d\n",
      "Container nxf-I6XrOifaCt8ZJMdhvkzJ0jaw has work directory /storage/nf-core/exec/work/7a/154783a3be78e8480d35b35325b601\n",
      "Container nxf-TRybVxfkjefOE9l8ZAg0A2V0 has work directory /storage/nf-core/exec/work/d6/a640d2e4e94a90e56fab916b0d971b\n",
      "Container nxf-9KChRJSoMLn41QtCOOygsbM4 has work directory /storage/nf-core/exec/work/d1/6e2e5eadbad624f0780ec57b51416b\n",
      "Container nxf-17PZ1CXKNtrXYRnSVuSXIP90 has work directory /storage/nf-core/exec/work/8e/85b15fdfa75fc29216385eca848833\n",
      "Container nxf-FK09qpVhYpNuW4qgv9xCpC0F has work directory /storage/nf-core/exec/work/c3/8cc387399b3063bb0da27427f74091\n",
      "Container nxf-f2WPQYT1LEvLmZ4EGxMhh7Wz has work directory /storage/nf-core/exec/work/92/06975c1f0b9636738dc42ab048cea1\n",
      "Container nxf-v0iqV5IxswF0Crd7UUyWoKiq has work directory /storage/nf-core/exec/work/d9/3c8dacafdac13cdd5d484a07709a44\n",
      "Container nxf-Ksbp2hziAJASNixhbbMCYWS1 has work directory /storage/nf-core/exec/work/5f/0c1ce8c2ce09b33b164ca0943bb067\n",
      "Container nxf-6QduqEH6u2HmDPRd17bIJuGb has work directory /storage/nf-core/exec/work/59/b9b3234b488e13365759786935638e\n",
      "Container nxf-QDM0fqM0okWKkVjLiomzPWVc has work directory /storage/nf-core/exec/work/cc/51ed1b75473388f794257f71f3870b\n",
      "Container nxf-K1nM7TuPLwhNIX4WTlGco0S9 has work directory /storage/nf-core/exec/work/81/0cdf624bca36b95d82b2b18906c097\n",
      "Container nxf-3tUfq48wO38A2QvTo1AOWcZD has work directory /storage/nf-core/exec/work/eb/fdc4e9cd63b2c33ee22e3e046ee536\n",
      "Container nxf-MSx6VKQ7DcVLSKSxV0W3MXvq has work directory /storage/nf-core/exec/work/29/a655232493a34767727ec3bef84d96\n",
      "Container nxf-itFfiFVQTk0OYgjS0INChlES has work directory /storage/nf-core/exec/work/b7/4fca6d5c91de7ef080ecbe35b69c4d\n",
      "Container nxf-xtvLjURTpiVBjHrNlwwcxCxc has work directory /storage/nf-core/exec/work/80/44cafd70309065d65f37d411d5225a\n",
      "Container nxf-g0xdtR6Gqt0awE3Yh9JIaZkz has work directory /storage/nf-core/exec/work/29/ad543a0e568e393ca16a1004fb9ffb\n",
      "Container nxf-zaY0VhtF0LbW6T7isSs6G3Ht has work directory /storage/nf-core/exec/work/2d/7ba6cf3af112bf94e0cb24a8d14ff2\n",
      "Container nxf-45iDdl89i3MbxtlAhTGOfEKe has work directory /storage/nf-core/exec/work/0c/743ccf1afc90cc655c75e8805a4b1f\n"
     ]
    }
   ],
   "source": [
    "# Write container workDirs into dict\n",
    "df = pd.read_csv(fin_containers)\n",
    "container_workdirs = {}\n",
    "# print(df.head())\n",
    "for idx, row in df.iterrows():\n",
    "    container_workdirs[row['Name']] = row['WorkDir']\n",
    "    \n",
    "for name, workdir in container_workdirs.items():\n",
    "    print(f\"Container {name} has work directory {workdir}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract each cAdvisor task in its own file and dataframe\n",
    "# nextflow_pattern = r\"nxf-[A-Za-z0-9]{23}\"\n",
    "# results = \"/usr/local/bin/results\"\n",
    "\n",
    "# for root, dirs, files in os.walk(results):\n",
    "#     if os.path.basename(root) == \"cAdvisor\":\n",
    "#         cAdvisor_path = root\n",
    "#         for metric in os.listdir(cAdvisor_path):\n",
    "#             metric_path = os.path.join(cAdvisor_path, metric)\n",
    "#             if os.path.isdir(metric_path):\n",
    "#                 containers_dir = os.path.join(metric_path, \"containers\")\n",
    "#                 os.makedirs(containers_dir, exist_ok=True)\n",
    "#                 for file in os.listdir(metric_path):\n",
    "#                     if file.endswith(\".csv\"):\n",
    "#                         file_path = os.path.join(metric_path, file)\n",
    "#                         print(f\"Processing file: {file_path}\")\n",
    "#                         df = pd.read_csv(file_path, index_col=0)\n",
    "#                         col = 'instance'\n",
    "#                         for container_name in df[col].unique():\n",
    "#                             if pd.isna(container_name):\n",
    "#                                 continue\n",
    "#                             if re.match(nextflow_pattern, str(container_name)):\n",
    "#                                 container_df = df[df[col] == container_name]\n",
    "#                                 out_path = os.path.join(containers_dir, f\"{container_name}.csv\")\n",
    "#                                 container_df.to_csv(out_path, index=False)\n",
    "#                                 # print(f\"Saved data for {container_name} to {out_path}\")\n",
    "\n",
    "# Add the containers workDir to each time-series entry\n",
    "for root, dirs, files in os.walk(cAdvisor_path):\n",
    "    if os.path.basename(root) == \"containers\":\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                fin_container_df = pd.read_csv(file_path)\n",
    "                container_name = os.path.splitext(file)[0]\n",
    "                if container_name in container_workdirs:\n",
    "                    workdir = container_workdirs[container_name]\n",
    "                    fin_container_df['WorkDir'] = workdir\n",
    "                    fin_container_df.to_csv(file_path, index=False)\n",
    "                    print(f\"Updated {file_path} with work directory {workdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract slurm jobs out of time-series data\n",
    "slurm_metadata_path = \"/usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id\"\n",
    "\n",
    "for file in os.listdir(slurm_metadata_path):\n",
    "    if file.endswith(\"slurm_job_id.csv\"):\n",
    "        file_path = os.path.join(slurm_metadata_path, file)\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        rm_columns = ['num_cpus', 'work_dir','job_name','value', 'instance', 'partition', 'priority', 'run_time',\n",
    "            'slurm_job_pid', 'std_err', 'std_in', 'submit_time', 'threads_per_core', 'user']\n",
    "        df.drop(columns=rm_columns, inplace=True, errors='ignore')\n",
    "        slurm_job_col = 'job_state'\n",
    "        \n",
    "        # print(df.head())\n",
    "        for job_name in df[slurm_job_col].unique():\n",
    "            if pd.isna(job_name):\n",
    "                continue\n",
    "            job_df = df[df[slurm_job_col] == job_name]\n",
    "            out_path = os.path.join(slurm_metadata_path, f\"{job_name}.csv\")\n",
    "            job_df.to_csv(out_path, index=False)\n",
    "            print(f\"Saved data for {job_name} to {out_path}\")\n",
    "\n",
    "for file in os.listdir(slurm_metadata_path):\n",
    "    if file.endswith(\"slurm_job_id.csv\"):\n",
    "        file_path = os.path.join(slurm_metadata_path, file)\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        rm_columns = ['num_cpus', 'work_dir','job_name','value', 'instance', 'partition', 'priority', 'run_time',\n",
    "            'slurm_job_pid', 'std_err', 'std_in', 'submit_time', 'threads_per_core', 'user']\n",
    "        df.drop(columns=rm_columns, inplace=True, errors='ignore')\n",
    "\n",
    "        fin_df = pd.read_csv(fin_containers)\n",
    "        if 'WorkDir' in fin_df.columns and 'num_tasks' in df.columns:\n",
    "            for idx, row in df.iterrows():\n",
    "                work_dir = row['num_tasks']  \n",
    "                slurm_job = row['job_state'] \n",
    "                if pd.isna(work_dir) or pd.isna(slurm_job):\n",
    "                    print(f\"Skipping row {idx} due to missing WorkDir or slurm_job.\")\n",
    "                    continue\n",
    "                # Update fin_df where WorkDir matches\n",
    "                fin_df.loc[fin_df['WorkDir'] == work_dir, 'slurm_job'] = slurm_job\n",
    "\n",
    "            # Write back the updated fin_df\n",
    "            fin_df.to_csv(fin_containers, index=False)\n",
    "            print(f\"Updated {fin_containers} with slurm job info.\")\n",
    "        else:\n",
    "            print(\"WorkDir or num_tasks column missing in DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b07579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series data treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0929880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharecomp-bB4WWry4-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
