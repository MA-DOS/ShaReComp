{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bad7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import shutil \n",
    "import os\n",
    "import docker\n",
    "import logging\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import re\n",
    "import pprint\n",
    "import yaml\n",
    "# Additional stuff for data handling and analysis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Specific libraries for machine learning\n",
    "# Feature extraction and preprocessing\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Dimensionality reduction and embedding\n",
    "from mvlearn.embed import KMCCA\n",
    "# Regression based learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59eeac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configurations.\n",
    "RESULTS_DIR = \"/usr/local/bin/results\"\n",
    "SCOPED_RESULTS_DIR = \"/usr/local/bin/scoped_results\"\n",
    "CONFIG_FILE = \"/usr/local/bin/scoped_results/config.yml\"\n",
    "FIN_CONTAINERS = \"/usr/local/bin/scoped_results/died_nextflow_containers.csv\"\n",
    "START_CONTAINERS = \"/usr/local/bin/scoped_results/started_nextflow_containers.csv\"\n",
    "META_DATA = \"slurm-job-exporter\"\n",
    "DATA_SOURCE = \"ebpf-mon\"\n",
    "POWER_METERING = \"ebpf-mon\"\n",
    "POWER_STATS= \"/usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c334ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: /usr/local/bin/results/died_nextflow_containers.csv\n",
      "Found CSV file: /usr/local/bin/results/started_nextflow_containers.csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/slurm_job_id.csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_MULTIQC.csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2).csv\n",
      "Found CSV file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test).csv\n",
      "Found CSV file: /usr/local/bin/results/task_energy_data/ebpf-mon/container_power/container_power.csv\n",
      "Found CSV file: /usr/local/bin/results/task_energy_data/docker-activity/energyUsage/energyUsage.csv\n",
      "Found CSV file: /usr/local/bin/results/task_cpu_data/ebpf-mon/container_weighted_cycles/container_weighted_cycles.csv\n",
      "Found CSV file: /usr/local/bin/results/task_cpu_data/cAdvisor/container_cpu_user_seconds_total/container_cpu_user_seconds_total.csv\n",
      "Found CSV file: /usr/local/bin/results/task_cpu_data/docker-activity/cpuPercent/cpuPercent.csv\n"
     ]
    }
   ],
   "source": [
    "# Read in the monitoring results data.\n",
    "results = \"/usr/local/bin/results\"\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "start_containers = \"/usr/local/bin/results/started_nextflow_containers.csv\"\n",
    "\n",
    "for root, dirs, files in os.walk(results):\n",
    "    # print(i)\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data = pd.read_csv(file_path, index_col=0)\n",
    "            print(f\"Found CSV file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42dc66e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'identifier': 'name', 'source': 'cAdvisor'},\n",
      " {'identifier': 'name', 'source': 'ebpf-mon'},\n",
      " {'identifier': 'container_name', 'source': 'docker-activity'}]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Include scope results constants here to not read in the data sources that wont be used.\n",
    "# Create a dict to for the data source and its corresponding primary key\n",
    "# Identifier to primary key mapping\n",
    "# name = nxf-container-name (used by cadvisor, ebpf-mon)\n",
    "# container_name = nxf-container-name (used by docker-activity)\n",
    "# workDir = nxf-container-workdir (used by slurm-exporter)\n",
    "\n",
    "def readInResultsConf(config_file):\n",
    "    \"\"\"\n",
    "    Read in the results configuration file and return a dictionary.\n",
    "    \"\"\"\n",
    "    monitoring_config = config_file\n",
    "    with open(monitoring_config, 'r') as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    filtered_sources = []\n",
    "    seen = set()\n",
    "    for target in data['monitoring_targets'].values():\n",
    "        ds = target.get('data_sources')\n",
    "        if ds:\n",
    "            if isinstance(ds, dict):\n",
    "                ds = [ds]\n",
    "            for entry in ds:\n",
    "                filtered = {k: entry[k] for k in ('identifier', 'source') if k in entry}\n",
    "                if (\n",
    "                    'source' in filtered and\n",
    "                    filtered['source'] == 'slurm-job-exporter'\n",
    "                ):\n",
    "                    continue\n",
    "                if 'source' in filtered and 'identifier' in filtered:\n",
    "                    key = (filtered['source'], filtered['identifier'])\n",
    "                    if key not in seen:\n",
    "                        filtered_sources.append(filtered)\n",
    "                        seen.add(key)\n",
    "    pprint.pprint(filtered_sources)\n",
    "    return filtered_sources\n",
    "\n",
    "filtered_sources = readInResultsConf(\"/usr/local/bin/results/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4e68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scoped results directory: /usr/local/bin/scoped_results\n"
     ]
    }
   ],
   "source": [
    "# Set the scope for the results data\n",
    "def resultsScope(results_dir, meta_data, data_source, power_metering):\n",
    "    \"\"\"\n",
    "   Creates a copy of the results directory and returns the cleaned file tree depending on the users scope definition.\n",
    "   Meta data, data source and power metering are mandatory scope definitions.\n",
    "    \"\"\"\n",
    "    scoped_results_dir = shutil.copytree(results_dir, \"/usr/local/bin/scoped_results\", dirs_exist_ok=True)\n",
    "    if data_source == 'all':\n",
    "        print(\"Data source is set to 'all', no filtering will be applied.\")\n",
    "        return scoped_results_dir\n",
    "    for metric in os.listdir(scoped_results_dir):\n",
    "        metric_path = os.path.join(scoped_results_dir, metric)\n",
    "        if not os.path.isdir(metric_path):\n",
    "           continue \n",
    "    # Walk from base dir and rm all dirs that do not match the scope and the power dirs. \n",
    "        for subdir in os.listdir(metric_path):\n",
    "            subdir_path = os.path.join(metric_path, subdir)\n",
    "            subdir_name = os.path.basename(subdir_path)\n",
    "            # print(\"Sub directory name:\", subdir_name)\n",
    "            if os.path.isdir(subdir_path) and subdir_name not in [meta_data, data_source, power_metering]:\n",
    "                shutil.rmtree(subdir_path, ignore_errors=True)\n",
    "    print(\"Successfully scoped results directory:\", scoped_results_dir) \n",
    "    return scoped_results_dir\n",
    "\n",
    "scoped_results = resultsScope(RESULTS_DIR, META_DATA, DATA_SOURCE, POWER_METERING) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee043a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting time series data by data source.\n"
     ]
    }
   ],
   "source": [
    "def split_task_timeseries_by_datasource(results_dir, datasource_identifier_map, nextflow_pattern=r\"nxf-[A-Za-z0-9]{23}\"):\n",
    "    \"\"\"\n",
    "    For each data source in datasource_identifier_map, traverse the results_dir,\n",
    "    and for each metric, split the time series CSVs into per-task files using the correct identifier column.\n",
    "    \"\"\"\n",
    "    for datasource, identifier in datasource_identifier_map.items():\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            if os.path.basename(root) == datasource:\n",
    "                for metric in os.listdir(root):\n",
    "                    metric_path = os.path.join(root, metric)\n",
    "                    if os.path.isdir(metric_path):\n",
    "                        containers_dir = os.path.join(metric_path, \"containers\")\n",
    "                        os.makedirs(containers_dir, exist_ok=True)\n",
    "                        for file in os.listdir(metric_path):\n",
    "                            if file.endswith(\".csv\"):\n",
    "                                file_path = os.path.join(metric_path, file)\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                if identifier not in df.columns:\n",
    "                                    print(f\"Identifier '{identifier}' not found in {file_path}, skipping.\")\n",
    "                                    continue\n",
    "                                for task_name in df[identifier].unique():\n",
    "                                    if pd.isna(task_name):\n",
    "                                        continue\n",
    "                                    if re.match(nextflow_pattern, str(task_name)):\n",
    "                                        task_df = df[df[identifier] == task_name]\n",
    "                                        out_path = os.path.join(containers_dir, f\"{task_name}.csv\")\n",
    "                                        task_df.to_csv(out_path, index=False)\n",
    "                                        # print(f\"Saved data for {task_name} to {out_path}\")\n",
    "    print(\"Finished splitting time series data by data source.\")\n",
    "\n",
    "datasource_identifier_map = {d['source']: d['identifier'] for d in filtered_sources}\n",
    "split_task_timeseries_by_datasource(scoped_results, datasource_identifier_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3f0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- cAdvisor ---\n",
      "Containers in monitored list but NOT in cAdvisor: {'nxf-Qru0CcRgTuUzX8HRaHO9owtd', 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt', 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-9eBqec7AlOZ3GnoFtEtY14ny', 'nxf-l4UOQ6vq023FfdVkhpq6uhFB', 'nxf-2OomksOk8DQ5FccAo70dfVRP', 'nxf-UY2XomSHbY5BM00lkqJ3KiSI', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0', 'nxf-r5ECchowA1ziKKWKd50nqfCS', 'nxf-1AUOV7AhBGVUbCmee5WApTRX', 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV', 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ', 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw', 'nxf-i3k55HVSqlStQlJ9rLveDORE', 'nxf-bQCEmlIiekPOOtkHpYmKBSn7', 'nxf-ChknE6ywWKGXejr2ww4QMS5U', 'nxf-nIl5dATth0K0iWt19eAMXSrN', 'nxf-favxiZeim630IZ9J6BMtGABS'}\n",
      "Count: 23\n",
      "Containers in cAdvisor but NOT in monitored list: set()\n",
      "Count: 0\n",
      "\n",
      "--- ebpf-mon ---\n",
      "Containers in monitored list but NOT in ebpf-mon: {'nxf-Qru0CcRgTuUzX8HRaHO9owtd', 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt', 'nxf-r5ECchowA1ziKKWKd50nqfCS', 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ', 'nxf-9eBqec7AlOZ3GnoFtEtY14ny', 'nxf-2OomksOk8DQ5FccAo70dfVRP', 'nxf-ChknE6ywWKGXejr2ww4QMS5U', 'nxf-nIl5dATth0K0iWt19eAMXSrN', 'nxf-favxiZeim630IZ9J6BMtGABS'}\n",
      "Count: 9\n",
      "Containers in ebpf-mon but NOT in monitored list: set()\n",
      "Count: 0\n",
      "\n",
      "--- docker-activity ---\n",
      "Containers in monitored list but NOT in docker-activity: {'nxf-Qru0CcRgTuUzX8HRaHO9owtd', 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt', 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-9eBqec7AlOZ3GnoFtEtY14ny', 'nxf-l4UOQ6vq023FfdVkhpq6uhFB', 'nxf-2OomksOk8DQ5FccAo70dfVRP', 'nxf-UY2XomSHbY5BM00lkqJ3KiSI', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0', 'nxf-r5ECchowA1ziKKWKd50nqfCS', 'nxf-1AUOV7AhBGVUbCmee5WApTRX', 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV', 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ', 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw', 'nxf-i3k55HVSqlStQlJ9rLveDORE', 'nxf-bQCEmlIiekPOOtkHpYmKBSn7', 'nxf-ChknE6ywWKGXejr2ww4QMS5U', 'nxf-nIl5dATth0K0iWt19eAMXSrN', 'nxf-favxiZeim630IZ9J6BMtGABS'}\n",
      "Count: 23\n",
      "Containers in docker-activity but NOT in monitored list: set()\n",
      "Count: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report_missing_tasks_all_sources(results_dir, datasource_identifier_map, fin_containers_df, container_workdirs, nextflow_pattern=r\"nxf-[A-Za-z0-9]{23}\"):\n",
    "    \"\"\"\n",
    "    For each data source, report how many tasks are missing compared to the finished containers.\n",
    "    \"\"\"\n",
    "    workdir_containers = set(container_workdirs.keys())\n",
    "    for datasource, identifier in datasource_identifier_map.items():\n",
    "        found_containers = set()\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            if os.path.basename(root) == datasource:\n",
    "                for metric in os.listdir(root):\n",
    "                    metric_path = os.path.join(root, metric)\n",
    "                    if os.path.isdir(metric_path):\n",
    "                        for file in os.listdir(metric_path):\n",
    "                            if file.endswith(\".csv\"):\n",
    "                                file_path = os.path.join(metric_path, file)\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                if identifier not in df.columns:\n",
    "                                    continue\n",
    "                                found_containers.update(\n",
    "                                    str(name) for name in df[identifier].unique()\n",
    "                                    if pd.notna(name) and re.match(nextflow_pattern, str(name))\n",
    "                                )\n",
    "        missing_in_source = workdir_containers - found_containers\n",
    "        missing_in_workdirs = found_containers - workdir_containers\n",
    "        print(f\"--- {datasource} ---\")\n",
    "        print(\"Containers in monitored list but NOT in\", datasource + \":\", missing_in_source)\n",
    "        print(\"Count:\", len(missing_in_source))\n",
    "        print(\"Containers in\", datasource, \"but NOT in monitored list:\", missing_in_workdirs)\n",
    "        print(\"Count:\", len(missing_in_workdirs))\n",
    "        print()\n",
    "        \n",
    "datasource_identifier_map = {d['source']: d['identifier'] for d in filtered_sources}\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "fin_containers_df = pd.read_csv(fin_containers)\n",
    "container_workdirs = {row['Name']: row['WorkDir'] for idx, row in fin_containers_df.iterrows()}\n",
    "report_missing_tasks_all_sources(scoped_results, datasource_identifier_map, fin_containers_df, container_workdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebcccbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with work directory /storage/nf-core/exec/work/48/3e276b831989e44e8449921d3e24fa\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with work directory /storage/nf-core/exec/work/94/2bf0ff8f1a2f6a32ee87cae434e8a0\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with work directory /storage/nf-core/exec/work/d4/ff637f253591c16675a74fe80e4927\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with work directory /storage/nf-core/exec/work/01/4f14e8b8126fbac7baccff8e4cae55\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with work directory /storage/nf-core/exec/work/6c/0f4e022e1426c1e3b45d00b68baa08\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with work directory /storage/nf-core/exec/work/1c/7b4be64c0d5e43fa128a5121c12713\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with work directory /storage/nf-core/exec/work/b4/9500f3660050f0de4517c81f1c237e\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with work directory /storage/nf-core/exec/work/c5/b2b3087f571565ee63e9c38d9df2fb\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with work directory /storage/nf-core/exec/work/6b/a7145d39cde6b860115892ce5bdf63\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with work directory /storage/nf-core/exec/work/50/5ebb11cf7cdf7e5678c657b2e95629\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with work directory /storage/nf-core/exec/work/2c/4c0cf1f1b0137da4c7cf62de9a36ef\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with work directory /storage/nf-core/exec/work/f7/2dbec7a19b449f9081f8b853198fd0\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with work directory /storage/nf-core/exec/work/52/e49117568959272d9c35156b02b7e8\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with work directory /storage/nf-core/exec/work/1c/413541c0e7a56746a9f9c69415bb90\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with work directory /storage/nf-core/exec/work/48/3e276b831989e44e8449921d3e24fa\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with work directory /storage/nf-core/exec/work/94/2bf0ff8f1a2f6a32ee87cae434e8a0\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with work directory /storage/nf-core/exec/work/d4/ff637f253591c16675a74fe80e4927\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with work directory /storage/nf-core/exec/work/01/4f14e8b8126fbac7baccff8e4cae55\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with work directory /storage/nf-core/exec/work/6c/0f4e022e1426c1e3b45d00b68baa08\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with work directory /storage/nf-core/exec/work/1c/7b4be64c0d5e43fa128a5121c12713\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with work directory /storage/nf-core/exec/work/b4/9500f3660050f0de4517c81f1c237e\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with work directory /storage/nf-core/exec/work/c5/b2b3087f571565ee63e9c38d9df2fb\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with work directory /storage/nf-core/exec/work/6b/a7145d39cde6b860115892ce5bdf63\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with work directory /storage/nf-core/exec/work/50/5ebb11cf7cdf7e5678c657b2e95629\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with work directory /storage/nf-core/exec/work/2c/4c0cf1f1b0137da4c7cf62de9a36ef\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with work directory /storage/nf-core/exec/work/f7/2dbec7a19b449f9081f8b853198fd0\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with work directory /storage/nf-core/exec/work/52/e49117568959272d9c35156b02b7e8\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with work directory /storage/nf-core/exec/work/1c/413541c0e7a56746a9f9c69415bb90\n"
     ]
    }
   ],
   "source": [
    "def add_workdir_to_all_task_csvs(results_dir, container_workdirs):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct WorkDir from container_workdirs.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    fin_container_df = pd.read_csv(file_path)\n",
    "                    container_name = os.path.splitext(file)[0]\n",
    "                    if container_name in container_workdirs:\n",
    "                        workdir = container_workdirs[container_name]\n",
    "                        fin_container_df['WorkDir'] = workdir\n",
    "                        fin_container_df.to_csv(file_path, index=False)\n",
    "                        print(f\"Updated {file_path} with work directory {workdir}\")\n",
    "\n",
    "add_workdir_to_all_task_csvs(scoped_results, container_workdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c21ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/slurm_job_id.csv\n",
      "Processing job: nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list)\n",
      "Saved data for nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list).csv\n",
      "Processing job: nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome)\n",
      "Saved data for nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_MULTIQC\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_MULTIQC to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_MULTIQC.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Maybe update results path with scoped results path.\n",
    "def extract_slurm_job_metadata(slurm_metadata_path, slurm_job_col=\"job_name\"):\n",
    "    \"\"\"\n",
    "    Extracts slurm job metadata from time-series CSVs and writes each job's data to a separate file.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(slurm_metadata_path):\n",
    "        if file.endswith(\"slurm_job_id.csv\"):\n",
    "            file_path = os.path.join(slurm_metadata_path, file)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            for job_name in df[slurm_job_col].unique():\n",
    "                if pd.isna(job_name):\n",
    "                    continue\n",
    "                print(f\"Processing job: {job_name}\")\n",
    "                job_df = df[df[slurm_job_col] == job_name]\n",
    "                out_path = os.path.join(slurm_metadata_path, f\"{job_name}.csv\")\n",
    "                job_df.to_csv(out_path, index=False)\n",
    "                print(f\"Saved data for {job_name} to {out_path}\")\n",
    "\n",
    "extract_slurm_job_metadata(\"/usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2980f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /usr/local/bin/scoped_results/task_metadata/slurm-job-exporter/slurm_job_id/slurm_job_id.csv\n",
      "Updated /usr/local/bin/scoped_results/died_nextflow_containers.csv with slurm job info.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Maybe update results path with scoped results path.\n",
    "def update_finished_containers_with_nfcore_task(slurm_metadata_path, fin_containers, workdir_col='WorkDir', slurm_workdir_col='work_dir', slurm_job_col='job_name'):\n",
    "    \"\"\"\n",
    "    Update the finished containers file with the nf-core task name (Nextflow) by matching work directories\n",
    "    with slurm job metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    updated = False\n",
    "    for file in os.listdir(slurm_metadata_path):\n",
    "        if file.endswith(\"slurm_job_id.csv\"):\n",
    "            file_path = os.path.join(slurm_metadata_path, file)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            fin_df = pd.read_csv(fin_containers)\n",
    "            if workdir_col in fin_df.columns and slurm_workdir_col in df.columns:\n",
    "                for idx, row in df.iterrows():\n",
    "                    work_dir = row[slurm_workdir_col]\n",
    "                    slurm_job = row[slurm_job_col]\n",
    "                    if pd.isna(work_dir) or pd.isna(slurm_job):\n",
    "                        print(f\"Skipping row {idx} due to missing WorkDir or slurm_job.\")\n",
    "                        continue\n",
    "                    # Update fin_df where WorkDir matches\n",
    "                    fin_df.loc[fin_df[workdir_col] == work_dir, 'Nextflow'] = slurm_job\n",
    "                # Write back the updated fin_df\n",
    "                fin_df.to_csv(fin_containers, index=False)\n",
    "                print(f\"Updated {fin_containers} with slurm job info.\")\n",
    "                updated = True\n",
    "            else:\n",
    "                print(\"WorkDir or job_name column missing in DataFrames.\")\n",
    "    if not updated:\n",
    "        print(\"No updates were made to the finished containers file.\")\n",
    "\n",
    "slurm_metadata_path = os.path.join(scoped_results, \"task_metadata\", \"slurm-job-exporter\", \"slurm_job_id\")\n",
    "update_finished_containers_with_nfcore_task(slurm_metadata_path, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9f3bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with Nextflow value nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with Nextflow value nf-NFCORE_SAREK_SAREK_MULTIQC\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with Nextflow value nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with Nextflow value nf-NFCORE_SAREK_SAREK_MULTIQC\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test)\n"
     ]
    }
   ],
   "source": [
    "def add_nextflow_to_all_task_csvs(results_dir, fin_containers_file, workdir_col='WorkDir', nextflow_col='Nextflow'):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct Nextflow task value from the finished containers file.\n",
    "    \"\"\"\n",
    "    fin_df = pd.read_csv(fin_containers_file)\n",
    "    # Ensure WorkDir is string and stripped in fin_df\n",
    "    fin_df[workdir_col] = fin_df[workdir_col].astype(str).str.strip()\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    container_df = pd.read_csv(file_path)\n",
    "                    if workdir_col in container_df.columns:\n",
    "                        # Ensure WorkDir is string and stripped in container_df\n",
    "                        container_df[workdir_col] = container_df[workdir_col].astype(str).str.strip()\n",
    "                        workdir = container_df[workdir_col].iloc[0]\n",
    "                        match = fin_df[fin_df[workdir_col] == workdir]\n",
    "                        if not match.empty and nextflow_col in match.columns:\n",
    "                            nextflow_value = match[nextflow_col].values[0]\n",
    "                            container_df[nextflow_col] = nextflow_value\n",
    "                            container_df.to_csv(file_path, index=False)\n",
    "                            print(f\"Updated {file_path} with Nextflow value {nextflow_value}\")\n",
    "                        else:\n",
    "                            print(f\"No matching Nextflow value found for WorkDir {workdir} in {file_path}\") \n",
    "\n",
    "add_nextflow_to_all_task_csvs(scoped_results, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0929880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-1AUOV7AhBGVUbCmee5WApTRX.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-i3k55HVSqlStQlJ9rLveDORE.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-8HEIDLPLcFSgNV2onUEea8wK.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv as it does not contain 'value' column.\n",
      "Skipping /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/PEAK_Series_nxf-0X0tQJagkeWOAir2jS124FfK.csv as it does not contain 'value' column.\n",
      "{'nxf-0X0tQJagkeWOAir2jS124FfK': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 25483109423.0,\n",
      "                                                                                        'mean': 192675972059.03488,\n",
      "                                                                                        'peak_value': 198719087576.0,\n",
      "                                                                                        'variance': 1.0222524387866007e+21}}},\n",
      " 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1032268530333.0,\n",
      "                                                                                        'mean': 1032268530333.0,\n",
      "                                                                                        'peak_value': 1032268530333.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 0.0,\n",
      "                                                                                        'mean': 81623193039.92046,\n",
      "                                                                                        'peak_value': 749995124404.0,\n",
      "                                                                                        'variance': 1.6268045250826192e+22}}},\n",
      " 'nxf-1AUOV7AhBGVUbCmee5WApTRX': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 10099893263.0,\n",
      "                                                                                        'mean': 10099893263.0,\n",
      "                                                                                        'peak_value': 10099893263.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-2OomksOk8DQ5FccAo70dfVRP': {'temporal_signatures': {}},\n",
      " 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 100384532.0,\n",
      "                                                                                        'mean': 100384532.0,\n",
      "                                                                                        'peak_value': 100384532.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-8HEIDLPLcFSgNV2onUEea8wK': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 3385406987.0,\n",
      "                                                                                        'mean': 21827950682.107143,\n",
      "                                                                                        'peak_value': 22511007856.0,\n",
      "                                                                                        'variance': 1.2749086616447406e+19}}},\n",
      " 'nxf-9eBqec7AlOZ3GnoFtEtY14ny': {'temporal_signatures': {}},\n",
      " 'nxf-ChknE6ywWKGXejr2ww4QMS5U': {'temporal_signatures': {}},\n",
      " 'nxf-Qru0CcRgTuUzX8HRaHO9owtd': {'temporal_signatures': {}},\n",
      " 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 2498983465.0,\n",
      "                                                                                        'mean': 2057853096370.8164,\n",
      "                                                                                        'peak_value': 2652160935417.0,\n",
      "                                                                                        'variance': 1.147040387264574e+24}}},\n",
      " 'nxf-TrD9qyudd3YDIKNfgNkKpu9H': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1215164914769.0,\n",
      "                                                                                        'mean': 1215164914769.0,\n",
      "                                                                                        'peak_value': 1215164914769.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-UY2XomSHbY5BM00lkqJ3KiSI': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 18318826.0,\n",
      "                                                                                        'mean': 975265508.1348314,\n",
      "                                                                                        'peak_value': 28397848096.0,\n",
      "                                                                                        'variance': 2.6530589336617357e+19}}},\n",
      " 'nxf-bQCEmlIiekPOOtkHpYmKBSn7': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 8992514122.0,\n",
      "                                                                                        'mean': 8992514122.0,\n",
      "                                                                                        'peak_value': 8992514122.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 61006529243.0,\n",
      "                                                                                        'mean': 1220750387110.952,\n",
      "                                                                                        'peak_value': 1264240781781.0,\n",
      "                                                                                        'variance': 5.105281221794935e+22}}},\n",
      " 'nxf-favxiZeim630IZ9J6BMtGABS': {'temporal_signatures': {}},\n",
      " 'nxf-i3k55HVSqlStQlJ9rLveDORE': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 101092567.0,\n",
      "                                                                                        'mean': 101092567.0,\n",
      "                                                                                        'peak_value': 101092567.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-l4UOQ6vq023FfdVkhpq6uhFB': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 69202580.0,\n",
      "                                                                                        'mean': 20647296047.94017,\n",
      "                                                                                        'peak_value': 174880615261.0,\n",
      "                                                                                        'variance': 1.5746105714743446e+21}}},\n",
      " 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ': {'temporal_signatures': {}},\n",
      " 'nxf-nIl5dATth0K0iWt19eAMXSrN': {'temporal_signatures': {}},\n",
      " 'nxf-qDilxwaxmY8uJ5TscM5fDPNc': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1232322940.0,\n",
      "                                                                                        'mean': 10042455993682.168,\n",
      "                                                                                        'peak_value': 10419001881335.0,\n",
      "                                                                                        'variance': 3.8270910104668226e+24}}},\n",
      " 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt': {'temporal_signatures': {}},\n",
      " 'nxf-r5ECchowA1ziKKWKd50nqfCS': {'temporal_signatures': {}}}\n"
     ]
    }
   ],
   "source": [
    "# TODO: If necessary deal with the numerical format of the signatures.\n",
    "def build_container_temporal_signatures_scoped_sources(results_dir, fin_containers_file):\n",
    "    \"\"\"\n",
    "    Build feature vectors for the scoped data sources and metrics by scanning every containers directory\n",
    "    under every metric for every data source. Returns a dictionary of container temporal signatures.\n",
    "    As the power consumption data of the workflow tasks will be used as labels to train models, it will be excluded from the temporal signatures.\n",
    "    Each container will have a 'temporal_signatures' dict with keys like 'source/metric' for every metric from the scoped data source(s).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fin_containers_file)\n",
    "    container_temporal_signatures = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        container_temporal_signatures[row['Name']] = {\n",
    "            'temporal_signatures': {}\n",
    "        }\n",
    "\n",
    "    # Feature vectors\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if \"task_energy_data\" in root.split(os.sep):\n",
    "            continue\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            metric_name = os.path.basename(os.path.dirname(root))\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    ts_container_df = pd.read_csv(file_path)\n",
    "                    ts_container_df['timestamp'] = pd.to_datetime(ts_container_df['timestamp'], unit='ns')\n",
    "                    ts_container_df.set_index('timestamp', inplace=True)\n",
    "                    value_cols = [col for col in ts_container_df.columns if col.startswith('Value')]\n",
    "                    if not value_cols:\n",
    "                        print(f\"Skipping {file_path} as it does not contain 'value' column.\")\n",
    "                        continue\n",
    "                    resource_series = ts_container_df[value_cols[0]]  \n",
    "\n",
    "                    # Feature extraction\n",
    "                    peak_value = resource_series.max()\n",
    "                    lowest_value = resource_series.min()\n",
    "                    mean_value = resource_series.mean()\n",
    "                    median_value = resource_series.median()\n",
    "                    variance = resource_series.var()\n",
    "                    mean_val = resource_series.mean()\n",
    "                    if mean_val == 0:\n",
    "                        relative_variance = 0.0  \n",
    "                    else:\n",
    "                        relative_variance = (resource_series.var() - mean_val**2) / (mean_val**2)\n",
    "                    std_dev = resource_series.std()\n",
    "                    pattern_vector = resource_series.iloc[np.round(np.linspace(0, len(resource_series) - 1, 10)).astype(int)].to_numpy()\n",
    "\n",
    "                    # The server spec can come from the host benchmark in nextflow\n",
    "                    server_spec = {\n",
    "                        'GHz x Cores': \"\",\n",
    "                        'GFlops': \"\",\n",
    "                        'RAM': \"\",\n",
    "                        'IOPS': \"\",\n",
    "                        'Max Network Throughput': \"\",\n",
    "                    }\n",
    "\n",
    "                    feature_vector = { \n",
    "                        'peak_value': peak_value, 'lowest_value': lowest_value, 'mean': mean_value, \n",
    "                        'variance': variance\n",
    "                    }\n",
    "\n",
    "                    # feature_vector = { \n",
    "                    #     'peak_value': peak_value, 'lowest_value': lowest_value, 'mean': mean_value, 'median': median_value, \n",
    "                    #     'variance': variance,'relative_variance': relative_variance, 'std_dev':std_dev, \n",
    "                    #     'pattern_vector': pattern_vector, 'server_spec': server_spec\n",
    "                    # }\n",
    "                    \n",
    "                    container_name = os.path.splitext(file)[0]\n",
    "                    if container_name in container_temporal_signatures:\n",
    "                        if feature_vector is not None and feature_vector != {}:\n",
    "                            # Validation step to account for missing feature values\n",
    "                            expected_keys = ['peak_value', 'lowest_value', 'mean', 'variance']\n",
    "                            missing_values = [key for key in expected_keys if key not in feature_vector or feature_vector[key] is None]\n",
    "                            if missing_values:\n",
    "                                print(f\"Warning: Missing values in feature vector for {container_name} in {metric_name}: {missing_values}\")\n",
    "                            if 'pattern_vector' in feature_vector:\n",
    "                                if not isinstance(feature_vector['pattern_vector'],np.ndarray):\n",
    "                                    print(f\"WARNING: {container_name} {metric_name} pattern_vector shape: {feature_vector['pattern_vector'].shape}\")\n",
    "                            container_temporal_signatures[container_name]['temporal_signatures'][metric_name] = feature_vector\n",
    "    pprint.pprint(container_temporal_signatures)\n",
    "    return container_temporal_signatures\n",
    "\n",
    "container_temporal_signatures = build_container_temporal_signatures_scoped_sources(scoped_results, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a7accfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total containers with no signature for any metric: 9\n",
      "Remaining containers after cleaning: 14\n",
      "All metrics found: ['container_weighted_cycles']\n",
      "Keeping 14 containers with all metrics.\n",
      "{'nxf-0X0tQJagkeWOAir2jS124FfK': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 25483109423.0,\n",
      "                                                                                        'mean': 192675972059.03488,\n",
      "                                                                                        'peak_value': 198719087576.0,\n",
      "                                                                                        'variance': 1.0222524387866007e+21}}},\n",
      " 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1032268530333.0,\n",
      "                                                                                        'mean': 1032268530333.0,\n",
      "                                                                                        'peak_value': 1032268530333.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 0.0,\n",
      "                                                                                        'mean': 81623193039.92046,\n",
      "                                                                                        'peak_value': 749995124404.0,\n",
      "                                                                                        'variance': 1.6268045250826192e+22}}},\n",
      " 'nxf-1AUOV7AhBGVUbCmee5WApTRX': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 10099893263.0,\n",
      "                                                                                        'mean': 10099893263.0,\n",
      "                                                                                        'peak_value': 10099893263.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 100384532.0,\n",
      "                                                                                        'mean': 100384532.0,\n",
      "                                                                                        'peak_value': 100384532.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-8HEIDLPLcFSgNV2onUEea8wK': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 3385406987.0,\n",
      "                                                                                        'mean': 21827950682.107143,\n",
      "                                                                                        'peak_value': 22511007856.0,\n",
      "                                                                                        'variance': 1.2749086616447406e+19}}},\n",
      " 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 2498983465.0,\n",
      "                                                                                        'mean': 2057853096370.8164,\n",
      "                                                                                        'peak_value': 2652160935417.0,\n",
      "                                                                                        'variance': 1.147040387264574e+24}}},\n",
      " 'nxf-TrD9qyudd3YDIKNfgNkKpu9H': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1215164914769.0,\n",
      "                                                                                        'mean': 1215164914769.0,\n",
      "                                                                                        'peak_value': 1215164914769.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-UY2XomSHbY5BM00lkqJ3KiSI': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 18318826.0,\n",
      "                                                                                        'mean': 975265508.1348314,\n",
      "                                                                                        'peak_value': 28397848096.0,\n",
      "                                                                                        'variance': 2.6530589336617357e+19}}},\n",
      " 'nxf-bQCEmlIiekPOOtkHpYmKBSn7': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 8992514122.0,\n",
      "                                                                                        'mean': 8992514122.0,\n",
      "                                                                                        'peak_value': 8992514122.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 61006529243.0,\n",
      "                                                                                        'mean': 1220750387110.952,\n",
      "                                                                                        'peak_value': 1264240781781.0,\n",
      "                                                                                        'variance': 5.105281221794935e+22}}},\n",
      " 'nxf-i3k55HVSqlStQlJ9rLveDORE': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 101092567.0,\n",
      "                                                                                        'mean': 101092567.0,\n",
      "                                                                                        'peak_value': 101092567.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-l4UOQ6vq023FfdVkhpq6uhFB': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 69202580.0,\n",
      "                                                                                        'mean': 20647296047.94017,\n",
      "                                                                                        'peak_value': 174880615261.0,\n",
      "                                                                                        'variance': 1.5746105714743446e+21}}},\n",
      " 'nxf-qDilxwaxmY8uJ5TscM5fDPNc': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1232322940.0,\n",
      "                                                                                        'mean': 10042455993682.168,\n",
      "                                                                                        'peak_value': 10419001881335.0,\n",
      "                                                                                        'variance': 3.8270910104668226e+24}}}}\n"
     ]
    }
   ],
   "source": [
    "def cleanFeatureVectors(container_temporal_signatures):\n",
    "    \"\"\"\n",
    "    Clean the feature vectors by removing containers that have no temporal signatures.\n",
    "    This function modifies the input dictionary in place.\n",
    "    \"\"\"\n",
    "    cleaned_container_temporal_signatures = container_temporal_signatures.copy()\n",
    "    none_counter = 0\n",
    "    to_delete = []\n",
    "    for name, info in cleaned_container_temporal_signatures.items():\n",
    "        if not info['temporal_signatures']:\n",
    "            none_counter += 1\n",
    "            # print(f\"Container {name} has no temporal signatures. Will be deleted.\")\n",
    "            to_delete.append(name)\n",
    "    print(f\"Total containers with no signature for any metric: {none_counter}\")\n",
    "\n",
    "    # Delete containers with no temporal signatures.\n",
    "    for name in to_delete:\n",
    "        del cleaned_container_temporal_signatures[name]\n",
    "    # pprint.pprint(container_temporal_signatures)\n",
    "\n",
    "    print(f\"Remaining containers after cleaning: {len(cleaned_container_temporal_signatures)}\")\n",
    "\n",
    "    all_metrics = set()\n",
    "    for info in cleaned_container_temporal_signatures.values():\n",
    "        all_metrics.update(info['temporal_signatures'].keys())\n",
    "    all_metrics = sorted(all_metrics)\n",
    "    print(f\"All metrics found: {all_metrics}\")\n",
    "\n",
    "    all_feature_names = set()\n",
    "    for info in cleaned_container_temporal_signatures.values():\n",
    "        for metric in info['temporal_signatures'].values():\n",
    "            all_feature_names.update([k for k in metric.keys()])\n",
    "    all_feature_names = sorted(all_feature_names)\n",
    "    \n",
    "    containers_with_all_metrics = []\n",
    "\n",
    "    for container, info in cleaned_container_temporal_signatures.items():\n",
    "        if set(info['temporal_signatures'].keys()) == set(all_metrics):\n",
    "            containers_with_all_metrics.append(container)\n",
    "    print(f\"Keeping {len(containers_with_all_metrics)} containers with all metrics.\")\n",
    "\n",
    "    # pprint.pprint(containers_with_all_metrics)\n",
    "    # pprint.pprint(all_metrics)\n",
    "    # pprint.pprint(all_feature_names)\n",
    "    pprint.pprint(cleaned_container_temporal_signatures)\n",
    "    return cleaned_container_temporal_signatures, containers_with_all_metrics, all_metrics, all_feature_names\n",
    "\n",
    "cleaned_container_temporal_signatures, containers_with_all_metrics, all_metrics, all_feature_names = cleanFeatureVectors(container_temporal_signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93204eb0",
   "metadata": {},
   "source": [
    "### Feature Extraction for Container Metrics\n",
    "\n",
    "#### Given\n",
    "\n",
    "- **Containers**  \n",
    "  `C = {c1, c2, …, cn}`  \n",
    "  *Example:* `nxf-0X0tQJagkeWOAir2jS124FfK`, `nxf-0mUZ0M8vpF30z1CEoXjCQQbH`, …\n",
    "\n",
    "- **Metrics**  \n",
    "  `M = {container_weighted_cycles}`\n",
    "\n",
    "- **Feature Table**\n",
    "\n",
    "| Container Name                | lowest_value        | mean               | peak_value         | variance           |\n",
    "|-------------------------------|---------------------|--------------------|--------------------|--------------------|\n",
    "| nxf-0X0tQJagkeWOAir2jS124FfK  | 2.55 | 1.93 | 1.99 | 1.02  |\n",
    "| nxf-0mUZ0M8vpF30z1CEoXjCQQbH  | 1.03  | 1.03| 1.03  | 0                 |\n",
    "\n",
    "- **Features per Metric**  \n",
    "  `F = {lowest_value, mean, peak_value, variance}`\n",
    "\n",
    "##### Feature Vector\n",
    "\n",
    "For each container `c_i` and metric `m` in `M`, extract:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i =\n",
    "\\begin{bmatrix}\n",
    "\\text{lowest\\_value}(c_i, m) \\\\\n",
    "\\text{mean}(c_i, m) \\\\\n",
    "\\text{peak\\_value}(c_i, m) \\\\\n",
    "\\text{variance}(c_i, m)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Matrix form\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} \\\\\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3} & x_{2,4} \\\\\\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\vdots  \\\\\\\\\n",
    "x_{n,1} & x_{n,2} & x_{n,3} & x_{n,4}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f1147d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14, 4)\n",
      "    lowest_value          mean    peak_value      variance\n",
      "0   6.100653e+10  1.220750e+12  1.264241e+12  5.105281e+22\n",
      "1   3.385407e+09  2.182795e+10  2.251101e+10  1.274909e+19\n",
      "2   2.548311e+10  1.926760e+11  1.987191e+11  1.022252e+21\n",
      "3   1.215165e+12  1.215165e+12  1.215165e+12  0.000000e+00\n",
      "4   1.232323e+09  1.004246e+13  1.041900e+13  3.827091e+24\n",
      "5   2.498983e+09  2.057853e+12  2.652161e+12  1.147040e+24\n",
      "6   1.009989e+10  1.009989e+10  1.009989e+10  0.000000e+00\n",
      "7   1.032269e+12  1.032269e+12  1.032269e+12  0.000000e+00\n",
      "8   0.000000e+00  8.162319e+10  7.499951e+11  1.626805e+22\n",
      "9   1.831883e+07  9.752655e+08  2.839785e+10  2.653059e+19\n",
      "10  8.992514e+09  8.992514e+09  8.992514e+09  0.000000e+00\n",
      "11  1.003845e+08  1.003845e+08  1.003845e+08  0.000000e+00\n",
      "12  1.010926e+08  1.010926e+08  1.010926e+08  0.000000e+00\n",
      "13  6.920258e+07  2.064730e+10  1.748806e+11  1.574611e+21\n"
     ]
    }
   ],
   "source": [
    "def buildFeatureMatriceInput(containers_with_all_metrics, all_metrics, all_feature_names):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the containers with all metrics.\n",
    "    Returns the feature matrix and the container names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Those containers who have all metrics (some are not caught by all exporters)\n",
    "    feature_matrix_x = []\n",
    "    container_names_x = []\n",
    "\n",
    "    for container in containers_with_all_metrics:\n",
    "        info = cleaned_container_temporal_signatures[container]\n",
    "        # pprint.pprint(info['temporal_signatures'])\n",
    "        row = []\n",
    "        for metric in all_metrics:\n",
    "            feats = info['temporal_signatures'].get(metric)\n",
    "            # print(f\"Processing container: {container}, metric: {metric}\")\n",
    "            if feats:\n",
    "                for name in all_feature_names:\n",
    "                    value = feats.get(name)\n",
    "                    if isinstance(value, np.ndarray): # handles the pattern feature being a numpy array\n",
    "                        row.extend(value.tolist())\n",
    "                    else:\n",
    "                        row.append(value)\n",
    "        feature_matrix_x.append(row)\n",
    "        container_names_x.append(container)\n",
    "\n",
    "    # Convert into feature matrix K_x\n",
    "    feature_matrix_x = np.array(feature_matrix_x)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_x.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_x, columns=all_feature_names)\n",
    "    print(df)\n",
    "    return feature_matrix_x, container_names_x\n",
    "\n",
    "feature_matrix_x = buildFeatureMatriceInput(containers_with_all_metrics, all_metrics, all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d11a34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add power values from one chosen data source to all nextflow files for each data source.\n",
    "# First just add the power values to fin_containers.\n",
    "def addPowerToFinContainers(fin_containers, containers_with_all_metrics, power_stats):\n",
    "    \"\"\"\n",
    "    Add power values to the finished containers file.\n",
    "    \"\"\"\n",
    "    fin_df = pd.read_csv(fin_containers)\n",
    "    power_stat_files = set(f[:-4] for f in os.listdir(power_stats) if f.endswith('.csv'))\n",
    "    # print(power_stat_files)\n",
    "\n",
    "    for container in containers_with_all_metrics:\n",
    "        # print(container)\n",
    "        if container in power_stat_files:\n",
    "            power_df = pd.read_csv(os.path.join(power_stats, f\"{container}.csv\"))\n",
    "            # print(power_df.head())\n",
    "            mean_power = power_df['Value (microjoules)'].mean() if 'Value (microjoules)' in power_df.columns else None\n",
    "            fin_df.loc[fin_df['Name'] == container, 'MeanPower'] = mean_power\n",
    "    fin_df.to_csv(fin_containers, index=False)\n",
    "    return fin_df\n",
    "\n",
    "fin_df = addPowerToFinContainers(FIN_CONTAINERS, containers_with_all_metrics, POWER_STATS)\n",
    "# pprint.pprint(fin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72877ef",
   "metadata": {},
   "source": [
    "### Feature Extraction for Container Runtime and Power\n",
    "\n",
    "#### Given\n",
    "\n",
    "- **Containers**  \n",
    "  `C = {c1, c2, …, cn}`  \n",
    "  *Example:* `nxf-0X0tQJagkeWOAir2jS124FfK`, `nxf-0mUZ0M8vpF30z1CEoXjCQQbH`, …\n",
    "\n",
    "- **Metrics**  \n",
    "  `M = {runtime, power}`\n",
    "\n",
    "- **Feature Table**\n",
    "\n",
    "| Container Name                | runtime (s) | power (μJ)      |\n",
    "|-------------------------------|-------------|-----------------|\n",
    "| nxf-0X0tQJagkeWOAir2jS124FfK  | 123.4       | 1.23        |\n",
    "| nxf-0mUZ0M8vpF30z1CEoXjCQQbH  | 98.7        | 2.34       |\n",
    "\n",
    "- **Features per Container**  \n",
    "  `F = {runtime, power}`\n",
    "\n",
    "##### Feature Vector\n",
    "\n",
    "For each container `c_i`, extract:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_i =\n",
    "\\begin{bmatrix}\n",
    "\\text{runtime}(c_i) \\\\\n",
    "\\text{power}(c_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Matrix form\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y_{1,1} & y_{1,2} \\\\\\\\\n",
    "y_{2,1} & y_{2,2} \\\\\\\\\n",
    "\\vdots  & \\vdots  \\\\\\\\\n",
    "y_{n,1} & y_{n,2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each row corresponds to a container, and the columns are:\n",
    "- `runtime`: execution time in seconds\n",
    "- `power`: mean power consumption in microjoules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31ea0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14, 2)\n",
      "      runtime        power\n",
      "0    3.291482   683.055173\n",
      "1    3.624712    12.204298\n",
      "2    4.641757   107.747945\n",
      "3    2.615504   588.790631\n",
      "4    2.757030  1566.518561\n",
      "5   16.219254    80.176704\n",
      "6    1.906432     1.850064\n",
      "7    4.927431   189.087464\n",
      "8    4.828914    21.191534\n",
      "9    6.161708     0.297813\n",
      "10   1.252003     0.776517\n",
      "11   1.344221     0.008668\n",
      "12   1.494906     0.008729\n",
      "13  19.257875     1.580281\n"
     ]
    }
   ],
   "source": [
    "# Build feature output matrix for KCCA model.\n",
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names only for containers with available power values.\n",
    "    \"\"\"\n",
    "    container_runtime_power = {}\n",
    "\n",
    "    fin_df['LifeTime_s'] = (\n",
    "        fin_df['LifeTime']\n",
    "        .str.extract(r'([0-9.]+)(ms|s)', expand=True)\n",
    "        .assign(\n",
    "            value=lambda x: x[0].astype(float),\n",
    "            seconds=lambda x: np.where(x[1] == 'ms', x['value'] / 1000, x['value'])\n",
    "        )['seconds']\n",
    "    )\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        container_runtime_power[row['Name']] = {\n",
    "            'runtime': row['LifeTime_s'],\n",
    "            'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    container_names_y = []\n",
    "\n",
    "    for container, info in container_runtime_power.items():\n",
    "        if container not in cleaned_container_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['runtime']) and pd.notna(info['power']):\n",
    "            feature_matrix_y.append([info['runtime'], info['power']])\n",
    "            container_names_y.append(container)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['runtime', 'power'])\n",
    "    print(df)\n",
    "\n",
    "    return feature_matrix_y, container_names_y\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, containers_with_all_metrics,POWER_STATS) \n",
    "feature_matrix_y = buildFeatureMatriceOutput(finished_containers_dfs_with_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d26ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X names: ['nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc']\n",
      "Y names: ['nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc']\n",
      "Length X: 14\n",
      "Length Y: 14\n",
      "All X in Y: True\n",
      "All Y in X: True\n",
      "Order identical: True\n"
     ]
    }
   ],
   "source": [
    "# Debugging output to check if the container names in X and Y match and order is the same.\n",
    "container_names_x = feature_matrix_x[1]\n",
    "container_names_y = feature_matrix_y[1]\n",
    "print(\"X names:\", container_names_x[:5])\n",
    "print(\"Y names:\", container_names_y[:5])\n",
    "print(\"Length X:\", len(container_names_x))\n",
    "print(\"Length Y:\", len(container_names_y))\n",
    "print(\"All X in Y:\", all(name in container_names_y for name in container_names_x))\n",
    "print(\"All Y in X:\", all(name in container_names_x for name in container_names_y))\n",
    "print(\"Order identical:\", container_names_x == container_names_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bfe3e",
   "metadata": {},
   "source": [
    "#### Z-Score Transformation and Standard Scaling performed on Feature and Label Matrices\n",
    "\n",
    "**Standard scaling** (also known as z-score normalization) is a technique used to standardize the features of a dataset so that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "##### Formula\n",
    "\n",
    "The standard score (z-score) for a value \\( x \\) is calculated as:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the original value,\n",
    "- \\( \\mu \\) is the mean of the training samples,\n",
    "- \\( \\sigma \\) is the standard deviation of the training samples.\n",
    "\n",
    "##### How StandardScaler Works\n",
    "\n",
    "- **Centering**: Subtracts the mean value of each feature so that the feature is centered around zero.\n",
    "- **Scaling**: Divides each centered feature by its standard deviation so that the resulting distribution has unit variance.\n",
    "\n",
    "This transformation is performed **independently for each feature**.\n",
    "- Many machine learning algorithms assume that all features are centered around zero and have the same scale.\n",
    "- Features with larger scales can dominate the objective function and negatively impact model performance.\n",
    "- Standard scaling ensures that each feature contributes equally to the model.\n",
    "- StandardScaler is sensitive to outliers: extreme values can affect the mean and standard deviation, leading to less robust scaling.\n",
    "- For sparse data, one can disable mean centering to preserve sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddfcb6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature matrix X shape: (14, 4)\n",
      "Scaled feature matrix Y shape: (14, 2)\n"
     ]
    }
   ],
   "source": [
    "def scaleFeatureMatrices(feature_matrix_x, feature_matrix_y):\n",
    "    \"\"\"\n",
    "    Scale the feature matrices using StandardScaler.\n",
    "    Returns the scaled feature matrices.\n",
    "    \"\"\"\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    scaled_x = scaler_x.fit_transform(feature_matrix_x)\n",
    "    scaled_y = scaler_y.fit_transform(feature_matrix_y)\n",
    "\n",
    "    print(f\"Scaled feature matrix X shape: {scaled_x.shape}\")\n",
    "    print(f\"Scaled feature matrix Y shape: {scaled_y.shape}\")\n",
    "    \n",
    "    return scaled_x, scaled_y, scaler_x, scaler_y\n",
    "\n",
    "scaled_feature_matrix_x, scaled_feature_matrix_y, scaler_x, scaler_y = scaleFeatureMatrices(feature_matrix_x[0], feature_matrix_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad0382",
   "metadata": {},
   "source": [
    "#### Train-Test Split Procedure\n",
    "\n",
    "To evaluate machine learning models, it is common practice to split the available data into **training** and **testing** subsets. This ensures that model evaluation is performed on data not seen during training, providing an unbiased estimate of model performance.\n",
    "\n",
    "The `train_test_split` function from scikit-learn is a utility that splits arrays or matrices into random train and test subsets. It shuffles and splits the data in a single call.\n",
    "\n",
    "**Parameters:**\n",
    "- **arrays**: Input data to split (e.g., numpy arrays, pandas DataFrames, lists). All arrays must have the same length.\n",
    "- **test_size**: Proportion (float between 0.0 and 1.0) or absolute number (int) of samples to include in the test split. Default is 0.25 if not specified.\n",
    "- **train_size**: Proportion or absolute number of samples to include in the train split. If None, set to the complement of `test_size`.\n",
    "- **random_state**: Controls the shuffling applied to the data before splitting. Setting an integer ensures reproducibility.\n",
    "- **shuffle**: Whether to shuffle the data before splitting (default: True).\n",
    "- **stratify**: If not None, data is split in a stratified fashion using this as class labels.\n",
    "\n",
    "**Returns:**\n",
    "- The function returns lists or arrays containing the train-test split of the inputs, preserving the input type (e.g., numpy array, pandas DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a6f5e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (11, 4), Test set shape: (3, 4)\n",
      "y_train DataFrame:       runtime        power\n",
      "0    1.494906     0.008729\n",
      "1   16.219254    80.176704\n",
      "2    4.828914    21.191534\n",
      "3    4.641757   107.747945\n",
      "4    3.624712    12.204298\n",
      "5   19.257875     1.580281\n",
      "6    2.757030  1566.518561\n",
      "7    4.927431   189.087464\n",
      "8    1.252003     0.776517\n",
      "9    2.615504   588.790631\n",
      "10   1.906432     1.850064\n",
      "y_test DataFrame:     runtime       power\n",
      "0  6.161708    0.297813\n",
      "1  1.344221    0.008668\n",
      "2  3.291482  683.055173\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "# Check the values of the test data set against the kcca predictions before z-score normalization.\n",
    "def splitFeatureMatrices(feature_matrix_x, feature_matrix_y, container_names_x, container_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = train_test_split(\n",
    "        feature_matrix_x, feature_matrix_y, container_names_x, container_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = splitFeatureMatrices(feature_matrix_x[0], feature_matrix_y[0], feature_matrix_x[1], feature_matrix_y[1])\n",
    "\n",
    "x_train_df = pd.DataFrame(X_train, columns=all_feature_names)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"X_train DataFrame:\", x_train_df)\n",
    "x_test_df = pd.DataFrame(X_test, columns=all_feature_names)\n",
    "# print(\"X_test DataFrame:\", x_test_df)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "print(\"y_train DataFrame:\", y_train_df)\n",
    "y_test_df = pd.DataFrame(y_test, columns=['runtime', 'power'])\n",
    "print(\"y_test DataFrame:\", y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1685ad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (11, 4), Test set shape: (3, 4)\n",
      "y_test DataFrame:     runtime     power\n",
      "0  0.160748 -0.542133\n",
      "1 -0.747207 -0.542809\n",
      "2 -0.380205  1.052769\n"
     ]
    }
   ],
   "source": [
    "def splitFeatureMatrices(feature_matrix_x, feature_matrix_y, container_names_x, container_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = train_test_split(\n",
    "        feature_matrix_x, feature_matrix_y, container_names_x, container_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = splitFeatureMatrices(scaled_feature_matrix_x, scaled_feature_matrix_y, feature_matrix_x[1], feature_matrix_y[1])\n",
    "\n",
    "x_train_df = pd.DataFrame(X_train, columns=all_feature_names)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"X_train DataFrame:\", x_train_df)\n",
    "x_test_df = pd.DataFrame(X_test, columns=all_feature_names)\n",
    "# print(\"X_test DataFrame:\", x_test_df)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"y_train DataFrame:\", y_train_df)\n",
    "y_test_df = pd.DataFrame(y_test, columns=['runtime', 'power'])\n",
    "print(\"y_test DataFrame:\", y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3828f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_matrix_x std: 0.9999999999999999\n",
      "feature_matrix_y std: 1.0000000000000002\n",
      "X_train (features) std: 1.107782946169594\n",
      "Y_train (features) std: 1.0772242164070853\n",
      "X_test (features) std: 0.16666988881367265\n",
      "Y_test (features) std: 0.6136376991171003\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "# Check variance of original feature matrices before kernelization\n",
    "print(\"feature_matrix_x std:\", np.std(scaled_feature_matrix_x))\n",
    "print(\"feature_matrix_y std:\", np.std(scaled_feature_matrix_y))\n",
    "\n",
    "# After splitting\n",
    "print(\"X_train (features) std:\", np.std(X_train))\n",
    "print(\"Y_train (features) std:\", np.std(y_train))\n",
    "print(\"X_test (features) std:\", np.std(X_test))\n",
    "print(\"Y_test (features) std:\", np.std(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955469f",
   "metadata": {},
   "source": [
    "### Building the Kernel Matrix for Workflow Tasks\n",
    "\n",
    "We build an $N \\times N$ matrix $K_x$ where the $(i, j)$-th entry is the kernel evaluation $k_x(x_i, x_j)$,  \n",
    "with $x_i$ and $x_j$ being the temporal signatures for tasks $i$ and $j$.\n",
    "\n",
    "- **Each row and column** corresponds to a workflow task.\n",
    "- **Each entry** $K_x[i, j] = k(x_i, x_j)$ measures the similarity between tasks $i$ and $j$ using a kernel function.\n",
    "- We use the **Gaussian (RBF) kernel**, which measures similarity based on the Euclidean distance in feature space, scaled by a parameter $\\sigma$.\n",
    "- This kernel gives higher values when two tasks have similar temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KCCA Model kernalizes the normalized input matrices itself.\n",
    "# def computeKernelMatrices(X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"\n",
    "#     Compute the RBF kernel matrices for train and test splits.\n",
    "#     Returns: K_x_train, K_x_test, K_y_train, K_y_test\n",
    "#     \"\"\"\n",
    "#     K_x_train = rbf_kernel(X_train, X_train)\n",
    "#     K_y_train = rbf_kernel(y_train, y_train)\n",
    "#     K_x_test = rbf_kernel(X_test, X_train)\n",
    "#     K_y_test = rbf_kernel(y_test, y_train)\n",
    "#     print(f\"K_x_train shape: {K_x_train.shape}, K_x_test shape: {K_x_test.shape}\")\n",
    "#     print(f\"K_y_train shape: {K_y_train.shape}, K_y_test shape: {K_y_test.shape}\")\n",
    "#     return K_x_train, K_x_test, K_y_train, K_y_test\n",
    "\n",
    "# K_x_train, K_x_test, K_y_train, K_y_test = computeKernelMatrices(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda0c12",
   "metadata": {},
   "source": [
    "### Kernel Canonical Correlation Analysis (KCCA) Overview\n",
    "\n",
    "The **KCCA algorithm** takes the kernel matrices \\( K_x \\) and \\( K_y \\) and solves a generalized eigenvector problem. This procedure finds subspaces in the linear space spanned by the eigenfunctions of the kernel functions such that projections onto these subspaces are **maximally correlated** [7]. Traditional Canonical Correlation Analysis (CCA) aims to find useful projections of features in each view of data by computing a weighted sum. However, due to its linearity, CCA may not extract meaningful descriptors of complex data.\n",
    "\n",
    "Kernel MCCA (KMCCA) addresses this limitation by first projecting the data into a higher-dimensional feature space **before** performing CCA in that new space.\n",
    "\n",
    "- We refer to these projections as the **resource usage projection** and the **metric projection**, respectively.\n",
    "- If the linear space associated with the Gaussian (RBF) kernel can be interpreted as clusters in the original feature space, then KCCA finds **correlated pairs of clusters** in the resource usage vector space and the performance/power vector space.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Compute kernel matrices** \\( K_x \\) and \\( K_y \\) for the resource and metric features.\n",
    "2. **Fit KCCA** using the training data kernel matrices.\n",
    "3. **Project data** into the maximally correlated subspaces for further analysis or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98851540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMCCA(kernel=&#x27;rbf&#x27;, n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMCCA</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_components',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_components&nbsp;</td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;rbf&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel_params',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel_params&nbsp;</td>\n",
       "            <td class=\"value\">{}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('regs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">regs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('signal_ranks',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">signal_ranks&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sval_thresh',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sval_thresh&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('diag_mode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">diag_mode&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;A&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('center',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">center&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('filter_params',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">filter_params&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multiview_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multiview_output&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pgso',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">pgso&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "KMCCA(kernel='rbf', n_components=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmcca = KMCCA(kernel='rbf', n_components=2)\n",
    "kmcca.fit([X_train, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fabaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project training and test data\n",
    "X_train_proj, Y_train_proj = kmcca.transform([X_train, y_train])\n",
    "X_test_proj, Y_test_proj = kmcca.transform([X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4361f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_proj shape: (11, 2)\n",
      "Y_train_proj shape: (11, 2)\n",
      "X_test_proj shape: (3, 2)\n",
      "Y_test_proj shape: (3, 2)\n",
      "X_train_proj mean/std: 3.658689512969266e-17 0.2132007163556105\n",
      "Y_train_proj mean/std: 2.7755575615628914e-17 0.21320071635561041\n",
      "First 3 rows of X_train_proj:\n",
      " [[-0.02583869 -0.11902035]\n",
      " [-0.31827816  0.57404992]\n",
      " [-0.03416744 -0.07056236]]\n",
      "First 3 rows of Y_train_proj:\n",
      " [[-0.02583869 -0.12117876]\n",
      " [-0.31827816  0.57402738]\n",
      " [-0.03416744 -0.07055146]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the projections\n",
    "print(\"X_train_proj shape:\", X_train_proj.shape)\n",
    "print(\"Y_train_proj shape:\", Y_train_proj.shape)\n",
    "print(\"X_test_proj shape:\", X_test_proj.shape)\n",
    "print(\"Y_test_proj shape:\", Y_test_proj.shape)\n",
    "\n",
    "print(\"X_train_proj mean/std:\", np.mean(X_train_proj), np.std(X_train_proj))\n",
    "print(\"Y_train_proj mean/std:\", np.mean(Y_train_proj), np.std(Y_train_proj))\n",
    "print(\"First 3 rows of X_train_proj:\\n\", X_train_proj[:3])\n",
    "print(\"First 3 rows of Y_train_proj:\\n\", Y_train_proj[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5715e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient between projections: 0.534\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "# Evaluate the correlation between the projections for test data\n",
    "corr, _ = pearsonr(X_test_proj.ravel(), Y_test_proj.ravel())\n",
    "print(f\"Pearson correlation coefficient between projections: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9062b3",
   "metadata": {},
   "source": [
    "### Predicting Power Consumption and Execution Time Using KCCA and Nearest Neighbors\n",
    "\n",
    "The **consumed power** and **execution time** for a one-to-one mapping of clusters to servers can be estimated using a KCCA model trained offline. The process is as follows:\n",
    "\n",
    "1. **Projection into Resource Subspace:**  \n",
    "   The input vector, which includes the temporal signature of the resource usage profile (and optionally server capacity), is projected into the resource subspace learned by KCCA.\n",
    "\n",
    "2. **Finding Nearest Neighbors:**  \n",
    "   For each test sample's resource projection (`X_test_proj`), find its *k* nearest neighbors among the training projections (`X_train_proj`).  \n",
    "   - This is typically done using Euclidean distance in the projected subspace.\n",
    "   - In our implementation, we use `k = 3`.\n",
    "\n",
    "3. **Inferring Metric Projections:**  \n",
    "   For each test sample, collect the metric projections (`Y_train_proj`) of its *k* nearest neighbors.\n",
    "\n",
    "4. **Weighted Sum for Prediction:**  \n",
    "   Compute a weighted sum of these metric projections, where the weight for each neighbor is the inverse of its distance to the test sample (closer neighbors have more influence).\n",
    "\n",
    "5. **Mapping Back to Original Metric Space:**  \n",
    "   The weighted sum gives an estimated metric projection for the test sample.  \n",
    "   - If your metrics were scaled, use the scaler's `inverse_transform` to convert the projection back to the original units (e.g., actual power and time).\n",
    "\n",
    "6. **Selecting the Optimal Point (Optional):**  \n",
    "   The optimal point of this iteration, with the minimum total power consumption, can be recorded for further analysis or scheduling.\n",
    "\n",
    "---\n",
    "- **KCCA** finds maximally correlated subspaces between resource usage and metrics, capturing nonlinear relationships.\n",
    "- By using nearest neighbors in the resource subspace, you leverage the learned relationship to predict metrics for new, unseen resource profiles.\n",
    "- The weighted sum ensures that predictions are more influenced by similar (closer) training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94b508",
   "metadata": {},
   "source": [
    "### Estimating Power and Execution Time via KCCA\n",
    "\n",
    "The consumed power and execution time for the **one-to-one mapping** of clusters to servers can be estimated using a **KCCA model** trained offline.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- The **input vector** — consisting of the **temporal signature** of the resource usage profile and the **server capacity** — is projected into the **resource subspace**.\n",
    "- The corresponding coordinates in the **metric subspace** are inferred using **k-nearest neighbors** (**\\( k = 3 \\)** in our implementation).\n",
    "- The **metric projection** is then mapped back to the original **metrics**:\n",
    "  - **Consumed Power**\n",
    "  - **Execution Time**\n",
    "\n",
    "A **weighted sum** of the metric projections from the \\( k \\) nearest neighbors is computed. The **weight** is defined as the **inverse of the distance** between projections in the subspace.\n",
    "\n",
    "The **optimal point** — the configuration with the **minimum total power consumption** — is recorded for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ba6b0",
   "metadata": {},
   "source": [
    "Then, temporal signature of the new cluster is updated from the consolidated workloads. Such consolidation iterations stop when the clusters cannot be merged anymore since merging will incur significant interference, and/or the degradation in application performance will be intolerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc058598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sample 0:\n",
      "  Neighbor indices: [4 0 8]\n",
      "  Neighbor distances: [0.00108099 0.00128203 0.00455475]\n",
      "  Neighbor actual values (runtime, power):\n",
      "[[  5.16790812 182.20237606]\n",
      " [  5.17170539 180.50309269]\n",
      " [  5.17175615 181.98233757]]\n",
      "\n",
      "Test sample 1:\n",
      "  Neighbor indices: [0 4 8]\n",
      "  Neighbor distances: [2.60579692e-07 8.01806036e-04 3.27796484e-03]\n",
      "  Neighbor actual values (runtime, power):\n",
      "[[  5.17170539 180.50309269]\n",
      " [  5.16790812 182.20237606]\n",
      " [  5.17175615 181.98233757]]\n",
      "\n",
      "Test sample 2:\n",
      "  Neighbor indices: [9 2 7]\n",
      "  Neighbor distances: [0.03455843 0.10857295 0.14554939]\n",
      "  Neighbor actual values (runtime, power):\n",
      "[[  4.94216538 256.07160502]\n",
      " [  5.12751413 202.175993  ]\n",
      " [  5.11518844 184.44314624]]\n",
      "(3, 2)\n",
      "Predicted metrics for test data:\n",
      "    runtime       power\n",
      "0  5.169886  181.488593\n",
      "1  5.171704  180.503788\n",
      "2  5.006494  234.112882\n"
     ]
    }
   ],
   "source": [
    "# Starting point for unseen data used with KCCA model.\n",
    "def predictKCCAUnseen(X_train_proj, Y_train_proj, X_test_proj, scaler_y, k=3):\n",
    "    \"\"\"\n",
    "    Based on the projections of the trained Input features\n",
    "    this func uses the unsupervised nearest neighbours algorithm\n",
    "    to find the nearest points from the unseen points to the training data\n",
    "    points. When the k-nearest neighbours are found a weighted average\n",
    "    denotes the prediction for the unseen data.\n",
    "    This function assumes that the unseen data is already scaled and prepared.\n",
    "    \n",
    "    Args:\n",
    "        X_train_proj: Projected training resource features (n_train, n_components)\n",
    "        Y_train_proj: Projected training metric features (n_train, n_components)\n",
    "        X_test_proj: Projected test resource features (n_test, n_components)\n",
    "        scaler_y: Fitted StandardScaler for the metric space (for inverse_transform)\n",
    "        k: Number of nearest neighbors to use (default: 3)\n",
    "    Returns:\n",
    "        Y_pred: Predicted metrics (runtime, power) in original units for test data (n_test, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "    # Fit the model on the training projections\n",
    "    nn.fit(X_train_proj)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X_test_proj)\n",
    "    \n",
    "    Y_pred_proj = []\n",
    "    for i, (dists, idxs) in enumerate(zip(distances, indices)):\n",
    "        print(f\"\\nTest sample {i}:\")\n",
    "        print(f\"  Neighbor indices: {idxs}\")\n",
    "        print(f\"  Neighbor distances: {dists}\")\n",
    "        actual_neighbor_values = scaler_y.inverse_transform(Y_train_proj[idxs])\n",
    "        print(f\"  Neighbor actual values (runtime, power):\\n{actual_neighbor_values}\")\n",
    "        weights = 1 / (dists + 1e-8)  # Avoid division by zero\n",
    "        weights /= weights.sum()\n",
    "        y_pred = np.average(Y_train_proj[idxs], axis=0, weights=weights)\n",
    "        Y_pred_proj.append(y_pred)\n",
    "    Y_pred_proj = np.array(Y_pred_proj)\n",
    "    Y_pred = scaler_y.inverse_transform(Y_pred_proj)\n",
    "    print(Y_pred.shape)\n",
    "    return Y_pred\n",
    "\n",
    "# Call func on test data projections\n",
    "Y_pred = predictKCCAUnseen(X_train_proj, Y_train_proj, X_test_proj, scaler_y, k=3)\n",
    "df = pd.DataFrame(Y_pred, columns=['runtime', 'power'])\n",
    "print(\"Predicted metrics for test data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4bb2e",
   "metadata": {},
   "source": [
    "#### Prediction Procedure Using k-Nearest Neighbors in Projected Space\n",
    "\n",
    "| Step | Purpose |\n",
    "|------|---------|\n",
    "| `nn.kneighbors(X_test_proj)` | Find \\(k\\) nearest neighbors for each test sample in the projected resource space |\n",
    "| Loop over test samples | For each test sample, perform the following steps: |\n",
    "| `weights = 1 / (dists + 1e-8)` | Compute inverse-distance weights |\n",
    "| `weights /= weights.sum()` | Normalize weights so they sum to 1 |\n",
    "| `np.average(Y_train_proj[idxs], axis=0, weights=weights)` | Compute the weighted average of neighbors' metric projections |\n",
    "| `Y_pred_proj.append(y_pred)` | Collect the prediction for this test sample |\n",
    "| `np.array(Y_pred_proj)` | Stack all predictions into a single matrix |\n",
    "| `scaler_y.inverse_transform(Y_pred_proj)` | Convert predictions back to original metric units |\n",
    "| `return Y_pred` | Output the final predictions |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c2399",
   "metadata": {},
   "source": [
    "Actual test values I think the predictions are off and somewhat similar to each other due to lack of enough training data points.\n",
    "\n",
    "Training set shape: (11, 4), Test set shape: (3, 4)\n",
    "y_test DataFrame:     runtime       power\n",
    "0  6.161708    0.297813\n",
    "1  1.344221    0.008668\n",
    "2  3.291482  683.055173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d21866",
   "metadata": {},
   "source": [
    "### Clustering for Workflow Task Consolidation\n",
    "\n",
    "Our consolidation problem can be viewed as a **clustering problem**. Traditionally, clustering algorithms group similar objects together based on a defined similarity or distance metric. However, in our context, the objective is different:\n",
    "\n",
    "- **Goal:** Group workflow tasks that are **dissimilar** in their resource requirements.\n",
    "- **Rationale:** By consolidating tasks with dissimilar resource usage, we can minimize resource contention and interference, leading to more efficient utilization of system resources.\n",
    "\n",
    "#### Custom Distance Measure\n",
    "\n",
    "To achieve this, we need to define a **distance measure** that captures the **interference** between the resource requirements of workflow tasks. Instead of grouping tasks with similar profiles, our distance metric should:\n",
    "\n",
    "- Assign **larger distances** to pairs of tasks with similar resource usage (to discourage grouping them together).\n",
    "- Assign **smaller distances** to pairs of tasks with complementary or non-overlapping resource usage (to encourage their consolidation).\n",
    "\n",
    "#### Approach\n",
    "\n",
    "1. **Feature Extraction:**  \n",
    "   Extract temporal signatures or resource usage profiles for each workflow task.\n",
    "\n",
    "2. **Distance Metric Design:**  \n",
    "   Design a distance function that reflects the potential for interference. For example, tasks with overlapping peaks in CPU, memory, or I/O usage should have a higher distance.\n",
    "\n",
    "3. **Clustering Algorithm:**  \n",
    "   Apply a clustering algorithm (e.g., k-means, hierarchical clustering, or custom algorithms) using the designed distance metric to group tasks.\n",
    "\n",
    "4. **Consolidation:**  \n",
    "   Assign tasks from different clusters to the same server or resource pool, ensuring that grouped tasks are as dissimilar as possible in their resource demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nextflow_task_peak_series(results_dir):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct Nextflow task value from the finished containers file.\n",
    "    \"\"\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    task_df = pd.read_csv(file_path)\n",
    "                    task_df['timestamp'] = pd.to_datetime(task_df['timestamp'], unit='ns')\n",
    "                    task_df.set_index('timestamp', inplace=True)\n",
    "                    value_cols = [col for col in task_df.columns if col.startswith('Value')]\n",
    "                    if not value_cols:\n",
    "                        # print(f\"Skipping {file_path} as it does not contain 'value' column.\")\n",
    "                        continue\n",
    "                    resource_series = task_df[value_cols[0]]\n",
    "                    # print(f\"Processing {file_path} with resource series: {resource_series.name}\")\n",
    "                    # Compute the peak series\n",
    "                    peak_series = resource_series.resample('1s').max()\n",
    "                    peak_df = peak_series.reset_index()\n",
    "                    # print(peak_series.head())\n",
    "                    peak_df.columns = ['timestamp','peak_value']\n",
    "                    out_file = os.path.join(root, f\"PEAK_Series_{file}\")\n",
    "                    peak_df.to_csv(out_file, index=False)\n",
    "                    print(f\"Updated {file_path} with peak series for distance calculation in {out_file}\")\n",
    "    return scoped_results\n",
    "\n",
    "compute_nextflow_task_peak_series(scoped_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include all affinity scores for all recoreded metrics somehow.\n",
    "# Currently simplified for only one metric.\n",
    "def computeTaskSignatureDistances(scoped_results, cleaned_container_temporal_signatures):\n",
    "    \"\"\"\n",
    "    Compute the distances between task signatures in the feature space.\n",
    "    Returns a distance matrix based on the custom distance function.\n",
    "    \n",
    "    Args:\n",
    "        scoped_results: Result dictionary holding the peak time series for each task's metric.\n",
    "    Returns:\n",
    "        distance_matrix: Numpy array of distances between task signatures.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Read in affinity scores from a file\n",
    "    affinity_scores = {'CpuCpu' : 0.9911813042346189, 'CpuMem': 0.9917353659941142,'FileIOCpu': 0.9919034036414974, 'FileIOFileIO': 0.8044261086909198, 'MemFileIO': 0.9676917390088071, 'MemMem': 0.9976325434871719}\n",
    "    # pprint.pprint(affinity_scores) \n",
    "    # pprint.pprint(cleaned_container_temporal_signatures)\n",
    "    \n",
    "    # Compute pairwise distances between all tasks\n",
    "\n",
    "    # Use the keys of cleaned_container_temporal_signatures as task identifiers\n",
    "    nextflow_jobs = list(cleaned_container_temporal_signatures.keys())\n",
    "\n",
    "    # Initialize the distance matrix\n",
    "    distance_matrix = np.zeros((len(nextflow_jobs), len(nextflow_jobs)))\n",
    "\n",
    "    # pprint.pprint(distance_matrix)\n",
    "    \n",
    "    \n",
    "    for i in range (len(nextflow_jobs)):\n",
    "        for j in range (i + 1, len(nextflow_jobs)):\n",
    "            job_i = nextflow_jobs[i]\n",
    "            job_j = nextflow_jobs[j]\n",
    "\n",
    "            # Apply custom distance calculation\n",
    "            \n",
    "            # Get affinity score for the recorded metrics in the order of the config file\n",
    "            aff_score = affinity_scores.get('CpuCpu')\n",
    "            \n",
    "            # Get the peak time series \n",
    "            peak_i = getPeakTimeSeriesForTask(job_i, scoped_results)\n",
    "            # print(peak_i)\n",
    "            peak_j = getPeakTimeSeriesForTask(job_j, scoped_results)\n",
    "            # print(peak_j)\n",
    "\n",
    "            if peak_i is None or peak_j is None:\n",
    "                print(f\"Skipping distance calculation for {job_i} and {job_j} due to missing peak time series.\")\n",
    "                continue\n",
    "\n",
    "            # Align on timestamp\n",
    "            merged = pd.merge(peak_i, peak_j, on=\"timestamp\", suffixes=('_i', '_j'))\n",
    "            if len(merged) < 2:\n",
    "                print(f\"Not enough overlapping data for {job_i} and {job_j}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Compute and return the correlation coefficient between the peak time series\n",
    "            peak_correlation = pearsonr(merged['peak_value_i'], merged['peak_value_j'])[0]\n",
    "\n",
    "            # Compute the distance based on the correlation coefficient and affinity score\n",
    "            distance = aff_score * (peak_correlation ** 2)\n",
    "            \n",
    "\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "# Helper to get the according peak time series for the current nextflow task.\n",
    "def getPeakTimeSeriesForTask(task_name, scoped_results):\n",
    "    \"\"\"\n",
    "    Get the peak time series for a given task name.\n",
    "    \"\"\"\n",
    "    for root ,dirs, files in os.walk(scoped_results):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            peak_file = os.path.join(root, f\"PEAK_Series_{task_name}.csv\")\n",
    "            if os.path.exists(peak_file):\n",
    "                return pd.read_csv(peak_file)\n",
    "            else:\n",
    "                print(f\"Peak time series file not found for task: {task_name}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "distance_matrix = computeTaskSignatureDistances(scoped_results, cleaned_container_temporal_signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17875ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agglomerative clustering algorithm on the distance matrix\n",
    "# TODO: Calculate the distance threshold based on distribution of the distances in the data.\n",
    "def runAgglomerativeClustering(distance_matrix):\n",
    "    \"\"\"\n",
    "    Run agglomerative clustering on the distance matrix.\n",
    "    Returns the cluster labels for each task.\n",
    "    \n",
    "    Args:\n",
    "        distance_matrix: Numpy array of distances between task signatures.\n",
    "    Returns:\n",
    "        cluster_labels: Numpy array of cluster labels for each task.\n",
    "    \"\"\"\n",
    "    clustering = AgglomerativeClustering(n_clusters = None, metric='precomputed', linkage='average', compute_full_tree=True, compute_distances=False, distance_threshold=5.0).fit(distance_matrix)\n",
    "    cluster_labels = clustering.labels_\n",
    "    print(f\"Number of clusters found: {len(set(cluster_labels))}\")\n",
    "    print(f\"Cluster labels: {cluster_labels}\")\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_labels = runAgglomerativeClustering(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the  complete temporal signature feature vectors of the tasks that are considered for colocation.\n",
    "def updateFeatureVectorOfColocatableTasks(cleaned_container_temporal_signatures, cluster_labels):\n",
    "    \"\"\"\n",
    "    Update the feature vectors of the colocatable tasks by adding the cluster label.\n",
    "    Returns the updated cleaned_container_temporal_signatures.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_container_temporal_signatures: Dictionary of cleaned container temporal signatures.\n",
    "        cluster_labels: Numpy array of cluster labels for each task.\n",
    "    Returns:\n",
    "        updated_cleaned_container_temporal_signatures: Updated dictionary with cluster labels.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update only the time series pattern of the feature vectors of the tasks that are considered for colocation.\n",
    "def updateFeatureVectorOfColocatableTasks(cleaned_container_temporal_signatures, cluster_labels):\n",
    "    \"\"\"\n",
    "    Update the feature vectors of the colocatable tasks by adding the cluster label.\n",
    "    Returns the updated cleaned_container_temporal_signatures.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_container_temporal_signatures: Dictionary of cleaned container temporal signatures.\n",
    "        cluster_labels: Numpy array of cluster labels for each task.\n",
    "    Returns:\n",
    "        updated_cleaned_container_temporal_signatures: Updated dictionary with cluster labels.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995999a",
   "metadata": {},
   "source": [
    "### Random Forest Regressor Modeling\n",
    "#### Some parts of the data processing are repeated here for better understandability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43bbe9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14, 1)\n",
      "      runtime\n",
      "0    3.291482\n",
      "1    3.624712\n",
      "2    4.641757\n",
      "3    2.615504\n",
      "4    2.757030\n",
      "5   16.219254\n",
      "6    1.906432\n",
      "7    4.927431\n",
      "8    4.828914\n",
      "9    6.161708\n",
      "10   1.252003\n",
      "11   1.344221\n",
      "12   1.494906\n",
      "13  19.257875\n"
     ]
    }
   ],
   "source": [
    "# Build feature output matrix on runtime labels for Random Forest Regression model.\n",
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names for the task's runtimes.\n",
    "    \"\"\"\n",
    "    container_runtime_power = {}\n",
    "\n",
    "    fin_df['LifeTime_s'] = (\n",
    "        fin_df['LifeTime']\n",
    "        .str.extract(r'([0-9.]+)(ms|s)', expand=True)\n",
    "        .assign(\n",
    "            value=lambda x: x[0].astype(float),\n",
    "            seconds=lambda x: np.where(x[1] == 'ms', x['value'] / 1000, x['value'])\n",
    "        )['seconds']\n",
    "    )\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        container_runtime_power[row['Name']] = {\n",
    "            'runtime': row['LifeTime_s'],\n",
    "            # 'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    container_names_y = []\n",
    "\n",
    "    for container, info in container_runtime_power.items():\n",
    "        if container not in cleaned_container_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['runtime']):\n",
    "            feature_matrix_y.append([info['runtime']])\n",
    "            container_names_y.append(container)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['runtime'])\n",
    "    print(df)\n",
    "\n",
    "    return feature_matrix_y, container_names_y\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, containers_with_all_metrics,POWER_STATS) \n",
    "reg_runtime_feature_matrix_y = buildFeatureMatriceOutput(finished_containers_dfs_with_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16d7a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14,)\n",
      "          power\n",
      "0    683.055173\n",
      "1     12.204298\n",
      "2    107.747945\n",
      "3    588.790631\n",
      "4   1566.518561\n",
      "5     80.176704\n",
      "6      1.850064\n",
      "7    189.087464\n",
      "8     21.191534\n",
      "9      0.297813\n",
      "10     0.776517\n",
      "11     0.008668\n",
      "12     0.008729\n",
      "13     1.580281\n",
      "(array([6.83055173e+02, 1.22042981e+01, 1.07747945e+02, 5.88790631e+02,\n",
      "       1.56651856e+03, 8.01767037e+01, 1.85006400e+00, 1.89087464e+02,\n",
      "       2.11915336e+01, 2.97812820e-01, 7.76517000e-01, 8.66800000e-03,\n",
      "       8.72900000e-03, 1.58028076e+00]),\n",
      " ['nxf-cPB62cVKMj0A2W3ZgiyXeXAy',\n",
      "  'nxf-8HEIDLPLcFSgNV2onUEea8wK',\n",
      "  'nxf-0X0tQJagkeWOAir2jS124FfK',\n",
      "  'nxf-TrD9qyudd3YDIKNfgNkKpu9H',\n",
      "  'nxf-qDilxwaxmY8uJ5TscM5fDPNc',\n",
      "  'nxf-SX1AWI1RbvjBo0PJOwC1FAFw',\n",
      "  'nxf-1AUOV7AhBGVUbCmee5WApTRX',\n",
      "  'nxf-0mUZ0M8vpF30z1CEoXjCQQbH',\n",
      "  'nxf-0pUrbbt0IplTwbj4uE7h1Lv0',\n",
      "  'nxf-UY2XomSHbY5BM00lkqJ3KiSI',\n",
      "  'nxf-bQCEmlIiekPOOtkHpYmKBSn7',\n",
      "  'nxf-6NsMcpYNvIhqIRPUkkVmSPjV',\n",
      "  'nxf-i3k55HVSqlStQlJ9rLveDORE',\n",
      "  'nxf-l4UOQ6vq023FfdVkhpq6uhFB'])\n"
     ]
    }
   ],
   "source": [
    "# Build feature output matrix on power consumption labels for Random Forest Regression model.\n",
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names for the task's power consumption.\n",
    "    \"\"\"\n",
    "    container_runtime_power = {}\n",
    "\n",
    "    # fin_df['LifeTime_s'] = (\n",
    "    #     fin_df['LifeTime']\n",
    "    #     .str.extract(r'([0-9.]+)(ms|s)', expand=True)\n",
    "    #     .assign(\n",
    "    #         value=lambda x: x[0].astype(float),\n",
    "    #         seconds=lambda x: np.where(x[1] == 'ms', x['value'] / 1000, x['value'])\n",
    "    #     )['seconds']\n",
    "    # )\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        container_runtime_power[row['Name']] = {\n",
    "            # 'runtime': row['LifeTime_s'],\n",
    "            'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    container_names_y = []\n",
    "\n",
    "    for container, info in container_runtime_power.items():\n",
    "        if container not in cleaned_container_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['power']):\n",
    "            feature_matrix_y.append(info['power'])\n",
    "            container_names_y.append(container)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['power'])\n",
    "    print(df)\n",
    "\n",
    "    return feature_matrix_y, container_names_y\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, containers_with_all_metrics,POWER_STATS) \n",
    "reg_power_feature_matrix_y = buildFeatureMatriceOutput(finished_containers_dfs_with_power)\n",
    "pprint.pprint(reg_power_feature_matrix_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5668fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.29148233]\n",
      " [ 3.62471178]\n",
      " [ 4.64175652]\n",
      " [ 2.61550419]\n",
      " [ 2.75703028]\n",
      " [16.21925428]\n",
      " [ 1.9064317 ]\n",
      " [ 4.92743082]\n",
      " [ 4.82891415]\n",
      " [ 6.16170765]\n",
      " [ 1.25200322]\n",
      " [ 1.34422121]\n",
      " [ 1.49490571]\n",
      " [19.25787532]]\n",
      "Scaled feature matrix X shape: (14, 4)\n",
      "Scaled feature matrix Y shape: (14, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale the feature matrices for regression models with runtime output labels.\n",
    "def scaleFeatureMatrices(feature_matrix_x, reg_runtime_feature_matrix_y):\n",
    "    \"\"\"\n",
    "    Scale the feature matrices using StandardScaler.\n",
    "    Returns the scaled feature matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape to 2D array\n",
    "    reg_runtime_y = np.array(reg_runtime_feature_matrix_y)\n",
    "    print(reg_runtime_y)\n",
    "    if reg_runtime_y.ndim == 1:\n",
    "        reg_runtime_y = reg_runtime_y.reshape(-1,1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    scaled_x = scaler_x.fit_transform(feature_matrix_x)\n",
    "    scaled_y = scaler_y.fit_transform(reg_runtime_y)\n",
    "\n",
    "    print(f\"Scaled feature matrix X shape: {scaled_x.shape}\")\n",
    "    print(f\"Scaled feature matrix Y shape: {scaled_y.shape}\")\n",
    "    \n",
    "    return scaled_x, scaled_y, scaler_x, scaler_y\n",
    "\n",
    "scaled_feature_matrix_x, scaled_reg_runtime_feature_matrix_y, scaler_x, reg_runtime_scaler_y = scaleFeatureMatrices(feature_matrix_x[0], reg_runtime_feature_matrix_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.83055173e+02 1.22042981e+01 1.07747945e+02 5.88790631e+02\n",
      " 1.56651856e+03 8.01767037e+01 1.85006400e+00 1.89087464e+02\n",
      " 2.11915336e+01 2.97812820e-01 7.76517000e-01 8.66800000e-03\n",
      " 8.72900000e-03 1.58028076e+00]\n",
      "Scaled feature matrix X shape: (14, 4)\n",
      "Scaled feature matrix Y shape: (14, 1)\n",
      "array([6.83055173e+02, 1.22042981e+01, 1.07747945e+02, 5.88790631e+02,\n",
      "       1.56651856e+03, 8.01767037e+01, 1.85006400e+00, 1.89087464e+02,\n",
      "       2.11915336e+01, 2.97812820e-01, 7.76517000e-01, 8.66800000e-03,\n",
      "       8.72900000e-03, 1.58028076e+00])\n"
     ]
    }
   ],
   "source": [
    "# Scale the feature matrices for regression models with power output labels.\n",
    "def scaleFeatureMatrices(feature_matrix_x, reg_power_feature_matrix_y):\n",
    "    \"\"\"\n",
    "    Scale the feature matrices using StandardScaler.\n",
    "    Returns the scaled feature matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape to 2D array\n",
    "    reg_power_y = np.array(reg_power_feature_matrix_y)\n",
    "    print(reg_power_y)\n",
    "    if reg_power_y.ndim == 1:\n",
    "        reg_power_y = reg_power_y.reshape(-1,1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    scaled_x = scaler_x.fit_transform(feature_matrix_x)\n",
    "    scaled_y = scaler_y.fit_transform(reg_power_y)\n",
    "\n",
    "    print(f\"Scaled feature matrix X shape: {scaled_x.shape}\")\n",
    "    print(f\"Scaled feature matrix Y shape: {scaled_y.shape}\")\n",
    "    \n",
    "    return scaled_x, scaled_y, scaler_x, scaler_y\n",
    "\n",
    "scaled_feature_matrix_x, scaled_reg_power_feature_matrix_y, scaler_x, reg_power_scaler_y = scaleFeatureMatrices(feature_matrix_x[0], reg_power_feature_matrix_y[0])\n",
    "# pprint.pprint(reg_power_feature_matrix_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06257538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (11, 4), Test set shape: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "def splitFeatureMatrices(scaled_feature_matrix_x, scaled_feature_matrix_y, container_names_x, container_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = train_test_split(\n",
    "        scaled_feature_matrix_x, scaled_feature_matrix_y, container_names_x, container_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y\n",
    "\n",
    "X_train, X_test, y_train_power, y_test_power, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = splitFeatureMatrices(scaled_feature_matrix_x, scaled_reg_power_feature_matrix_y, feature_matrix_x[1], feature_matrix_y[1])\n",
    "# pprint.pprint(scaled_reg_power_feature_matrix_y.shape)\n",
    "\n",
    "# x_train_df = pd.DataFrame(X_train, columns=all_feature_names)\n",
    "# y_train_df = pd.DataFrame(y_train, columns=['power'])\n",
    "# print(\"X_train DataFrame:\", x_train_df)\n",
    "# x_test_df = pd.DataFrame(X_test, columns=all_feature_names)\n",
    "# print(\"X_test DataFrame:\", x_test_df)\n",
    "\n",
    "# y_train_df = pd.DataFrame(y_train, columns=['power'])\n",
    "# print(\"y_train DataFrame:\", y_train_df)\n",
    "# y_test_df = pd.DataFrame(y_test, columns=['power'])\n",
    "# print(\"y_test DataFrame:\", y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3698fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (11, 4), Test set shape: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "def splitFeatureMatrices(scaled_feature_matrix_x, scaled_feature_matrix_y, container_names_x, container_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = train_test_split(\n",
    "        scaled_feature_matrix_x, scaled_feature_matrix_y, container_names_x, container_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y\n",
    "\n",
    "X_train, X_test, y_train_runtime, y_test_runtime, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = splitFeatureMatrices(scaled_feature_matrix_x, scaled_reg_runtime_feature_matrix_y, feature_matrix_x[1], feature_matrix_y[1])\n",
    "# pprint.pprint(scaled_reg_power_feature_matrix_y.shape)\n",
    "\n",
    "# x_train_df = pd.DataFrame(X_train, columns=all_feature_names)\n",
    "# y_train_df = pd.DataFrame(y_train, columns=['power'])\n",
    "# print(\"X_train DataFrame:\", x_train_df)\n",
    "# x_test_df = pd.DataFrame(X_test, columns=all_feature_names)\n",
    "# print(\"X_test DataFrame:\", x_test_df)\n",
    "\n",
    "# y_train_df = pd.DataFrame(y_train, columns=['power'])\n",
    "# print(\"y_train DataFrame:\", y_train_df)\n",
    "# y_test_df = pd.DataFrame(y_test, columns=['power'])\n",
    "# print(\"y_test DataFrame:\", y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9afdeafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor to predict the power of colocatable tasks\n",
    "def trainPowerWithRandomForest(X, y):\n",
    "    \"\"\"\n",
    "    Train a Random Forest regressor to predict power consumption based on the feature matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = make_regression(n_features=4, n_informative=2,\n",
    "                        random_state=0, shuffle=False)\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "    regr.fit(X, y)\n",
    "\n",
    "    return regr\n",
    "\n",
    "\n",
    "def predictPowerWithRandomForest(regressor, test_Data):\n",
    "    \"\"\"\n",
    "    Predict the power consumption using the trained Random Forest regressor.\n",
    "    \n",
    "    Args:\n",
    "        regressor: Trained Random Forest regressor.\n",
    "        test_data: Test data for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        Predicted power consumption values.\n",
    "    \"\"\"\n",
    "\n",
    "    return regressor.predict(test_Data)\n",
    "\n",
    "# Fit the model.\n",
    "trainedPredictor = trainPowerWithRandomForest(X_train, y_train_power)\n",
    "# Predict power consumption for the test data.\n",
    "predicted_power = predictPowerWithRandomForest(trainedPredictor, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ff76d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor to predict the runtime of colocatable tasks\n",
    "def trainRuntimeWithRandomForest(X, y):\n",
    "    X, y = make_regression(n_features=4, n_informative=2,\n",
    "                        random_state=0, shuffle=False)\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "    regr.fit(X, y)\n",
    "    return regr\n",
    "    \n",
    "def predictRuntimeWithRandomForest(regressor, test_Data):\n",
    "    \"\"\"\n",
    "    Predict the runtime using the trained Random Forest regressor.\n",
    "    \n",
    "    Args:\n",
    "        regressor: Trained Random Forest regressor.\n",
    "        test_data: Test data for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        Predicted runtime values.\n",
    "    \"\"\"\n",
    "\n",
    "    return regressor.predict(test_Data)\n",
    "    \n",
    "\n",
    "# Fit the model.\n",
    "trainedPredictor = trainRuntimeWithRandomForest(X_train, y_train_runtime)\n",
    "\n",
    "# Predict the runtime for the test data.\n",
    "predicted_runtime = predictRuntimeWithRandomForest(trainedPredictor, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharecomp-bB4WWry4-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
