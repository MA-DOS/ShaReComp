{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import shutil \n",
    "import os\n",
    "import docker\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import yaml\n",
    "import re\n",
    "import pprint\n",
    "import math\n",
    "# Additional stuff for data handling and analysis\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Specific libraries for machine learning\n",
    "# Feature extraction and preprocessing\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "# Clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Dimensionality reduction and embedding\n",
    "from mvlearn.embed import KMCCA\n",
    "from cca_zoo.nonparametric import KCCA\n",
    "from cca_zoo.linear import CCA, MCCA\n",
    "from sklearn import decomposition\n",
    "from cca_zoo.preprocessing import MultiViewPreprocessing\n",
    "from cca_zoo.model_selection import cross_validate\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from cca_zoo.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Regression based learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4dfca3",
   "metadata": {},
   "source": [
    "#### [Processing Pipeline]\n",
    "##### Read in CSV Data and format CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configurations.\n",
    "RESULTS_DIR = \"/usr/local/bin/results\"\n",
    "SCOPED_RESULTS_DIR = \"./scoped_results\"\n",
    "CONFIG_FILE = \"/usr/local/bin/scoped_results/config.yml\"\n",
    "FIN_CONTAINERS = \"./scoped_results/died_nextflow_containers.csv\"\n",
    "START_CONTAINERS = \"/usr/local/bin/scoped_results/started_nextflow_containers.csv\"\n",
    "META_DATA = \"slurm-job-exporter\"\n",
    "DATA_SOURCE = \"all\"\n",
    "POWER_METERING = \"ebpf-mon\"\n",
    "POWER_STATS= \"./scoped_results/task_energy_data/ebpf-mon/container_power/containers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the monitoring results data.\n",
    "results = \"/usr/local/bin/results\"\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "start_containers = \"/usr/local/bin/results/started_nextflow_containers.csv\"\n",
    "\n",
    "for root, dirs, files in os.walk(results):\n",
    "    # print(i)\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data = pd.read_csv(file_path, index_col=0)\n",
    "            # print(f\"Found CSV file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInResultsConf(config_file):\n",
    "    \"\"\"\n",
    "    Read in the results configuration file and return a dictionary.\n",
    "    \"\"\"\n",
    "    monitoring_config = config_file\n",
    "    with open(monitoring_config, 'r') as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    filtered_sources = []\n",
    "    seen = set()\n",
    "    for target in data['monitoring_targets'].values():\n",
    "        ds = target.get('data_sources')\n",
    "        if ds:\n",
    "            if isinstance(ds, dict):\n",
    "                ds = [ds]\n",
    "            for entry in ds:\n",
    "                filtered = {k: entry[k] for k in ('identifier', 'source') if k in entry}\n",
    "                if (\n",
    "                    'source' in filtered and\n",
    "                    filtered['source'] == 'slurm-job-exporter'\n",
    "                ):\n",
    "                    continue\n",
    "                if 'source' in filtered and 'identifier' in filtered:\n",
    "                    key = (filtered['source'], filtered['identifier'])\n",
    "                    if key not in seen:\n",
    "                        filtered_sources.append(filtered)\n",
    "                        seen.add(key)\n",
    "    pprint.pprint(filtered_sources)\n",
    "    return filtered_sources\n",
    "\n",
    "filtered_sources = readInResultsConf(\"/usr/local/bin/results/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the scope for the results data\n",
    "def resultsScope(results_dir, meta_data, data_source, power_metering):\n",
    "    \"\"\"\n",
    "   Creates a copy of the results directory and returns the cleaned file tree depending on the users scope definition.\n",
    "   Meta data, data source and power metering are mandatory scope definitions.\n",
    "    \"\"\"\n",
    "    # scoped_results_dir = shutil.copytree(results_dir, \"/usr/local/bin/scoped_results\", dirs_exist_ok=True)\n",
    "    scoped_results_dir = shutil.copytree(results_dir, \"./scoped_results\", dirs_exist_ok=True)\n",
    "    if data_source == 'all':\n",
    "        print(\"Data source is set to 'all', no filtering will be applied.\")\n",
    "        return scoped_results_dir\n",
    "    for metric in os.listdir(scoped_results_dir):\n",
    "        metric_path = os.path.join(scoped_results_dir, metric)\n",
    "        if not os.path.isdir(metric_path):\n",
    "           continue \n",
    "    # Decide which subdir to keep for this metric\n",
    "        if metric == \"task_metadata\":\n",
    "            keep = [meta_data]\n",
    "        elif metric == \"task_energy_data\":\n",
    "            keep = [power_metering]\n",
    "        else:\n",
    "            keep = [data_source]\n",
    "    # Walk from base dir and rm all dirs that do not match the scope and the power dirs. \n",
    "        for subdir in os.listdir(metric_path):\n",
    "            subdir_path = os.path.join(metric_path, subdir)\n",
    "            if os.path.isdir(subdir_path) and subdir not in keep:\n",
    "                shutil.rmtree(subdir_path, ignore_errors=True)\n",
    "    print(\"Successfully scoped results directory:\", scoped_results_dir)\n",
    "    return scoped_results_dir\n",
    "    #         subdir_name = os.path.basename(subdir_path)\n",
    "    #         # print(\"Sub directory name:\", subdir_name)\n",
    "    #         if os.path.isdir(subdir_path) and subdir_name not in [meta_data, data_source, power_metering]:\n",
    "    #             shutil.rmtree(subdir_path, ignore_errors=True)\n",
    "    # print(\"Successfully scoped results directory:\", scoped_results_dir) \n",
    "    # return scoped_results_dir\n",
    "\n",
    "scoped_results = resultsScope(RESULTS_DIR, META_DATA, DATA_SOURCE, POWER_METERING) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee043a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_task_timeseries_by_datasource(results_dir, datasource_identifier_map, nextflow_pattern=r\"nxf-[A-Za-z0-9]{23}\"):\n",
    "    \"\"\"\n",
    "    For each data source in datasource_identifier_map, traverse the results_dir,\n",
    "    and for each metric, split the time series CSVs into per-task files using the correct identifier column.\n",
    "    \"\"\"\n",
    "    for datasource, identifier in datasource_identifier_map.items():\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            if os.path.basename(root) == datasource:\n",
    "                for metric in os.listdir(root):\n",
    "                    metric_path = os.path.join(root, metric)\n",
    "                    if os.path.isdir(metric_path):\n",
    "                        containers_dir = os.path.join(metric_path, \"containers\")\n",
    "                        os.makedirs(containers_dir, exist_ok=True)\n",
    "                        for file in os.listdir(metric_path):\n",
    "                            if file.endswith(\".csv\"):\n",
    "                                file_path = os.path.join(metric_path, file)\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                if identifier not in df.columns:\n",
    "                                    print(f\"Identifier '{identifier}' not found in {file_path}, skipping.\")\n",
    "                                    continue\n",
    "                                for task_name in df[identifier].unique():\n",
    "                                    if pd.isna(task_name):\n",
    "                                        continue\n",
    "                                    if re.match(nextflow_pattern, str(task_name)):\n",
    "                                        task_df = df[df[identifier] == task_name]\n",
    "                                        out_path = os.path.join(containers_dir, f\"{task_name}.csv\")\n",
    "                                        task_df.to_csv(out_path, index=False)\n",
    "                                        # print(f\"Saved data for {task_name} to {out_path}\")\n",
    "    print(\"Finished splitting time series data by data source.\")\n",
    "\n",
    "datasource_identifier_map = {d['source']: d['identifier'] for d in filtered_sources}\n",
    "split_task_timeseries_by_datasource(scoped_results, datasource_identifier_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_missing_tasks_all_sources(results_dir, datasource_identifier_map, fin_containers_df, container_workdirs, nextflow_pattern=r\"nxf-[A-Za-z0-9]{23}\"):\n",
    "    \"\"\"\n",
    "    For each data source, report how many tasks are missing compared to the finished containers.\n",
    "    \"\"\"\n",
    "    workdir_containers = set(container_workdirs.keys())\n",
    "    for datasource, identifier in datasource_identifier_map.items():\n",
    "        found_containers = set()\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            if os.path.basename(root) == datasource:\n",
    "                for metric in os.listdir(root):\n",
    "                    metric_path = os.path.join(root, metric)\n",
    "                    if os.path.isdir(metric_path):\n",
    "                        for file in os.listdir(metric_path):\n",
    "                            if file.endswith(\".csv\"):\n",
    "                                file_path = os.path.join(metric_path, file)\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                if identifier not in df.columns:\n",
    "                                    continue\n",
    "                                found_containers.update(\n",
    "                                    str(name) for name in df[identifier].unique()\n",
    "                                    if pd.notna(name) and re.match(nextflow_pattern, str(name))\n",
    "                                )\n",
    "        missing_in_source = workdir_containers - found_containers\n",
    "        missing_in_workdirs = found_containers - workdir_containers\n",
    "        print(f\"--- {datasource} ---\")\n",
    "        print(\"Containers in monitored list but NOT in\", datasource + \":\", missing_in_source)\n",
    "        print(\"Count:\", len(missing_in_source))\n",
    "        print(\"Containers in\", datasource, \"but NOT in monitored list:\", missing_in_workdirs)\n",
    "        print(\"Count:\", len(missing_in_workdirs))\n",
    "        print()\n",
    "        \n",
    "datasource_identifier_map = {d['source']: d['identifier'] for d in filtered_sources}\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "fin_containers_df = pd.read_csv(fin_containers)\n",
    "container_workdirs = {row['Name']: row['WorkDir'] for idx, row in fin_containers_df.iterrows()}\n",
    "report_missing_tasks_all_sources(scoped_results, datasource_identifier_map, fin_containers_df, container_workdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_workdir_to_all_task_csvs(results_dir, container_workdirs):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct WorkDir from container_workdirs.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    fin_container_df = pd.read_csv(file_path)\n",
    "                    container_name = os.path.splitext(file)[0]\n",
    "                    if container_name in container_workdirs:\n",
    "                        workdir = container_workdirs[container_name]\n",
    "                        fin_container_df['WorkDir'] = workdir\n",
    "                        fin_container_df.to_csv(file_path, index=False)\n",
    "                        # print(f\"Updated {file_path} with work directory {workdir}\")\n",
    "\n",
    "add_workdir_to_all_task_csvs(scoped_results, container_workdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slurm_job_metadata(slurm_metadata_path, slurm_job_col=\"job_name\"):\n",
    "    \"\"\"\n",
    "    Extracts slurm job metadata from time-series CSVs and writes each job's data to a separate file.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(slurm_metadata_path):\n",
    "        if file.endswith(\"slurm_job_id.csv\"):\n",
    "            file_path = os.path.join(slurm_metadata_path, file)\n",
    "            # print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            for job_name in df[slurm_job_col].unique():\n",
    "                if pd.isna(job_name):\n",
    "                    continue\n",
    "                # print(f\"Processing job: {job_name}\")\n",
    "                job_df = df[df[slurm_job_col] == job_name]\n",
    "                out_path = os.path.join(slurm_metadata_path, f\"{job_name}.csv\")\n",
    "                job_df.to_csv(out_path, index=False)\n",
    "                # print(f\"Saved data for {job_name} to {out_path}\")\n",
    "\n",
    "extract_slurm_job_metadata(\"/usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2980f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_finished_containers_with_nfcore_task(slurm_metadata_path, fin_containers, workdir_col='WorkDir', slurm_workdir_col='work_dir', slurm_job_col='job_name'):\n",
    "    \"\"\"\n",
    "    Update the finished containers file with the nf-core task name (Nextflow) by matching work directories\n",
    "    with slurm job metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    updated = False\n",
    "    for file in os.listdir(slurm_metadata_path):\n",
    "        if file.endswith(\"slurm_job_id.csv\"):\n",
    "            file_path = os.path.join(slurm_metadata_path, file)\n",
    "            # print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            fin_df = pd.read_csv(fin_containers)\n",
    "            if workdir_col in fin_df.columns and slurm_workdir_col in df.columns:\n",
    "                for idx, row in df.iterrows():\n",
    "                    work_dir = row[slurm_workdir_col]\n",
    "                    slurm_job = row[slurm_job_col]\n",
    "                    if pd.isna(work_dir) or pd.isna(slurm_job):\n",
    "                        print(f\"Skipping row {idx} due to missing WorkDir or slurm_job.\")\n",
    "                        continue\n",
    "                    # Update fin_df where WorkDir matches\n",
    "                    fin_df.loc[fin_df[workdir_col] == work_dir, 'Nextflow'] = slurm_job\n",
    "                # Write back the updated fin_df\n",
    "                fin_df.to_csv(fin_containers, index=False)\n",
    "                print(f\"Updated {fin_containers} with slurm job info.\")\n",
    "                updated = True\n",
    "            else:\n",
    "                print(\"WorkDir or job_name column missing in DataFrames.\")\n",
    "    if not updated:\n",
    "        print(\"No updates were made to the finished containers file.\")\n",
    "\n",
    "slurm_metadata_path = os.path.join(scoped_results, \"task_metadata\", \"slurm-job-exporter\", \"slurm_job_id\")\n",
    "update_finished_containers_with_nfcore_task(slurm_metadata_path, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nextflow_to_all_task_csvs(results_dir, fin_containers_file, workdir_col='WorkDir', nextflow_col='Nextflow'):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct Nextflow task value from the finished containers file.\n",
    "    \"\"\"\n",
    "    fin_df = pd.read_csv(fin_containers_file)\n",
    "    # Ensure WorkDir is string and stripped in fin_df\n",
    "    fin_df[workdir_col] = fin_df[workdir_col].astype(str).str.strip()\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    container_df = pd.read_csv(file_path)\n",
    "                    if workdir_col in container_df.columns:\n",
    "                        # Ensure WorkDir is string and stripped in container_df\n",
    "                        container_df[workdir_col] = container_df[workdir_col].astype(str).str.strip()\n",
    "                        workdir = container_df[workdir_col].iloc[0]\n",
    "                        match = fin_df[fin_df[workdir_col] == workdir]\n",
    "                        if not match.empty and nextflow_col in match.columns:\n",
    "                            nextflow_value = match[nextflow_col].values[0]\n",
    "                            container_df[nextflow_col] = nextflow_value\n",
    "                            container_df.to_csv(file_path, index=False)\n",
    "                            # print(f\"Updated {file_path} with Nextflow value {nextflow_value}\")\n",
    "                        else:\n",
    "                            print(f\"No matching Nextflow value found for WorkDir {workdir} in {file_path}\") \n",
    "\n",
    "add_nextflow_to_all_task_csvs(scoped_results, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394daa49",
   "metadata": {},
   "source": [
    "##### Feature Extraction\n",
    "##### Build feature vectors for every observed task.\n",
    "Use the raw time-series, smooth the data and and for equal length vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe0053",
   "metadata": {},
   "source": [
    "In the paper, the “Pattern” feature is defined as follows: the continuous resource usage signal x(t) is sampled at fixed 1-second intervals to obtain a discrete sequence \\{x_n\\}. These sampled values are then partitioned into ten groups such that similar values fall into the same group. For each group, the mean is calculated, and the resulting ten averages form the pattern vector. This representation preserves the distributional characteristics of the original time series while discarding fine temporal ordering, and the choice of ten groups balances fidelity with computational simplicity for KCCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0729358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_container_temporal_signatures_scoped_sources(results_dir, fin_containers_file):\n",
    "    \"\"\"\n",
    "    Build feature vectors for the scoped data sources and metrics by scanning every containers directory\n",
    "    under every metric for every data source. Returns a dictionary of container temporal signatures.\n",
    "    As the power consumption data of the workflow tasks will be used as labels to train models, it will be excluded from the temporal signatures.\n",
    "    Each container will have a 'temporal_signatures' dict with keys like 'source/metric' for every metric from the scoped data source(s).\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(fin_containers_file)\n",
    "    pattern_temporal_signatures = {}\n",
    "        \n",
    "    for idx, row in df.iterrows():\n",
    "        if  row['Nextflow'] != '':\n",
    "            pattern_temporal_signatures[row['Nextflow']] = {\n",
    "                'temporal_signatures': {}\n",
    "            }\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Feature vectors\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if \"task_energy_data\" in root.split(os.sep):\n",
    "            continue\n",
    "        if \"task_\" in os.path.basename(root):\n",
    "            workload_name = os.path.basename(root)\n",
    "            print(\"Processing workload data:\", workload_name)\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            metric_name = os.path.basename(os.path.dirname(root))\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    ts_container_df = pd.read_csv(file_path)\n",
    "                    if 'Nextflow' in ts_container_df.columns:\n",
    "                        task_name = ts_container_df['Nextflow'].iloc[0]\n",
    "                        if task_name is not None and pd.isna(task_name):\n",
    "                            # print(f\"Nextflow task name missing in {file_path}, skipping.\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        # print(f\"'Nextflow' column missing in {file_path}, skipping.\")\n",
    "                        continue\n",
    "                    # print(f\"Processing task and file :\", {task_name}, {file})\n",
    "                    ts_container_df['timestamp'] = pd.to_datetime(ts_container_df['timestamp'], unit='ns')\n",
    "                    ts_container_df.set_index('timestamp', inplace=True)\n",
    "                    value_cols = [col for col in ts_container_df.columns if col.startswith('Value')]\n",
    "                    if not value_cols:\n",
    "                        continue\n",
    "                    resource_series = ts_container_df[value_cols[0]]  \n",
    "\n",
    "                    # Feature extraction\n",
    "\n",
    "                    # This only takes 10 evenly spaced samples from the time series as a simple pattern representation.\n",
    "                    pattern_vector = resource_series.iloc[np.round(np.linspace(0, len(resource_series) - 1, 10)).astype(int)].to_numpy()\n",
    "\n",
    "                    # replace each point by the Gaussian-weighted mean of the surrounding 6 samples (≈3 s window, std=2 points), drops the initial NaNs, and outputs the resulting smoothed values\n",
    "                    # window=6 sets the smoothing scale to 3 s at 500 ms sampling, while std=2 makes the Gaussian weight concentrate on the central few points but still include the full window, yielding features that emphasize short-term local patterns without being dominated by noise\n",
    "                    # pattern_vector = resource_series.rolling(window=6, win_type='gaussian').mean(std=2).dropna().to_numpy()\n",
    "\n",
    "                    # # PPA \n",
    "                    # n_segments = 10  # Define the number of segments\n",
    "                    # segment_size = len(resource_series) // n_segments\n",
    "\n",
    "                    # Truncate the series to make it divisible by the number of segments\n",
    "                    # truncated_series = resource_series[:segment_size * n_segments]\n",
    "\n",
    "                    # Reshape the series into segments and calculate the mean of each segment\n",
    "                    # segment_vector = truncated_series.values.reshape(n_segments, segment_size).mean(axis=1)\n",
    "                    \n",
    "\n",
    "                    # Truncate the series or pad them to fixed length if intra-task lenght variability is too high\n",
    "                    # pattern_vector = np.pad(pattern_vector, (0, max(0, max_length - len(pattern_vector))), mode='constant')\n",
    "\n",
    "                    # server_spec = {\n",
    "                    #     'GHz x Cores': \"\",\n",
    "                    #     'GFlops': \"\",\n",
    "                    #     'RAM': \"\",\n",
    "                    #     'IOPS': \"\",\n",
    "                    #     'Max Network Throughput': \"\",\n",
    "                    # }\n",
    "                    \n",
    "                    feature_vector = { \n",
    "                        'pattern' : pattern_vector\n",
    "                    }\n",
    "\n",
    "                    # container_name = os.path.splitext(file)[0]\n",
    "                    if task_name in pattern_temporal_signatures:\n",
    "                        # Validation step to account for missing feature values\n",
    "                        if feature_vector is not None and feature_vector != {}:\n",
    "                            expected_keys = ['pattern']\n",
    "                            missing_values = [key for key in expected_keys if key not in feature_vector or feature_vector[key] is None]\n",
    "                            if missing_values:\n",
    "                                print(f\"Warning: Missing values in feature vector for {metric_name}: {missing_values}\")\n",
    "                            if 'pattern_vector' in feature_vector:\n",
    "                                if not isinstance(feature_vector['pattern_vector'],np.ndarray):\n",
    "                                    print(f\"WARNING: {metric_name} pattern_vector shape: {feature_vector['pattern_vector'].shape}\")\n",
    "                            if workload_name not in pattern_temporal_signatures[task_name]['temporal_signatures']:\n",
    "                                pattern_temporal_signatures[task_name]['temporal_signatures'][workload_name] = {} \n",
    "                            pattern_temporal_signatures[task_name]['temporal_signatures'][workload_name][metric_name] = feature_vector\n",
    "    # pprint.pprint(pattern_temporal_signatures)\n",
    "    return pattern_temporal_signatures\n",
    "\n",
    "task_pattern_temporal_signatures = build_container_temporal_signatures_scoped_sources(scoped_results, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4510ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFeatureVectors(task_temporal_signatures):\n",
    "    \"\"\"\n",
    "    Clean the feature vectors by removing containers that have no temporal signatures.\n",
    "    This function modifies the input dictionary in place.\n",
    "    Works with nested structure: {'container': {'temporal_signatures': {'workload': {'metric': {...}}}}}\n",
    "    \"\"\"\n",
    "    cleaned_task_temporal_signatures = task_temporal_signatures.copy()\n",
    "    none_counter = 0\n",
    "    to_delete = []\n",
    "    for name, info in cleaned_task_temporal_signatures.items():\n",
    "        if not info['temporal_signatures']:\n",
    "            none_counter += 1\n",
    "            to_delete.append(name)\n",
    "    print(f\"Total containers with no signature for any metric: {none_counter}\")\n",
    "\n",
    "    for name in to_delete:\n",
    "        del cleaned_task_temporal_signatures[name]\n",
    "\n",
    "    print(f\"Remaining containers after cleaning: {len(cleaned_task_temporal_signatures)}\")\n",
    "\n",
    "    # Collect all (workload, metric) pairs present in the data\n",
    "    all_workloads = set()\n",
    "    all_metrics = set()\n",
    "    all_pairs = set()\n",
    "    for info in cleaned_task_temporal_signatures.values():\n",
    "        for workload, metrics in info['temporal_signatures'].items():\n",
    "            all_workloads.add(workload)\n",
    "            for metric in metrics.keys():\n",
    "                all_metrics.add(metric)\n",
    "                all_pairs.add((workload, metric))\n",
    "    all_workloads = sorted(all_workloads)\n",
    "    all_metrics = sorted(all_metrics)\n",
    "    all_pairs = sorted(all_pairs)\n",
    "    print(f\"All workloads found: {all_workloads}\")\n",
    "    print(f\"All metrics found: {all_metrics}\")\n",
    "\n",
    "    all_feature_names = set()\n",
    "    for info in cleaned_task_temporal_signatures.values():\n",
    "        for workload_metrics in info['temporal_signatures'].values():\n",
    "            for metric in workload_metrics.values():\n",
    "                all_feature_names.update(metric.keys())\n",
    "    all_feature_names = sorted(all_feature_names)\n",
    "\n",
    "    containers_with_all_pairs = []\n",
    "    for container, info in cleaned_task_temporal_signatures.items():\n",
    "        container_pairs = set()\n",
    "        for workload, metrics in info['temporal_signatures'].items():\n",
    "            for metric in metrics.keys():\n",
    "                container_pairs.add((workload, metric))\n",
    "        if container_pairs == set(all_pairs):\n",
    "            containers_with_all_pairs.append(container)\n",
    "    print(f\"Keeping {len(containers_with_all_pairs)} containers with all workload/metric pairs.\")\n",
    "\n",
    "    # Filtered dict: only containers in containers_with_all_pairs\n",
    "    filtered_containers_temporal_signatures = {\n",
    "        k: v for k, v in cleaned_task_temporal_signatures.items()\n",
    "        if k in containers_with_all_pairs\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        containers_with_all_pairs,\n",
    "        all_pairs,\n",
    "        all_feature_names,\n",
    "        filtered_containers_temporal_signatures,\n",
    "        all_metrics\n",
    "    )\n",
    "tasks_with_all_pairs, all_pairs, all_feature_names, filtered_tasks_temporal_signatures, all_metrics = cleanFeatureVectors(task_pattern_temporal_signatures)\n",
    "# pprint.pprint(filtered_tasks_temporal_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatureMatriceInput(tasks_with_all_pairs, filtered_tasks_temporal_signatures):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the containers with all metrics and all workloads.\n",
    "    Returns the feature matrix and the container names.\n",
    "    \"\"\"\n",
    "    # Collect all (workload, metric, feature) triplets present in the data\n",
    "    all_triplets = set()\n",
    "    for info in filtered_tasks_temporal_signatures.values():\n",
    "        for workload, metrics in info['temporal_signatures'].items():\n",
    "            for metric, feats in metrics.items():\n",
    "                for feat in feats.keys():\n",
    "                    all_triplets.add((workload, metric, feat))\n",
    "    all_triplets = sorted(all_triplets)\n",
    "\n",
    "    # Build full feature names\n",
    "    full_feature_names = [f\"{w}_{m}_{f}\" for (w, m, f) in all_triplets]\n",
    "    \n",
    "    # print(f\"Total features: {full_feature_names}\")\n",
    "    \n",
    "    feature_matrix_x = []\n",
    "    task_names_x = []\n",
    "    for task in tasks_with_all_pairs:\n",
    "        info = filtered_tasks_temporal_signatures[task]\n",
    "        row = []\n",
    "        for workload, metric, feat in all_triplets:\n",
    "            value = (\n",
    "                info['temporal_signatures']\n",
    "                .get(workload, {})\n",
    "                .get(metric, {})\n",
    "                .get(feat, None)\n",
    "            )\n",
    "            if isinstance(value, np.ndarray):\n",
    "                row.extend(value.tolist())\n",
    "            else:\n",
    "                row.append(value)\n",
    "        feature_matrix_x.append(row)\n",
    "        task_names_x.append(task)\n",
    "\n",
    "        \n",
    "    feature_matrix_x = np.array(feature_matrix_x)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_x.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_x)\n",
    "    # print(df)\n",
    "    return feature_matrix_x, full_feature_names, task_names_x\n",
    "\n",
    "# With pattern temporal signatures\n",
    "feature_matrix_x, full_feature_names, task_names_x = buildFeatureMatriceInput(\n",
    "    tasks_with_all_pairs, filtered_tasks_temporal_signatures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add power values from one chosen data source to all nextflow files for each data source.\n",
    "# First just add the power values to fin_containers.\n",
    "def addPowerToFinContainers(fin_containers, tasks_with_all_pairs, power_stats):\n",
    "    \"\"\"\n",
    "    Add power values to the finished containers file.\n",
    "    \"\"\"\n",
    "    \n",
    "    fin_df = pd.read_csv(fin_containers)\n",
    "    # power_stat_container_names = set(f[:-4] for f in os.listdir(power_stats) if f.endswith('.csv'))\n",
    "    # power_stat_nextflow_names = set(\n",
    "    # pd.read_csv(os.path.join(power_stats, f))['Nextflow'].iloc[0]\n",
    "    # for f in os.listdir(power_stats)\n",
    "    # if f.endswith('.csv') and 'Nextflow' in pd.read_csv(os.path.join(power_stats, f)).columns)\n",
    "\n",
    "    container_to_nextflow = {}\n",
    "    for f in os.listdir(power_stats):\n",
    "        if f.endswith('.csv'):\n",
    "            container_name = f[:-4]\n",
    "            nextflow_name = pd.read_csv(os.path.join(power_stats, f))['Nextflow'].iloc[0] if 'Nextflow' in pd.read_csv(os.path.join(power_stats, f)).columns else None\n",
    "            container_to_nextflow[nextflow_name] = container_name\n",
    "\n",
    "    for task in tasks_with_all_pairs:\n",
    "        power_df = pd.read_csv(os.path.join(power_stats, f\"{container_to_nextflow[task]}.csv\"))\n",
    "        mean_power = power_df['Value (microjoules)'].mean() if 'Value (microjoules)' in power_df.columns else None\n",
    "        fin_df.loc[fin_df['Name'] == container_to_nextflow[task], 'MeanPower'] = mean_power\n",
    "    fin_df.to_csv(fin_containers, index=False)\n",
    "    # return fin_df, container_to_nextflow\n",
    "    return fin_df\n",
    "\n",
    "# fin_df, container_to_nextflow = addPowerToFinContainers(FIN_CONTAINERS, tasks_with_all_pairs, POWER_STATS)\n",
    "fin_df = addPowerToFinContainers(FIN_CONTAINERS, tasks_with_all_pairs, POWER_STATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72877ef",
   "metadata": {},
   "source": [
    "### Feature Extraction for Container Runtime and Power\n",
    "\n",
    "#### Given\n",
    "\n",
    "- **Containers**  \n",
    "  `C = {c1, c2, …, cn}`  \n",
    "  *Example:* `nxf-0X0tQJagkeWOAir2jS124FfK`, `nxf-0mUZ0M8vpF30z1CEoXjCQQbH`, …\n",
    "\n",
    "- **Metrics**  \n",
    "  `M = {runtime, power}`\n",
    "\n",
    "- **Feature Table**\n",
    "\n",
    "| Container Name                | runtime (s) | power (μJ)      |\n",
    "|-------------------------------|-------------|-----------------|\n",
    "| nxf-0X0tQJagkeWOAir2jS124FfK  | 123.4       | 1.23        |\n",
    "| nxf-0mUZ0M8vpF30z1CEoXjCQQbH  | 98.7        | 2.34       |\n",
    "\n",
    "- **Features per Container**  \n",
    "  `F = {runtime, power}`\n",
    "\n",
    "##### Feature Vector\n",
    "\n",
    "For each container `c_i`, extract:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_i =\n",
    "\\begin{bmatrix}\n",
    "\\text{runtime}(c_i) \\\\\n",
    "\\text{power}(c_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Matrix form\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y_{1,1} & y_{1,2} \\\\\\\\\n",
    "y_{2,1} & y_{2,2} \\\\\\\\\n",
    "\\vdots  & \\vdots  \\\\\\\\\n",
    "y_{n,1} & y_{n,2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each row corresponds to a container, and the columns are:\n",
    "- `runtime`: execution time in seconds\n",
    "- `power`: mean power consumption in microjoules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature output matrix for KCCA model.\n",
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names only for containers with available power values.\n",
    "    \"\"\"\n",
    "    task_runtime_power = {}\n",
    "\n",
    "    fin_df['LifeTime_s'] = (\n",
    "        fin_df['LifeTime']\n",
    "        .str.extract(r'([0-9.]+)(ms|s)', expand=True)\n",
    "        .assign(\n",
    "            value=lambda x: x[0].astype(float),\n",
    "            seconds=lambda x: np.where(x[1] == 'ms', x['value'] / 1000, x['value'])\n",
    "        )['seconds']\n",
    "    )\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        task_runtime_power[row['Nextflow']] = {\n",
    "            'runtime': row['LifeTime_s'],\n",
    "            'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    task_names_y = []\n",
    "\n",
    "    for task, info in task_runtime_power.items():\n",
    "        # if container not in cleaned_container_temporal_signatures:\n",
    "        \n",
    "        if task not in filtered_tasks_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['runtime']) and pd.notna(info['power']):\n",
    "            feature_matrix_y.append([info['runtime'], info['power']])\n",
    "            task_names_y.append(task)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['runtime', 'power'])\n",
    "\n",
    "    return feature_matrix_y, task_names_y\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, tasks_with_all_pairs, POWER_STATS)\n",
    "filtered_fin_df = finished_containers_dfs_with_power[\n",
    "    finished_containers_dfs_with_power['Nextflow'].isin(tasks_with_all_pairs)\n",
    "].copy()\n",
    "feature_matrix_y, task_names_y = buildFeatureMatriceOutput(filtered_fin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501343d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only as workaorund if needed \n",
    "def make_same_dimension(feature_matrix_x_patterns, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Ensure that the feature matrix X and task names X only include tasks that are common with task names Y.\n",
    "    \"\"\"\n",
    "    # Find the indices of common tasks\n",
    "    common_tasks = set(task_names_x).intersection(set(task_names_y))\n",
    "    indices_to_keep = [i for i, task in enumerate(task_names_x) if task in common_tasks]\n",
    "\n",
    "    # Filter the feature matrix and task names\n",
    "    feature_matrix_x_patterns = feature_matrix_x_patterns[indices_to_keep]\n",
    "    task_names_x = [task for task in task_names_x if task in common_tasks]\n",
    "\n",
    "    print(f\"Filtered feature matrix shape: {feature_matrix_x_patterns.shape}\")\n",
    "\n",
    "    return feature_matrix_x_patterns, task_names_x\n",
    "\n",
    "# Call the function\n",
    "feature_matrix_x, task_names_x = make_same_dimension(feature_matrix_x, task_names_x, task_names_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging output to check if the container names in X and Y match and order is the same.\n",
    "container_names_x = task_names_x\n",
    "container_names_y = task_names_y\n",
    "print(\"X names:\", container_names_x[:5])\n",
    "print(\"Y names:\", container_names_y[:5])\n",
    "print(\"Length X:\", len(container_names_x))\n",
    "print(\"Length Y:\", len(container_names_y))\n",
    "print(\"All X in Y:\", all(name in container_names_y for name in container_names_x))\n",
    "print(\"All Y in X:\", all(name in container_names_x for name in container_names_y))\n",
    "print(\"Order identical:\", container_names_x == container_names_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bfe3e",
   "metadata": {},
   "source": [
    "#### Z-Score Transformation and Standard Scaling performed on Feature and Label Matrices\n",
    "\n",
    "**Standard scaling** (also known as z-score normalization) is a technique used to standardize the features of a dataset so that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "##### Formula\n",
    "\n",
    "The standard score (z-score) for a value \\( x \\) is calculated as:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the original value,\n",
    "- \\( \\mu \\) is the mean of the training samples,\n",
    "- \\( \\sigma \\) is the standard deviation of the training samples.\n",
    "\n",
    "This transformation is performed **independently for each feature**.\n",
    "- Many machine learning algorithms assume that all features are centered around zero and have the same scale.\n",
    "- Features with larger scales can dominate the objective function and negatively impact model performance.\n",
    "- Standard scaling ensures that each feature contributes equally to the model.\n",
    "- StandardScaler is sensitive to outliers: extreme values can affect the mean and standard deviation, leading to less robust scaling.\n",
    "- For sparse data, one can disable mean centering to preserve sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de465a6",
   "metadata": {},
   "source": [
    "#### KCCA model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065aa788",
   "metadata": {},
   "source": [
    "#### Basic Model fitting and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data manually\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature_matrix_x, feature_matrix_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# MultiView Preprocessing \n",
    "preproc = MultiViewPreprocessing([StandardScaler(), StandardScaler()])\n",
    "X_train_scaled, Y_train_scaled = preproc.fit_transform([X_train, Y_train])\n",
    "X_test_scaled, Y_test_scaled = preproc.transform([X_test, Y_test])\n",
    "\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"Y_train_scaled shape:\", Y_train_scaled.shape)\n",
    "\n",
    "# Find c by cv, try different kernel functions\n",
    "# Define an kcca instance\n",
    "kcca = KCCA(latent_dimensions=2, kernel=\"rbf\")\n",
    "\n",
    "# Fit the instance\n",
    "kcca.fit((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "# Fit & transform\n",
    "# kcca.fit_transform((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "pairwise_correlations = kcca.pairwise_correlations((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "# explained_variance = kcca.explained_variance((X_train_scaled, Y_train_scaled))\n",
    "# # print(\"Explained variance:\", explained_variance)\n",
    "\n",
    "# explained_covariance = kcca.explained_covariance((X_train_scaled, Y_train_scaled))\n",
    "# print(\"Explained covariance:\", explained_covariance)\n",
    "\n",
    "score = kcca.score((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "print(\"Score of KCCA is: \", score)\n",
    "\n",
    "print(\"Pairwise correlations:\", pairwise_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71146a",
   "metadata": {},
   "source": [
    "#### Model Selection via Hyperparameter-tuning & cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation with cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    estimator=kcca,\n",
    "    views=(X_train_scaled, Y_train_scaled),\n",
    "    cv=5,  # Number of folds\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Setting the params for the cross-validation\n",
    "# param_grid = {\"kernel\": [\"poly\"], \"c\": [[1e-1], [1e-1, 2e-1]], \"degree\": [[2], [2, 3]]}\n",
    "# Define the parameter grid with only the kernel parameter\n",
    "param_grid = {\n",
    "    \"kernel\": [\"linear\", \"cosine\", \"rbf\", \"laplacian\", \"poly\", \"polynomial\", \"sigmoid\"]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with the simplified parameter grid\n",
    "kcca_grid = GridSearchCV(\n",
    "    KCCA(latent_dimensions=2),  # KCCA with default parameters except latent_dimensions\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    verbose=True,\n",
    ").fit([X_train_scaled, Y_train_scaled])\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", kcca_grid.best_params_)\n",
    "print(\"Best cross-validation score:\", kcca_grid.best_score_)\n",
    "\n",
    "# Applying the results\n",
    "best_kcca = KCCA(\n",
    "    latent_dimensions=2,\n",
    "    kernel='laplacian',\n",
    ")\n",
    "best_kcca.fit((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "best_model_score = best_kcca.score((X_test_scaled, Y_test_scaled))\n",
    "print(\"Best model score:\", best_model_score)\n",
    "\n",
    "best_model_pairwise_correlations = best_kcca.pairwise_correlations((X_test_scaled, Y_test_scaled))\n",
    "print(\"Best model pairwise correlations:\", best_model_pairwise_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49c88b",
   "metadata": {},
   "source": [
    "#### Enabling view predictions from latent space X to orig. space Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bb82e",
   "metadata": {},
   "source": [
    "#### (1) Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I think scaling is not yet considered, therefore high errors\n",
    "# Making the model predictive by regressing the latent space X to Y's space\n",
    "X_train_latent, _ = kcca.fit_transform((X_train_scaled, Y_train_scaled))\n",
    "reg = LinearRegression().fit(X_train_latent, Y_train)\n",
    "X_test_latent = kcca.transform((X_test_scaled, None))[0]\n",
    "Y_pred = reg.predict(X_test_latent)  # already in original Y-units\n",
    "\n",
    "# Evaluate basic Linear Regression model\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R²:\", r2)\n",
    "print(\"True Y (first 3 rows):\\n\", Y_test[:3])\n",
    "print(\"Predicted Y (first 3 rows):\\n\", Y_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc4b65",
   "metadata": {},
   "source": [
    "#### (2) Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a572b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why does scaling make no difference here?\n",
    "# TODO: Do kernel ridge regression here\n",
    "# TODO: I think scaling is not yet considered, therefore high errors\n",
    "# Making the model predictive by regressing the latent space X to Y's space\n",
    "X_train_latent, _ = best_kcca.fit_transform((X_train_scaled, Y_train_scaled))\n",
    "# reg = LinearRegression().fit(X_train_latent, Y_train_scaled)\n",
    "krr = KernelRidge().fit(X_train_latent, Y_train_scaled)\n",
    "X_test_latent = best_kcca.transform((X_test_scaled, None))[0]\n",
    "Y_pred_scaled = krr.predict(X_test_latent)  # already in original Y-units\n",
    "\n",
    "# Evaluate basic Linear Regression model\n",
    "mse = mean_squared_error(Y_test_scaled, Y_pred_scaled)\n",
    "r2 = r2_score(Y_test_scaled, Y_pred_scaled)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R²:\", r2)\n",
    "print(\"True Y (first 3 rows):\\n\", Y_test_scaled[:3])\n",
    "print(\"Predicted Y (first 3 rows):\\n\", Y_pred_scaled[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c1b24",
   "metadata": {},
   "source": [
    "#### CCA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e857d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data manually\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature_matrix_x, feature_matrix_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# MultiView Preprocessing \n",
    "preproc = MultiViewPreprocessing([StandardScaler(), StandardScaler()])\n",
    "X_train_scaled, Y_train_scaled = preproc.fit_transform([X_train, Y_train])\n",
    "X_test_scaled, Y_test_scaled = preproc.transform([X_test, Y_test])\n",
    "\n",
    "# Find c by cv, try different kernel functions\n",
    "# Define an kcca instance\n",
    "mcca = MCCA(latent_dimensions=2)\n",
    "\n",
    "# Fit the instance\n",
    "mcca.fit((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "# # Fit & transform\n",
    "mcca.fit_transform((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "pairwise_correlations = mcca.pairwise_correlations((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "score = mcca.score((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "print(\"Score of CCA is: \", score)\n",
    "\n",
    "print(\"Pairwise correlations:\", pairwise_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fe1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Sklearn CCA for comparison\n",
    "\n",
    "# Split the data manually\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature_matrix_x, feature_matrix_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# MultiView Preprocessing \n",
    "preproc = MultiViewPreprocessing([StandardScaler(), StandardScaler()])\n",
    "X_train_scaled, Y_train_scaled = preproc.fit_transform([X_train, Y_train])\n",
    "X_test_scaled, Y_test_scaled = preproc.transform([X_test, Y_test])\n",
    "\n",
    "# Find c by cv, try different kernel functions\n",
    "# Define an kcca instance\n",
    "cca = CCA(n_components=2)\n",
    "\n",
    "# Fit the instance\n",
    "cca.fit((X_train_scaled), (Y_train_scaled))\n",
    "\n",
    "# # Fit & transform\n",
    "# cca.fit_transform((X_train_scaled, Y_train_scaled))\n",
    "\n",
    "score = cca.score(X_train_scaled, Y_train_scaled)\n",
    "\n",
    "print(\"Score of CCA is: \", score)\n",
    "\n",
    "print(\"Pairwise correlations:\", pairwise_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d821631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I think scaling is not yet considered, therefore high errors\n",
    "# Making the model predictive by regressing the latent space X to Y's space\n",
    "X_train_latent, Y_train_latent = cca.fit_transform(X_train_scaled, Y_train_scaled)\n",
    "reg = LinearRegression().fit(X_train_latent, Y_train_latent)\n",
    "X_test_latent, _= cca.transform(X_test_scaled, Y_test_scaled)\n",
    "print(\"X_test_latent shape:\", X_test_latent.shape)\n",
    "Y_pred_non_inverse = reg.predict((X_test_latent))  \n",
    "print(\"Y_pred_non_inverse shape:\", Y_pred_non_inverse.shape)\n",
    "_, Y_pred_inverse = cca.inverse_transform(X_test_latent,Y_pred_non_inverse)\n",
    "\n",
    "# Evaluate basic Linear Regression model\n",
    "mse = mean_squared_error(Y_test_scaled, Y_pred_inverse)\n",
    "r2 = r2_score(Y_test_scaled, Y_pred_inverse)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R²:\", r2)\n",
    "print(\"True Y (first 3 rows):\\n\", Y_test_scaled[:3])\n",
    "print(\"Predicted Y (first 3 rows):\\n\", Y_pred_inverse[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the predict function of sklearn cca\n",
    "X_train_latent, Y_train_latent = cca.fit_transform(X_train_scaled, Y_train_scaled)\n",
    "Y_test_scaled_pred = cca.predict(X_test_scaled)\n",
    "\n",
    "score = cca.score(X_test_scaled, Y_test_scaled)\n",
    "print(\"Score of CCA predict is: \", score)\n",
    "\n",
    "# print(\"Y_test_scaled_pred :\", Y_test_scaled_pred)\n",
    "# print(\"Y_test_scaled :\", Y_test_scaled)\n",
    "\n",
    "# Evaluate basic Linear Regression model\n",
    "# mse = mean_squared_error(Y_test_scaled, Y_pred_inverse)\n",
    "# r2 = r2_score(Y_test_scaled, Y_pred_inverse)\n",
    "\n",
    "# print(\"MSE:\", mse)\n",
    "# print(\"R²:\", r2)\n",
    "# print(\"True Y (first 3 rows):\\n\", Y_test_scaled[:3])\n",
    "# print(\"Predicted Y (first 3 rows):\\n\", Y_pred_inverse[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955469f",
   "metadata": {},
   "source": [
    "### Building the Kernel Matrix for Workflow Tasks\n",
    "\n",
    "We build an $N \\times N$ matrix $K_x$ where the $(i, j)$-th entry is the kernel evaluation $k_x(x_i, x_j)$,  \n",
    "with $x_i$ and $x_j$ being the temporal signatures for tasks $i$ and $j$.\n",
    "\n",
    "- **Each row and column** corresponds to a workflow task.\n",
    "- **Each entry** $K_x[i, j] = k(x_i, x_j)$ measures the similarity between tasks $i$ and $j$ using a kernel function.\n",
    "- We use the **Gaussian (RBF) kernel**, which measures similarity based on the Euclidean distance in feature space, scaled by a parameter $\\sigma$.\n",
    "- This kernel gives higher values when two tasks have similar temporal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda0c12",
   "metadata": {},
   "source": [
    "### Kernel Canonical Correlation Analysis (KCCA) Overview\n",
    "\n",
    "The **KCCA algorithm** takes the kernel matrices \\( K_x \\) and \\( K_y \\) and solves a generalized eigenvector problem. This procedure finds subspaces in the linear space spanned by the eigenfunctions of the kernel functions such that projections onto these subspaces are **maximally correlated** [7]. Traditional Canonical Correlation Analysis (CCA) aims to find useful projections of features in each view of data by computing a weighted sum. However, due to its linearity, CCA may not extract meaningful descriptors of complex data.\n",
    "\n",
    "Kernel MCCA (KMCCA) addresses this limitation by first projecting the data into a higher-dimensional feature space **before** performing CCA in that new space.\n",
    "\n",
    "- We refer to these projections as the **resource usage projection** and the **metric projection**, respectively.\n",
    "- If the linear space associated with the Gaussian (RBF) kernel can be interpreted as clusters in the original feature space, then KCCA finds **correlated pairs of clusters** in the resource usage vector space and the performance/power vector space.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Compute kernel matrices** \\( K_x \\) and \\( K_y \\) for the resource and metric features.\n",
    "2. **Fit KCCA** using the training data kernel matrices.\n",
    "3. **Project data** into the maximally correlated subspaces for further analysis or prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ba6b0",
   "metadata": {},
   "source": [
    "Then, temporal signature of the new cluster is updated from the consolidated workloads. Such consolidation iterations stop when the clusters cannot be merged anymore since merging will incur significant interference, and/or the degradation in application performance will be intolerable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d21866",
   "metadata": {},
   "source": [
    "### Clustering for Workflow Task Consolidation\n",
    "\n",
    "Our consolidation problem can be viewed as a **clustering problem**. Traditionally, clustering algorithms group similar objects together based on a defined similarity or distance metric. However, in our context, the objective is different:\n",
    "\n",
    "- **Goal:** Group workflow tasks that are **dissimilar** in their resource requirements.\n",
    "- **Rationale:** By consolidating tasks with dissimilar resource usage, we can minimize resource contention and interference, leading to more efficient utilization of system resources.\n",
    "\n",
    "#### Custom Distance Measure\n",
    "\n",
    "To achieve this, we need to define a **distance measure** that captures the **interference** between the resource requirements of workflow tasks. Instead of grouping tasks with similar profiles, our distance metric should:\n",
    "\n",
    "- Assign **larger distances** to pairs of tasks with similar resource usage (to discourage grouping them together).\n",
    "- Assign **smaller distances** to pairs of tasks with complementary or non-overlapping resource usage (to encourage their consolidation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nextflow_task_peak_series(results_dir):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct Nextflow task value from the finished containers file.\n",
    "    \"\"\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    task_df = pd.read_csv(file_path)\n",
    "                    task_df['timestamp'] = pd.to_datetime(task_df['timestamp'], unit='ns')\n",
    "                    task_df.set_index('timestamp', inplace=True)\n",
    "                    value_cols = [col for col in task_df.columns if col.startswith('Value')]\n",
    "                    if not value_cols:\n",
    "                        # print(f\"Skipping {file_path} as it does not contain 'value' column.\")\n",
    "                        continue\n",
    "                    resource_series = task_df[value_cols[0]]\n",
    "                    # print(f\"Processing {file_path} with resource series: {resource_series.name}\")\n",
    "                    # Compute the peak series\n",
    "                    peak_series = resource_series.resample('3s').max()\n",
    "                    peak_df = peak_series.reset_index()\n",
    "                    # print(peak_series.head())\n",
    "                    peak_df.columns = ['timestamp','peak_value']\n",
    "                    out_file = os.path.join(root, f\"PEAK_Series_{file}\")\n",
    "                    peak_df.to_csv(out_file, index=False)\n",
    "                    # print(f\"Updated {file_path} with peak series for distance calculation in {out_file}\")\n",
    "    return scoped_results\n",
    "\n",
    "compute_nextflow_task_peak_series(scoped_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7893740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some ideas on how to handle the raw peak time series data for the workload types.\n",
    "def normalizePeakTimeSeries(df):\n",
    "    \"\"\"\n",
    "    Normalize the peak time series by scaling the 'peak_value' column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df['relative_time'] = (pd.to_datetime(df['timestamp']) - pd.to_datetime(df['timestamp']).iloc[0]).dt.total_seconds()\n",
    "\n",
    "    return df\n",
    "\n",
    "def interpolatePeakTimeSeries(df, n_points=100):\n",
    "    df = df.copy()\n",
    "    # Ensure rel_time is sorted\n",
    "    df = df.sort_values('relative_time')\n",
    "    # Interpolate peak_value to n_points\n",
    "    interp_times = np.linspace(df['relative_time'].min(), df['relative_time'].max(), n_points)\n",
    "    interp_values = np.interp(interp_times, df['relative_time'], df['peak_value'])\n",
    "    return interp_times, interp_values\n",
    "\n",
    "def truncatePeakTimeSeries(df_i, df_j):\n",
    "    \"\"\"\n",
    "    Truncate the peak time series to the length of the shorter series.\n",
    "    \"\"\"\n",
    "    if len(df_i) == len(df_j):\n",
    "        print(\"Both series are of equal length:\", len(df_i))\n",
    "        return df_i, df_j\n",
    "    min_length = min(len(df_i), len(df_j))\n",
    "    df_i = df_i.iloc[:min_length]\n",
    "    df_j = df_j.iloc[:min_length]\n",
    "    print(\"Truncated series to length:\", min_length)\n",
    "    return df_i, df_j\n",
    "\n",
    "def truncateTaskInput(filtered_tasks_temporal_signatures, n=40):\n",
    "    # Select n random keys to keep\n",
    "    keep_keys = random.sample(list(filtered_tasks_temporal_signatures.keys()), n)\n",
    "    # Build a new dict with only those keys\n",
    "    shortened_filtered_tasks_temporal_signatures = {k: filtered_tasks_temporal_signatures[k] for k in keep_keys}\n",
    "    return shortened_filtered_tasks_temporal_signatures\n",
    "\n",
    "shortened_filtered_tasks_temporal_signatures = truncateTaskInput(filtered_tasks_temporal_signatures, n=40)\n",
    "pprint.pprint(shortened_filtered_tasks_temporal_signatures)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b01a1",
   "metadata": {},
   "source": [
    "If two tasks have very similar peak patterns, then the correlation terms are high, the product is large, and the resulting distance value is large. That means they are “similar tasks” in terms of contention risk.\n",
    "\n",
    "If two tasks have uncorrelated or negatively correlated peaks, the correlation terms are low or near zero, so the product is small and the summed distance is small. That corresponds to “dissimilar tasks” that use resources at different times or in different ways, i.e. good consolidation candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to get the according peak time series for the current nextflow task.\n",
    "def getPeakTimeSeriesForTask(task_name, scoped_results, type = None):\n",
    "    \"\"\"\n",
    "    Get the peak time series for a given task name.\n",
    "    \"\"\"\n",
    "    \n",
    "    inverted_workload_type = next((k for k, v in workload_type_map.items() if v == type), None)\n",
    "\n",
    "    current_workload_dir = os.path.join(scoped_results, inverted_workload_type) if inverted_workload_type else scoped_results\n",
    "\n",
    "    for root, dirs, files in os.walk(current_workload_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            peak_file = os.path.join(root, f\"PEAK_Series_{task_name}.csv\")\n",
    "            if os.path.exists(peak_file):\n",
    "                if type is not None:\n",
    "                    print(f\"Found peak time series file for {task_name} with workload type {type}\") \n",
    "                return pd.read_csv(peak_file)\n",
    "    print(f\"Peak time series file not found for task: {task_name}\")\n",
    "    return None\n",
    "\n",
    "# To get affinity score for a pair:\n",
    "def get_affinity_score(type1, type2, aff_df):\n",
    "    # Try both (type1, type2) and (type2, type1) for symmetry\n",
    "    row = aff_df[\n",
    "        ((aff_df['workload_1'] == type1) & (aff_df['workload_2'] == type2)) |\n",
    "        ((aff_df['workload_1'] == type2) & (aff_df['workload_2'] == type1))\n",
    "    ]\n",
    "    if not row.empty:\n",
    "        return row['affinity_score'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Correlation can be NaN if two compared time series have no overlapping timestamps or if one of them has constant values.\n",
    "# For some containers one time disk reads or for short lived container the memory consumption is constant\n",
    "# which hinders the correlation calculation.\n",
    "# delete this task with constant peak series from the distance matrix\n",
    "def computeTaskSignatureDistances(scoped_results, filtered_tasks_temporal_signatures, container_to_nextflow):\n",
    "    \"\"\"\n",
    "    Compute the distances between task signatures in the feature space.\n",
    "    Returns a distance matrix based on the custom distance function.\n",
    "    \n",
    "    Args:\n",
    "        scoped_results: Result dictionary holding the peak time series for each task's metric.\n",
    "    Returns:\n",
    "        distance_matrix: Numpy array of distances between task signatures.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the affinity scores of the workload experiments\n",
    "    aff_df = pd.read_csv(\"affinity_score_matrix.csv\")\n",
    "    \n",
    "    # Use the keys of cleaned_container_temporal_signatures as task identifiers\n",
    "    nextflow_jobs = list(filtered_tasks_temporal_signatures.keys())\n",
    "    \n",
    "    filtered_jobs = []\n",
    "    for job in nextflow_jobs:\n",
    "        # print(f\"Processing job: {job}\")\n",
    "        peak_df = getPeakTimeSeriesForTask(container_to_nextflow[job], scoped_results, container_to_nextflow)\n",
    "        if peak_df is not None and not peak_df['peak_value'].nunique() == 1:\n",
    "            filtered_jobs.append(job)\n",
    "\n",
    "    distance_matrix = np.full((len(filtered_jobs), len(filtered_jobs)), np.nan)\n",
    "    \n",
    "    # Catch the calculated distances for the job pair i,j for distribution mapping\n",
    "    distances = []\n",
    "\n",
    "    for i in range(len(filtered_jobs)):\n",
    "        for j in range(i + 1, len(filtered_jobs)):\n",
    "            job_i = filtered_jobs[i]\n",
    "            job_j = filtered_jobs[j]\n",
    "            workloads_i = list(filtered_tasks_temporal_signatures[job_i]['temporal_signatures'].keys())\n",
    "            workloads_j = list(filtered_tasks_temporal_signatures[job_j]['temporal_signatures'].keys())\n",
    "\n",
    "            # Reset the temporary terms for each job pair\n",
    "            distance_i_j = 0.0\n",
    "            print(\"Reset distance for next job pair:\", job_i, job_j)\n",
    "            \n",
    "            # Keep track of processed affinity pairs per task\n",
    "            processed_pairs = set()\n",
    "            for wi in workloads_i:\n",
    "                for wj in workloads_j:\n",
    "                    type_i = workload_type_map.get(wi, wi)\n",
    "                    type_j = workload_type_map.get(wj, wj)\n",
    "\n",
    "                    pair = frozenset([type_i, type_j])\n",
    "                    if pair in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair)\n",
    "\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    # TERM 1 of the distance equation for each job i, j: Get the affinity score for the pair of workload types\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    affinity_score = get_affinity_score(type_i, type_j, aff_df)\n",
    "                    print(f\"Processing jobs {job_i} and {job_j} with workload type {type_i} vs {type_j}: affinity_score={affinity_score}\")\n",
    "\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    # Term 2 of the distance equation for each job i, j: Get the peak time series of the workload type 1\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    # If one time series is constant, set the distance to 0.\n",
    "                    print(\"Computing correlation for workload types in TERM 2:\", type_i, type_i)\n",
    "\n",
    "                    peak_df_i = getPeakTimeSeriesForTask(container_to_nextflow[job_i], scoped_results, type_i)\n",
    "                    peak_df_j = getPeakTimeSeriesForTask(container_to_nextflow[job_j], scoped_results, type_i)\n",
    "\n",
    "                    # Truncate the peak time series in place to the same lenght\n",
    "                    trun_peak_df_i, trun_peak_df_j = truncatePeakTimeSeries(peak_df_i, peak_df_j)\n",
    "                    \n",
    "                    # Compute the correlation for the peak time series of the same workload type\n",
    "                    try:\n",
    "                        corr_i_j_R1 = pearsonr(trun_peak_df_i['peak_value'], trun_peak_df_j['peak_value'])[0] \n",
    "                        if corr_i_j_R1 is None or np.isnan(corr_i_j_R1):\n",
    "                            corr_i_j_R1 = 0.0\n",
    "                            print(f\"Setting correlation to 0 for {job_i} vs {job_j} with workload type {type_i} due to NaN value.\")\n",
    "                        print(f\"Correlation for {job_i} vs {job_j} with workload type {type_i}: {corr_i_j_R1}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error computing correlation for {job_i} vs {job_j} with workload type {type_i}{type_i}: {e}\")\n",
    "                        corr_i_j_R1 = 0.0\n",
    "                        print(f\"Setting correlation to 0 for {job_i} vs {job_j} with workload type {type_i}{type_i} due to one of two series being constant.\")\n",
    "                    \n",
    "                    \n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    # TERM 3 of the distance equation for each job i, j: Get the correlation of the peak time series of the identical workload types 2\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    # If one time series is constant, set the distance to 0.\n",
    "                    print(\"Computing correlation for workload types in TERM 3:\", type_j, type_j)\n",
    "                    \n",
    "                    peak_df_i = getPeakTimeSeriesForTask(container_to_nextflow[job_i], scoped_results, type_j)\n",
    "                    peak_df_j = getPeakTimeSeriesForTask(container_to_nextflow[job_j], scoped_results, type_j)\n",
    "                    \n",
    "                    # Truncate the peak time series in place to the same lenght\n",
    "                    trun_peak_df_i, trun_peak_df_j = truncatePeakTimeSeries(peak_df_i, peak_df_j)\n",
    "\n",
    "                    # Compute the correlation for the peak time series of the same workload type\n",
    "                    try:\n",
    "                        corr_i_j_R2 = pearsonr(trun_peak_df_i['peak_value'], trun_peak_df_j['peak_value'])[0] \n",
    "                        if corr_i_j_R2 is None or np.isnan(corr_i_j_R2):\n",
    "                            corr_i_j_R2 = 0.0\n",
    "                            print(f\"Setting correlation to 0 for {job_i} vs {job_j} with workload type {type_i} due to NaN value.\")\n",
    "                        print(f\"Correlation for {job_i} vs {job_j} with workload type {type_j} {type_j}: {corr_i_j_R2}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error computing correlation for {job_i} vs {job_j} with workload type {type_i}: {e}\")\n",
    "                        corr_i_j_R2 = 0.0\n",
    "                        print(f\"Setting correlation to 0 for {job_i} vs {job_j} with workload type {type_j} {type_j} due to one of two series being constant.\")\n",
    "\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    # Sum over jobs i,j per metric pair\n",
    "                    # -------------------------------------------------------------------------------------------------\n",
    "                    distance_i_j += affinity_score * corr_i_j_R1 * corr_i_j_R2\n",
    "                \n",
    "            # -------------------------------------------------------------------------------------------------\n",
    "            # Write distance matrix entry for the job pair i,j\n",
    "            # -------------------------------------------------------------------------------------------------\n",
    "            print(f\"Distance for job pair ({job_i}, {job_j}): {distance_i_j}\")\n",
    "            \n",
    "            # Write the distances into list for distribution mapping\n",
    "            distances.append(distance_i_j)\n",
    "            \n",
    "            distance_matrix[i, j] = distance_i_j\n",
    "            # I think only one triangle of the matrix is enough. May increase performance.\n",
    "            distance_matrix[j, i] = distance_i_j\n",
    "\n",
    "    print(\"Distance matrix computed.\")\n",
    "    # Fill the diagonal with zeros (distance to self is zero)\n",
    "    np.fill_diagonal(distance_matrix, 0.0)\n",
    "    print(\"Distance matrix:\\n\", distance_matrix)\n",
    "\n",
    "    distance_df = pd.DataFrame(distance_matrix, index=filtered_jobs, columns=filtered_jobs)\n",
    "                    \n",
    "    return distance_matrix, distances, distance_df\n",
    "\n",
    "# Map the workload types to the affinity score matrix\n",
    "workload_type_map = {\n",
    "\"task_memory_data\": \"mem\",\n",
    "\"task_cpu_data\": \"cpu\",\n",
    "\"task_disk_data\": \"fileio\"\n",
    "}\n",
    "    \n",
    "distance_matrix, distances, distance_df = computeTaskSignatureDistances(scoped_results, shortened_filtered_tasks_temporal_signatures, container_to_nextflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43449918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5d88a",
   "metadata": {},
   "source": [
    "The merge threshold sets the cutoff for when two tasks are considered “dissimilar enough” to be clustered together. A lower threshold means only pairs with very small distance values (i.e. highly compatible or non-interfering tasks) will be merged, leading to fewer merges and more clusters overall. A higher threshold allows merging even when tasks are less compatible, which results in more merges and fewer clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a82ae",
   "metadata": {},
   "source": [
    "You first normalize the pairwise distances with a QuantileTransformer so they follow a standard distribution. Then you look only at the unique pairwise values (lower triangle of the distance matrix) and pick a percentile cutoff. Interpreted:\n",
    "Distances below this cutoff are judged “small enough” to merge.\n",
    "Distances above it are kept apart.\n",
    "\n",
    "The intuition is that the distance scale depends on the dataset (different workloads, resource ranges, correlations), so there is no universal numeric threshold. By picking a percentile, you tie the merge rule to the empirical distribution of distances in the current dataset.\n",
    "\n",
    "Correctness comes from this: a low percentile means you only merge the most dissimilar tasks (the very smallest distances) → conservative merging, more clusters left. A higher percentile means you treat a larger fraction of tasks as “mergeable” → more aggressive merging, fewer clusters.\n",
    "\n",
    "So the threshold is not an absolute notion of similarity but a relative cutoff within the observed distances, which makes the clustering adaptive to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f402e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMergeThreshold(distance_matrix):\n",
    "\n",
    "    # n_quantiles is set to the training set size rather than the default value\n",
    "    # to avoid a warning being raised by this example\n",
    "    qt = QuantileTransformer(\n",
    "        n_quantiles=len(distance_matrix), output_distribution=\"normal\" \n",
    "    )\n",
    "\n",
    "    # transformed_distances = qt.fit_transform(np.array(distances)).reshape(-1, 1)\n",
    "    transformed_distances = qt.fit_transform(distance_matrix)\n",
    "    # print(transformed_distances)\n",
    "\n",
    "    # Determine threshold\n",
    "    # 1. Get the lower triangle of the distance matrix without the diagonal\n",
    "    tril_values = transformed_distances[np.tril_indices_from(transformed_distances, k=-1)]\n",
    "    tril_values_raw = distance_matrix[np.tril_indices_from(distance_matrix, k=-1)]\n",
    "\n",
    "    # 2. Compute the 10th percentile\n",
    "    threshold_transformed = np.percentile(tril_values, 15)\n",
    "    # I only use the raw thesholds for now.\n",
    "    threshold_raw = np.percentile(tril_values_raw, 40)\n",
    "\n",
    "    print(\"Raw Threshold for current distance matrix:\", threshold_raw)\n",
    "\n",
    "    return threshold_raw, transformed_distances\n",
    "\n",
    "threshold_raw, transformed_distances = computeMergeThreshold(distance_matrix)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17875ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agglomerative clustering algorithm on the distance matrix\n",
    "# TODO: Calculate the distance threshold based on distribution of the distances in the data.\n",
    "def runAgglomerativeClustering(distance_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Run agglomerative clustering on the distance matrix.\n",
    "    Returns the cluster labels for each task.\n",
    "    \n",
    "    Args:\n",
    "        distance_matrix: Numpy array of distances between task signatures.\n",
    "    Returns:\n",
    "        cluster_labels: Numpy array of cluster labels for each task.\n",
    "    \"\"\"\n",
    "    clustering = AgglomerativeClustering(n_clusters = None, metric='precomputed', linkage='average', compute_full_tree=True, compute_distances=False, distance_threshold=threshold).fit(distance_matrix)\n",
    "    cluster_labels = clustering.labels_\n",
    "    print(f\"Number of clusters found: {len(set(cluster_labels))}\")\n",
    "    print(f\"Cluster labels: {cluster_labels}\")\n",
    "    job_to_cluster = dict(zip(distance_matrix.index, cluster_labels))\n",
    "    return cluster_labels, job_to_cluster\n",
    "\n",
    "cluster_labels, job_to_cluster = runAgglomerativeClustering(distance_df, threshold_raw)\n",
    "pprint.pprint(job_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterToJobs(job_to_cluster):\n",
    "    \"\"\"\n",
    "    Convert job to cluster mapping to cluster to jobs mapping.\n",
    "    Returns a dictionary where keys are clusters and values are lists of jobs in those clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_to_jobs = defaultdict(list)\n",
    "    for k, v in job_to_cluster.items():\n",
    "        cluster_to_jobs[v].append(k)\n",
    "\n",
    "    # Collect keys to delete\n",
    "    keys_to_delete = [k for k, v in cluster_to_jobs.items() if len(v) == 1]\n",
    "    for k in keys_to_delete:\n",
    "        del cluster_to_jobs[k]\n",
    "\n",
    "    return cluster_to_jobs\n",
    "\n",
    "cluster_to_jobs = clusterToJobs(job_to_cluster)\n",
    "pprint.pprint(cluster_to_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1efa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_signature_dict(signature_dict):\n",
    "    # signature_dict: the nested dict for one job\n",
    "    flat = {}\n",
    "    for workload, metrics in signature_dict.items():\n",
    "        for metric, features in metrics.items():\n",
    "            for feature, value in features.items():\n",
    "                flat_key = f\"{workload}/{metric}/{feature}\"\n",
    "                flat[flat_key] = value\n",
    "    return flat\n",
    "\n",
    "# TODO: Update to not only sum up the values but to weigh them with 2 factors:\n",
    "# 1) The time that the jobs actually spends in his peak/min\n",
    "# 2) How much the peak/min overlaps between the jobs.\n",
    "def updateTaskSignatureToColoc(cluster_to_jobs, shortened_filtered_tasks_temporal_signatures):\n",
    "    for k, v in cluster_to_jobs.items():\n",
    "        \n",
    "        # Initialize the coloc task signature\n",
    "        coloc_dataframes = []\n",
    "        \n",
    "        for job in v:\n",
    "            # print(job) \n",
    "            vector = shortened_filtered_tasks_temporal_signatures[job]['temporal_signatures']\n",
    "            flattened_vector = flatten_signature_dict(vector)\n",
    "            df = pd.DataFrame([flattened_vector])\n",
    "            coloc_dataframes.append(df)\n",
    "            \n",
    "        # Merge the dataframes for the coloc task and write back to updated dict\n",
    "        summed_df = pd.concat(coloc_dataframes).groupby(level=0).sum()\n",
    "        print(f\"Summed DataFrame for cluster {k}:\\n\", summed_df.head())\n",
    "            \n",
    "            \n",
    "updateTaskSignatureToColoc(cluster_to_jobs, shortened_filtered_tasks_temporal_signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e0c5d",
   "metadata": {},
   "source": [
    "#### Trying out some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortened_filtered_tasks_temporal_signatures['nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_MERGE_STRELKA_(HCC1395N)']['temporal_signatures'])\n",
    "# print(shortened_filtered_tasks_temporal_signatures['nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(HCC1395N)']['temporal_signatures'])\n",
    "print(shortened_filtered_tasks_temporal_signatures['nf-NFCORE_ATACSEQ_ATACSEQ_MERGED_LIBRARY_BIGWIG_PLOT_DEEPTOOLS_DEEPTOOLS_PLOTHEATMAP_(GM12878_OMNI_REP2)']['temporal_signatures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111200e",
   "metadata": {},
   "source": [
    "#### For some probes: Calculate the means etc. and compare them in a table against random co-locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995999a",
   "metadata": {},
   "source": [
    "### Random Forest Regressor Modeling\n",
    "#### Some parts of the data processing are repeated here for better understandability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bbe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature output matrix with runtime labels\n",
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names only for containers with available power values.\n",
    "    \"\"\"\n",
    "    task_runtime_power = {}\n",
    "\n",
    "    fin_df['LifeTime_s'] = (\n",
    "        fin_df['LifeTime']\n",
    "        .str.extract(r'([0-9.]+)(ms|s)', expand=True)\n",
    "        .assign(\n",
    "            value=lambda x: x[0].astype(float),\n",
    "            seconds=lambda x: np.where(x[1] == 'ms', x['value'] / 1000, x['value'])\n",
    "        )['seconds']\n",
    "    )\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        task_runtime_power[row['Nextflow']] = {\n",
    "            'runtime': row['LifeTime_s'],\n",
    "            # 'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    task_names_y = []\n",
    "\n",
    "    for task, info in task_runtime_power.items():\n",
    "        # if container not in cleaned_container_temporal_signatures:\n",
    "        \n",
    "        if task not in filtered_tasks_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['runtime']):\n",
    "            feature_matrix_y.append([info['runtime']])\n",
    "            task_names_y.append(task)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['runtime'])\n",
    "\n",
    "    return feature_matrix_y, task_names_y, df\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, tasks_with_all_pairs, POWER_STATS)\n",
    "filtered_fin_df = finished_containers_dfs_with_power[\n",
    "    finished_containers_dfs_with_power['Nextflow'].isin(tasks_with_all_pairs)\n",
    "].copy()\n",
    "feature_matrix_y_runtime, task_names_y, df  = buildFeatureMatriceOutput(filtered_fin_df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature output matrix for KCCA model.\n",
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names only for containers with available power values.\n",
    "    \"\"\"\n",
    "    task_runtime_power = {}\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        task_runtime_power[row['Nextflow']] = {\n",
    "            # 'runtime': row['LifeTime_s'],\n",
    "            'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    task_names_y = []\n",
    "\n",
    "    for task, info in task_runtime_power.items():\n",
    "        # if container not in cleaned_container_temporal_signatures:\n",
    "        \n",
    "        if task not in filtered_tasks_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['power']):\n",
    "            feature_matrix_y.append([info['power']])\n",
    "            task_names_y.append(task)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['power'])\n",
    "\n",
    "    return feature_matrix_y, task_names_y, df\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, tasks_with_all_pairs, POWER_STATS)\n",
    "filtered_fin_df = finished_containers_dfs_with_power[\n",
    "    finished_containers_dfs_with_power['Nextflow'].isin(tasks_with_all_pairs)\n",
    "].copy()\n",
    "feature_matrix_y_power, task_names_y,df = buildFeatureMatriceOutput(filtered_fin_df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check, might not be needed for forests.\n",
    "# Scale the feature matrices for regression models with runtime output labels.\n",
    "def scaleFeatureMatrices(feature_matrix_x, reg_runtime_feature_matrix_y):\n",
    "    \"\"\"\n",
    "    Scale the feature matrices using StandardScaler.\n",
    "    Returns the scaled feature matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape to 2D array\n",
    "    reg_runtime_y = np.array(reg_runtime_feature_matrix_y)\n",
    "    # print(reg_runtime_y)\n",
    "    if reg_runtime_y.ndim == 1:\n",
    "        reg_runtime_y = reg_runtime_y.reshape(-1,1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    scaled_x = scaler_x.fit_transform(feature_matrix_x)\n",
    "    scaled_y = scaler_y.fit_transform(reg_runtime_y)\n",
    "\n",
    "    print(f\"Scaled feature matrix X shape: {scaled_x.shape}\")\n",
    "    print(f\"Scaled feature matrix Y with runtime labels shape: {scaled_y.shape}\")\n",
    "    \n",
    "    return scaled_x, scaled_y, scaler_x, scaler_y\n",
    "\n",
    "scaled_feature_matrix_x, scaled_runtime_feature_matrix_y, scaler_x, runtime_scaler_y = scaleFeatureMatrices(feature_matrix_x, feature_matrix_y_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check, might not be needed for forests.\n",
    "# Only as workaorund if needed \n",
    "def make_same_dimension(feature_matrix_x_patterns, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Ensure that the feature matrix X and task names X only include tasks that are common with task names Y.\n",
    "    \"\"\"\n",
    "    # Find the indices of common tasks\n",
    "    common_tasks = set(task_names_x).intersection(set(task_names_y))\n",
    "    indices_to_keep = [i for i, task in enumerate(task_names_x) if task in common_tasks]\n",
    "\n",
    "    # Filter the feature matrix and task names\n",
    "    feature_matrix_x_patterns = feature_matrix_x_patterns[indices_to_keep]\n",
    "    task_names_x = [task for task in task_names_x if task in common_tasks]\n",
    "\n",
    "    print(f\"Filtered feature matrix shape: {feature_matrix_x_patterns.shape}\")\n",
    "\n",
    "    return feature_matrix_x_patterns, task_names_x\n",
    "\n",
    "# Call the function\n",
    "scaled_runtime_feature_matrix_y, task_names_y = make_same_dimension(scaled_runtime_feature_matrix_y, task_names_x, task_names_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature matrices for regression models with power output labels.\n",
    "def scaleFeatureMatrices(feature_matrix_x, reg_power_feature_matrix_y):\n",
    "    \"\"\"\n",
    "    Scale the feature matrices using StandardScaler.\n",
    "    Returns the scaled feature matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape to 2D array\n",
    "    reg_power_y = np.array(reg_power_feature_matrix_y)\n",
    "    # print(reg_power_y)\n",
    "    if reg_power_y.ndim == 1:\n",
    "        reg_power_y = reg_power_y.reshape(-1,1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    scaled_x = scaler_x.fit_transform(feature_matrix_x)\n",
    "    scaled_y = scaler_y.fit_transform(reg_power_y)\n",
    "\n",
    "    print(f\"Scaled feature matrix X shape: {scaled_x.shape}\")\n",
    "    print(f\"Scaled feature matrix Y with power labels shape: {scaled_y.shape}\")\n",
    "    \n",
    "    return scaled_x, scaled_y, scaler_x, scaler_y\n",
    "\n",
    "scaled_feature_matrix_x, scaled_power_feature_matrix_y, scaler_x, reg_power_scaler_y = scaleFeatureMatrices(feature_matrix_x, feature_matrix_y_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06257538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power\n",
    "def splitFeatureMatrices(scaled_feature_matrix_x, scaled_power_feature_matrix_y, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = train_test_split(\n",
    "        scaled_feature_matrix_x, scaled_power_feature_matrix_y, task_names_x, task_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y\n",
    "\n",
    "X_train, X_test, y_train_power, y_test_power, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = splitFeatureMatrices(scaled_feature_matrix_x, scaled_power_feature_matrix_y, task_names_x, task_names_y)\n",
    "pprint.pprint(scaled_power_feature_matrix_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power unscaled\n",
    "def splitFeatureMatrices(feature_matrix_x, feature_matrix_y_power, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = train_test_split(\n",
    "        feature_matrix_x, feature_matrix_y_power, task_names_x, task_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y\n",
    "\n",
    "X_train, X_test, y_train_power, y_test_power, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = splitFeatureMatrices(feature_matrix_x, feature_matrix_y_power, task_names_x, task_names_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3698fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime\n",
    "def splitFeatureMatrices(scaled_feature_matrix_x, scaled_runtime_feature_matrix_y, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = train_test_split(\n",
    "        scaled_feature_matrix_x, scaled_runtime_feature_matrix_y, task_names_x, task_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y\n",
    "\n",
    "X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = splitFeatureMatrices(scaled_feature_matrix_x, scaled_runtime_feature_matrix_y, task_names_x, task_names_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ee10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check, might not be needed for forests.\n",
    "# Only as workaorund if needed \n",
    "def make_same_dimension(feature_matrix_x_patterns, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Ensure that the feature matrix X and task names X only include tasks that are common with task names Y.\n",
    "    \"\"\"\n",
    "    # Find the indices of common tasks\n",
    "    common_tasks = set(task_names_x).intersection(set(task_names_y))\n",
    "    indices_to_keep = [i for i, task in enumerate(task_names_x) if task in common_tasks]\n",
    "\n",
    "    # Filter the feature matrix and task names\n",
    "    feature_matrix_x_patterns = feature_matrix_x_patterns[indices_to_keep]\n",
    "    task_names_x = [task for task in task_names_x if task in common_tasks]\n",
    "\n",
    "    print(f\"Filtered feature matrix shape: {feature_matrix_x_patterns.shape}\")\n",
    "\n",
    "    return feature_matrix_x_patterns, task_names_x\n",
    "\n",
    "# Call the function\n",
    "feature_matrix_y_runtime, task_names_x = make_same_dimension(feature_matrix_y_runtime, task_names_x, task_names_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8826f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of feature_matrix_x: {len(feature_matrix_x)}\")\n",
    "print(f\"Length of feature_matrix_y_runtime: {len(feature_matrix_y_runtime)}\")\n",
    "print(f\"Length of task_names_x: {len(task_names_x)}\")\n",
    "print(f\"Length of task_names_y: {len(task_names_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34113351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime unscaled\n",
    "def splitFeatureMatrices(feature_matrix_x, feature_matrix_y_runtime, task_names_x, task_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = train_test_split(\n",
    "        feature_matrix_x, feature_matrix_y_runtime, task_names_x, task_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train_runtime, y_test_runtime, train_task_names_x, test_task_names_x, train_task_names_y, test_task_names_y = splitFeatureMatrices(feature_matrix_x, feature_matrix_y_runtime, task_names_x, task_names_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7d39c",
   "metadata": {},
   "source": [
    "#### Establish a baseline estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d78fa5",
   "metadata": {},
   "source": [
    "The baseline provides a reference point to evaluate the effectiveness of the model.\n",
    "Baseline Prediction: The baseline prediction is the mean of the training labels. \n",
    "Baseline Errors: The absolute difference between the baseline predictions and the actual test labels is calculated to measure the baseline error.\n",
    "Average Baseline Error: The mean of the baseline errors is reported as the baseline performance.\n",
    "If the model cannot outperform this baseline, it indicates that the model is not learning meaningful patterns from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669da67",
   "metadata": {},
   "source": [
    "#### Power prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the baseline predictions\n",
    "baseline_preds = np.mean(y_train_power)  # Use the mean of the training labels as the baseline\n",
    "\n",
    "# Repeat the baseline prediction for all test samples\n",
    "baseline_preds = np.full(y_test_power.shape, baseline_preds)\n",
    "\n",
    "# Calculate the baseline errors\n",
    "baseline_errors = abs(baseline_preds - y_test_power)\n",
    "\n",
    "# Display the average baseline error\n",
    "print('Average baseline error (power):', round(np.mean(baseline_errors), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd594fc",
   "metadata": {},
   "source": [
    "#### Runtime prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0687b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the baseline predictions\n",
    "baseline_preds_runtime = np.mean(y_train_runtime)  # Use the mean of the training labels as the baseline\n",
    "\n",
    "# Repeat the baseline prediction for all test samples\n",
    "baseline_preds_runtime = np.full(y_test_runtime.shape, baseline_preds_runtime)\n",
    "\n",
    "# Calculate the baseline errors\n",
    "baseline_errors_runtime = abs(baseline_preds_runtime - y_test_runtime)\n",
    "\n",
    "# Display the average baseline error\n",
    "print('Average baseline error (runtime):', round(np.mean(baseline_errors_runtime), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719415fc",
   "metadata": {},
   "source": [
    "The baseline predictions are off by 0.66 units (e.g., seconds for runtime or watts for power) from the actual test values.\n",
    "Similarly, the baseline predictions are off by 0.8 units on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77edf48",
   "metadata": {},
   "source": [
    "#### Predictive Modeling for Power Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afdeafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor to predict the power of tasks, if co-located\n",
    "def trainPowerWithRandomForest(X, y):\n",
    "    \"\"\"\n",
    "    Train a Random Forest regressor to predict power consumption based on the feature matrix.\n",
    "    \"\"\"\n",
    "    regr = RandomForestRegressor(n_estimators=400,max_depth=2, max_features='log2',random_state=42)\n",
    "\n",
    "    regr.fit(X, y.ravel()) # Flatten y-vector to 1D\n",
    "\n",
    "    return regr\n",
    "\n",
    "\n",
    "def predictPowerWithRandomForest(regressor, test_Data):\n",
    "    \"\"\"\n",
    "    Predict the power consumption using the trained Random Forest regressor.\n",
    "    \n",
    "    Args:\n",
    "        regressor: Trained Random Forest regressor.\n",
    "        test_data: Test data for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        Predicted power consumption values.\n",
    "    \"\"\"\n",
    "\n",
    "    return regressor.predict(test_Data)\n",
    "\n",
    "# Fit the model.\n",
    "trainedPowerPredictor = trainPowerWithRandomForest(X_train, y_train_power.ravel())\n",
    "# Predict power consumption for the test data.\n",
    "predicted_power = predictPowerWithRandomForest(trainedPowerPredictor, X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "errors_power = abs(predicted_power - y_test_power.ravel())\n",
    "\n",
    "# MAE (Mean Absolute Error)\n",
    "print('Mean Absolute Error (power):', round(np.mean(errors_power), 2), 'units.')\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape_power = 100 * (errors_power / y_test_power.ravel())\n",
    "\n",
    "# Accuracy for power prediction\n",
    "accuracy_power = 100 - np.mean(mape_power)\n",
    "print('Power prediction accuracy:', round(accuracy_power, 2), '%')\n",
    "\n",
    "model_score = trainedPowerPredictor.score(X_test, y_test_power.ravel())\n",
    "print(\"Model score (R²):\", model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters currently in use\n",
    "print('Parameters currently in use:\\n')\n",
    "print(trainedPowerPredictor.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning (RandomizedSearchCV)\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "criterion = ['squared_error', 'absolute_error', 'friedman_mse', ]\n",
    "max_features = ['log2', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 610, num=30)]\n",
    "max_depth.append(None)\n",
    "max_samples = [0.5, 0.75, 1.0]\n",
    "min_samples_split = [2, 5, 10, 15, 17, 20]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'max_samples': max_samples,\n",
    "    'criterion': criterion,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    # 'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=200,  # Number of random combinations to try\n",
    "    cv=3,        \n",
    "    verbose=2,\n",
    "    random_state=0,\n",
    "    n_jobs=-1    # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_random.fit(X_train, y_train_runtime.ravel())\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters Found:\")\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e62aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_model = rf_random.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "errors_power = abs(predictions - y_test_runtime.ravel())\n",
    "print('Mean Absolute Error (runtime):', round(np.mean(errors_power), 2), 'units.')\n",
    "\n",
    "# Best model's score\n",
    "best_model_score = best_model.score(X_test, y_test_power.ravel())\n",
    "print(\"Best model score (R²):\", best_model_score)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test_runtime, predictions)\n",
    "print('Mean Absolute Error (MAE):', round(mae, 2), 'units.')\n",
    "\n",
    "mse = mean_squared_error(y_test_runtime, predictions)\n",
    "print(\"MSE is :\", round(mse, 2), 'units.')\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape_power = 100 * (errors_power / y_test_power.ravel())\n",
    "\n",
    "# Accuracy for power prediction\n",
    "accuracy_power = 100 - np.mean(mape_power)\n",
    "print('Runtime prediction accuracy:', round(accuracy_power, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef4e55",
   "metadata": {},
   "source": [
    "#### Predictive Modeling for Runtime Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff76d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor to predict the runtime of colocatable tasks\n",
    "def trainRuntimeWithRandomForest(X, y):\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "    regr.fit(X, y.ravel())\n",
    "    return regr\n",
    "    \n",
    "def predictRuntimeWithRandomForest(regressor, test_Data):\n",
    "    \"\"\"\n",
    "    Predict the runtime using the trained Random Forest regressor.\n",
    "    \n",
    "    Args:\n",
    "        regressor: Trained Random Forest regressor.\n",
    "        test_data: Test data for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        Predicted runtime values.\n",
    "    \"\"\"\n",
    "\n",
    "    return regressor.predict(test_Data)\n",
    "    \n",
    "\n",
    "# Fit the model.\n",
    "trainedRuntimePredictor = trainRuntimeWithRandomForest(X_train, y_train_runtime.ravel())\n",
    "\n",
    "# Predict the runtime for the test data.\n",
    "predicted_runtime = predictRuntimeWithRandomForest(trainedRuntimePredictor, X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "errors_runtime = abs(predicted_runtime - y_test_runtime.ravel())\n",
    "\n",
    "# MAE\n",
    "print('Mean Absolute Error (runtime):', round(np.mean(errors_runtime), 2), 'units.')\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape_runtime = 100 * (errors_runtime / y_test_runtime.ravel())\n",
    "\n",
    "# Accuracy for power prediction\n",
    "accuracy_runtime = 100 - np.mean(mape_runtime)\n",
    "print('Runtime prediction ccuracy:', round(accuracy_runtime, 2), '%')\n",
    "\n",
    "model_score = trainedRuntimePredictor.score(X_test, y_test_power.ravel())\n",
    "print(\"Model score (R²):\", model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters currently in use\n",
    "print('Parameters currently in use:\\n')\n",
    "print(trainedRuntimePredictor.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning (RandomizedSearchCV)\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "max_features = ['log2', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=200,  # Number of random combinations to try\n",
    "    cv=7,        # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=0,\n",
    "    n_jobs=-1    # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_random.fit(X_train, y_train_runtime.ravel())\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters Found:\")\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_model = rf_random.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "errors_runtime = abs(predictions - y_test_runtime.ravel())\n",
    "print('Mean Absolute Error (runtime):', round(np.mean(errors_runtime), 2), 'units.')\n",
    "\n",
    "# MAE\n",
    "print('Mean Absolute Error (runtime):', round(np.mean(errors_runtime), 2), 'units.')\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape_runtime = 100 * (errors_runtime / y_test_runtime.ravel())\n",
    "\n",
    "score = best_model.score(X_test, y_test_runtime.ravel())\n",
    "print(\"Best model score (R²):\", score)\n",
    "\n",
    "# Accuracy for power prediction\n",
    "accuracy_runtime = 100 - np.mean(mape_runtime)\n",
    "print('Runtime prediction ccuracy:', round(accuracy_runtime, 2), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharecomp-bB4WWry4-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
