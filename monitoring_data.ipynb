{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bad7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import docker\n",
    "import logging\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import re\n",
    "import pprint\n",
    "import yaml\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from mvlearn.embed import KMCCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import shutil \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59eeac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configurations.\n",
    "RESULTS_DIR = \"/usr/local/bin/results\"\n",
    "SCOPED_RESULTS_DIR = \"/usr/local/bin/scoped_results\"\n",
    "CONFIG_FILE = \"/usr/local/bin/scoped_results/config.yml\"\n",
    "FIN_CONTAINERS = \"/usr/local/bin/scoped_results/died_nextflow_containers.csv\"\n",
    "START_CONTAINERS = \"/usr/local/bin/scoped_results/started_nextflow_containers.csv\"\n",
    "META_DATA = \"slurm-job-exporter\"\n",
    "DATA_SOURCE = \"ebpf-mon\"\n",
    "POWER_METERING = \"ebpf-mon\"\n",
    "POWER_STATS= \"/usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the monitoring results data.\n",
    "results = \"/usr/local/bin/results\"\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "start_containers = \"/usr/local/bin/results/started_nextflow_containers.csv\"\n",
    "\n",
    "for root, dirs, files in os.walk(results):\n",
    "    # print(i)\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data = pd.read_csv(file_path, index_col=0)\n",
    "            print(f\"Found CSV file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc66e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'identifier': 'name', 'source': 'cAdvisor'},\n",
      " {'identifier': 'name', 'source': 'ebpf-mon'},\n",
      " {'identifier': 'container_name', 'source': 'docker-activity'}]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Include scope results constants here to not read in the data sources that wont be used.\n",
    "# Create a dict to for the data source and its corresponding primary key\n",
    "# Identifier to primary key mapping\n",
    "# name = nxf-container-name (used by cadvisor, ebpf-mon)\n",
    "# container_name = nxf-container-name (used by docker-activity)\n",
    "# workDir = nxf-container-workdir (used by slurm-exporter)\n",
    "\n",
    "def readInResultsConf(config_file):\n",
    "    \"\"\"\n",
    "    Read in the results configuration file and return a dictionary.\n",
    "    \"\"\"\n",
    "    monitoring_config = config_file\n",
    "    with open(monitoring_config, 'r') as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    filtered_sources = []\n",
    "    seen = set()\n",
    "    for target in data['monitoring_targets'].values():\n",
    "        ds = target.get('data_sources')\n",
    "        if ds:\n",
    "            if isinstance(ds, dict):\n",
    "                ds = [ds]\n",
    "            for entry in ds:\n",
    "                filtered = {k: entry[k] for k in ('identifier', 'source') if k in entry}\n",
    "                if (\n",
    "                    'source' in filtered and\n",
    "                    filtered['source'] == 'slurm-job-exporter'\n",
    "                ):\n",
    "                    continue\n",
    "                if 'source' in filtered and 'identifier' in filtered:\n",
    "                    key = (filtered['source'], filtered['identifier'])\n",
    "                    if key not in seen:\n",
    "                        filtered_sources.append(filtered)\n",
    "                        seen.add(key)\n",
    "    pprint.pprint(filtered_sources)\n",
    "    return filtered_sources\n",
    "\n",
    "filtered_sources = readInResultsConf(\"/usr/local/bin/results/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d4e68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scoped results directory: /usr/local/bin/scoped_results\n"
     ]
    }
   ],
   "source": [
    "# Set the scope for the results data\n",
    "def resultsScope(results_dir, meta_data, data_source, power_metering):\n",
    "    \"\"\"\n",
    "   Creates a copy of the results directory and returns the cleaned file tree depending on the users scope definition.\n",
    "   Meta data, data source and power metering are mandatory scope definitions.\n",
    "    \"\"\"\n",
    "    scoped_results_dir = shutil.copytree(results_dir, \"/usr/local/bin/scoped_results\", dirs_exist_ok=True)\n",
    "    if data_source == 'all':\n",
    "        print(\"Data source is set to 'all', no filtering will be applied.\")\n",
    "        return scoped_results_dir\n",
    "    for metric in os.listdir(scoped_results_dir):\n",
    "        metric_path = os.path.join(scoped_results_dir, metric)\n",
    "        if not os.path.isdir(metric_path):\n",
    "           continue \n",
    "    # Walk from base dir and rm all dirs that do not match the scope and the power dirs. \n",
    "        for subdir in os.listdir(metric_path):\n",
    "            subdir_path = os.path.join(metric_path, subdir)\n",
    "            subdir_name = os.path.basename(subdir_path)\n",
    "            # print(\"Sub directory name:\", subdir_name)\n",
    "            if os.path.isdir(subdir_path) and subdir_name not in [meta_data, data_source, power_metering]:\n",
    "                shutil.rmtree(subdir_path, ignore_errors=True)\n",
    "    print(\"Successfully scoped results directory:\", scoped_results_dir) \n",
    "    return scoped_results_dir\n",
    "\n",
    "scoped_results = resultsScope(RESULTS_DIR, META_DATA, DATA_SOURCE, POWER_METERING) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bee043a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting time series data by data source.\n"
     ]
    }
   ],
   "source": [
    "def split_task_timeseries_by_datasource(results_dir, datasource_identifier_map, nextflow_pattern=r\"nxf-[A-Za-z0-9]{23}\"):\n",
    "    \"\"\"\n",
    "    For each data source in datasource_identifier_map, traverse the results_dir,\n",
    "    and for each metric, split the time series CSVs into per-task files using the correct identifier column.\n",
    "    \"\"\"\n",
    "    for datasource, identifier in datasource_identifier_map.items():\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            if os.path.basename(root) == datasource:\n",
    "                for metric in os.listdir(root):\n",
    "                    metric_path = os.path.join(root, metric)\n",
    "                    if os.path.isdir(metric_path):\n",
    "                        containers_dir = os.path.join(metric_path, \"containers\")\n",
    "                        os.makedirs(containers_dir, exist_ok=True)\n",
    "                        for file in os.listdir(metric_path):\n",
    "                            if file.endswith(\".csv\"):\n",
    "                                file_path = os.path.join(metric_path, file)\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                if identifier not in df.columns:\n",
    "                                    print(f\"Identifier '{identifier}' not found in {file_path}, skipping.\")\n",
    "                                    continue\n",
    "                                for task_name in df[identifier].unique():\n",
    "                                    if pd.isna(task_name):\n",
    "                                        continue\n",
    "                                    if re.match(nextflow_pattern, str(task_name)):\n",
    "                                        task_df = df[df[identifier] == task_name]\n",
    "                                        out_path = os.path.join(containers_dir, f\"{task_name}.csv\")\n",
    "                                        task_df.to_csv(out_path, index=False)\n",
    "                                        # print(f\"Saved data for {task_name} to {out_path}\")\n",
    "    print(\"Finished splitting time series data by data source.\")\n",
    "\n",
    "datasource_identifier_map = {d['source']: d['identifier'] for d in filtered_sources}\n",
    "split_task_timeseries_by_datasource(scoped_results, datasource_identifier_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b3f0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- cAdvisor ---\n",
      "Containers in monitored list but NOT in cAdvisor: {'nxf-Qru0CcRgTuUzX8HRaHO9owtd', 'nxf-9eBqec7AlOZ3GnoFtEtY14ny', 'nxf-1AUOV7AhBGVUbCmee5WApTRX', 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-UY2XomSHbY5BM00lkqJ3KiSI', 'nxf-2OomksOk8DQ5FccAo70dfVRP', 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ', 'nxf-ChknE6ywWKGXejr2ww4QMS5U', 'nxf-r5ECchowA1ziKKWKd50nqfCS', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc', 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt', 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0', 'nxf-l4UOQ6vq023FfdVkhpq6uhFB', 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw', 'nxf-favxiZeim630IZ9J6BMtGABS', 'nxf-i3k55HVSqlStQlJ9rLveDORE', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-nIl5dATth0K0iWt19eAMXSrN', 'nxf-bQCEmlIiekPOOtkHpYmKBSn7', 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV'}\n",
      "Count: 23\n",
      "Containers in cAdvisor but NOT in monitored list: set()\n",
      "Count: 0\n",
      "\n",
      "--- ebpf-mon ---\n",
      "Containers in monitored list but NOT in ebpf-mon: {'nxf-Qru0CcRgTuUzX8HRaHO9owtd', 'nxf-9eBqec7AlOZ3GnoFtEtY14ny', 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt', 'nxf-favxiZeim630IZ9J6BMtGABS', 'nxf-nIl5dATth0K0iWt19eAMXSrN', 'nxf-2OomksOk8DQ5FccAo70dfVRP', 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ', 'nxf-ChknE6ywWKGXejr2ww4QMS5U', 'nxf-r5ECchowA1ziKKWKd50nqfCS'}\n",
      "Count: 9\n",
      "Containers in ebpf-mon but NOT in monitored list: set()\n",
      "Count: 0\n",
      "\n",
      "--- docker-activity ---\n",
      "Containers in monitored list but NOT in docker-activity: {'nxf-Qru0CcRgTuUzX8HRaHO9owtd', 'nxf-9eBqec7AlOZ3GnoFtEtY14ny', 'nxf-1AUOV7AhBGVUbCmee5WApTRX', 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-UY2XomSHbY5BM00lkqJ3KiSI', 'nxf-2OomksOk8DQ5FccAo70dfVRP', 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ', 'nxf-ChknE6ywWKGXejr2ww4QMS5U', 'nxf-r5ECchowA1ziKKWKd50nqfCS', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc', 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt', 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0', 'nxf-l4UOQ6vq023FfdVkhpq6uhFB', 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw', 'nxf-favxiZeim630IZ9J6BMtGABS', 'nxf-i3k55HVSqlStQlJ9rLveDORE', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-nIl5dATth0K0iWt19eAMXSrN', 'nxf-bQCEmlIiekPOOtkHpYmKBSn7', 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV'}\n",
      "Count: 23\n",
      "Containers in docker-activity but NOT in monitored list: set()\n",
      "Count: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report_missing_tasks_all_sources(results_dir, datasource_identifier_map, fin_containers_df, container_workdirs, nextflow_pattern=r\"nxf-[A-Za-z0-9]{23}\"):\n",
    "    \"\"\"\n",
    "    For each data source, report how many tasks are missing compared to the finished containers.\n",
    "    \"\"\"\n",
    "    workdir_containers = set(container_workdirs.keys())\n",
    "    for datasource, identifier in datasource_identifier_map.items():\n",
    "        found_containers = set()\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            if os.path.basename(root) == datasource:\n",
    "                for metric in os.listdir(root):\n",
    "                    metric_path = os.path.join(root, metric)\n",
    "                    if os.path.isdir(metric_path):\n",
    "                        for file in os.listdir(metric_path):\n",
    "                            if file.endswith(\".csv\"):\n",
    "                                file_path = os.path.join(metric_path, file)\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                if identifier not in df.columns:\n",
    "                                    continue\n",
    "                                found_containers.update(\n",
    "                                    str(name) for name in df[identifier].unique()\n",
    "                                    if pd.notna(name) and re.match(nextflow_pattern, str(name))\n",
    "                                )\n",
    "        missing_in_source = workdir_containers - found_containers\n",
    "        missing_in_workdirs = found_containers - workdir_containers\n",
    "        print(f\"--- {datasource} ---\")\n",
    "        print(\"Containers in monitored list but NOT in\", datasource + \":\", missing_in_source)\n",
    "        print(\"Count:\", len(missing_in_source))\n",
    "        print(\"Containers in\", datasource, \"but NOT in monitored list:\", missing_in_workdirs)\n",
    "        print(\"Count:\", len(missing_in_workdirs))\n",
    "        print()\n",
    "        \n",
    "datasource_identifier_map = {d['source']: d['identifier'] for d in filtered_sources}\n",
    "fin_containers = \"/usr/local/bin/results/died_nextflow_containers.csv\"\n",
    "fin_containers_df = pd.read_csv(fin_containers)\n",
    "container_workdirs = {row['Name']: row['WorkDir'] for idx, row in fin_containers_df.iterrows()}\n",
    "report_missing_tasks_all_sources(scoped_results, datasource_identifier_map, fin_containers_df, container_workdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebcccbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with work directory /storage/nf-core/exec/work/48/3e276b831989e44e8449921d3e24fa\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with work directory /storage/nf-core/exec/work/94/2bf0ff8f1a2f6a32ee87cae434e8a0\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with work directory /storage/nf-core/exec/work/d4/ff637f253591c16675a74fe80e4927\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with work directory /storage/nf-core/exec/work/01/4f14e8b8126fbac7baccff8e4cae55\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with work directory /storage/nf-core/exec/work/6c/0f4e022e1426c1e3b45d00b68baa08\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with work directory /storage/nf-core/exec/work/1c/7b4be64c0d5e43fa128a5121c12713\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with work directory /storage/nf-core/exec/work/b4/9500f3660050f0de4517c81f1c237e\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with work directory /storage/nf-core/exec/work/c5/b2b3087f571565ee63e9c38d9df2fb\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with work directory /storage/nf-core/exec/work/6b/a7145d39cde6b860115892ce5bdf63\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with work directory /storage/nf-core/exec/work/50/5ebb11cf7cdf7e5678c657b2e95629\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with work directory /storage/nf-core/exec/work/2c/4c0cf1f1b0137da4c7cf62de9a36ef\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with work directory /storage/nf-core/exec/work/f7/2dbec7a19b449f9081f8b853198fd0\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with work directory /storage/nf-core/exec/work/52/e49117568959272d9c35156b02b7e8\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with work directory /storage/nf-core/exec/work/1c/413541c0e7a56746a9f9c69415bb90\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with work directory /storage/nf-core/exec/work/48/3e276b831989e44e8449921d3e24fa\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with work directory /storage/nf-core/exec/work/94/2bf0ff8f1a2f6a32ee87cae434e8a0\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with work directory /storage/nf-core/exec/work/d4/ff637f253591c16675a74fe80e4927\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with work directory /storage/nf-core/exec/work/01/4f14e8b8126fbac7baccff8e4cae55\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with work directory /storage/nf-core/exec/work/6c/0f4e022e1426c1e3b45d00b68baa08\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with work directory /storage/nf-core/exec/work/1c/7b4be64c0d5e43fa128a5121c12713\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with work directory /storage/nf-core/exec/work/b4/9500f3660050f0de4517c81f1c237e\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with work directory /storage/nf-core/exec/work/c5/b2b3087f571565ee63e9c38d9df2fb\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with work directory /storage/nf-core/exec/work/6b/a7145d39cde6b860115892ce5bdf63\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with work directory /storage/nf-core/exec/work/50/5ebb11cf7cdf7e5678c657b2e95629\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with work directory /storage/nf-core/exec/work/2c/4c0cf1f1b0137da4c7cf62de9a36ef\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with work directory /storage/nf-core/exec/work/f7/2dbec7a19b449f9081f8b853198fd0\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with work directory /storage/nf-core/exec/work/52/e49117568959272d9c35156b02b7e8\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with work directory /storage/nf-core/exec/work/1c/413541c0e7a56746a9f9c69415bb90\n"
     ]
    }
   ],
   "source": [
    "def add_workdir_to_all_task_csvs(results_dir, container_workdirs):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct WorkDir from container_workdirs.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    fin_container_df = pd.read_csv(file_path)\n",
    "                    container_name = os.path.splitext(file)[0]\n",
    "                    if container_name in container_workdirs:\n",
    "                        workdir = container_workdirs[container_name]\n",
    "                        fin_container_df['WorkDir'] = workdir\n",
    "                        fin_container_df.to_csv(file_path, index=False)\n",
    "                        print(f\"Updated {file_path} with work directory {workdir}\")\n",
    "\n",
    "add_workdir_to_all_task_csvs(scoped_results, container_workdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/slurm_job_id.csv\n",
      "Processing job: nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list)\n",
      "Saved data for nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_PREPARE_INTERVALS_CREATE_INTERVALS_BED_(genome.interval_list).csv\n",
      "Processing job: nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome)\n",
      "Saved data for nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test)\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test) to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test).csv\n",
      "Processing job: nf-NFCORE_SAREK_SAREK_MULTIQC\n",
      "Saved data for nf-NFCORE_SAREK_SAREK_MULTIQC to /usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id/nf-NFCORE_SAREK_SAREK_MULTIQC.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Maybe update results path with scoped results path.\n",
    "def extract_slurm_job_metadata(slurm_metadata_path, slurm_job_col=\"job_name\"):\n",
    "    \"\"\"\n",
    "    Extracts slurm job metadata from time-series CSVs and writes each job's data to a separate file.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(slurm_metadata_path):\n",
    "        if file.endswith(\"slurm_job_id.csv\"):\n",
    "            file_path = os.path.join(slurm_metadata_path, file)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            for job_name in df[slurm_job_col].unique():\n",
    "                if pd.isna(job_name):\n",
    "                    continue\n",
    "                print(f\"Processing job: {job_name}\")\n",
    "                job_df = df[df[slurm_job_col] == job_name]\n",
    "                out_path = os.path.join(slurm_metadata_path, f\"{job_name}.csv\")\n",
    "                job_df.to_csv(out_path, index=False)\n",
    "                print(f\"Saved data for {job_name} to {out_path}\")\n",
    "\n",
    "extract_slurm_job_metadata(\"/usr/local/bin/results/task_metadata/slurm-job-exporter/slurm_job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a2980f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /usr/local/bin/scoped_results/task_metadata/slurm-job-exporter/slurm_job_id/slurm_job_id.csv\n",
      "Updated /usr/local/bin/scoped_results/died_nextflow_containers.csv with slurm job info.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Maybe update results path with scoped results path.\n",
    "def update_finished_containers_with_nfcore_task(slurm_metadata_path, fin_containers, workdir_col='WorkDir', slurm_workdir_col='work_dir', slurm_job_col='job_name'):\n",
    "    \"\"\"\n",
    "    Update the finished containers file with the nf-core task name (Nextflow) by matching work directories\n",
    "    with slurm job metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    updated = False\n",
    "    for file in os.listdir(slurm_metadata_path):\n",
    "        if file.endswith(\"slurm_job_id.csv\"):\n",
    "            file_path = os.path.join(slurm_metadata_path, file)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            fin_df = pd.read_csv(fin_containers)\n",
    "            if workdir_col in fin_df.columns and slurm_workdir_col in df.columns:\n",
    "                for idx, row in df.iterrows():\n",
    "                    work_dir = row[slurm_workdir_col]\n",
    "                    slurm_job = row[slurm_job_col]\n",
    "                    if pd.isna(work_dir) or pd.isna(slurm_job):\n",
    "                        print(f\"Skipping row {idx} due to missing WorkDir or slurm_job.\")\n",
    "                        continue\n",
    "                    # Update fin_df where WorkDir matches\n",
    "                    fin_df.loc[fin_df[workdir_col] == work_dir, 'Nextflow'] = slurm_job\n",
    "                # Write back the updated fin_df\n",
    "                fin_df.to_csv(fin_containers, index=False)\n",
    "                print(f\"Updated {fin_containers} with slurm job info.\")\n",
    "                updated = True\n",
    "            else:\n",
    "                print(\"WorkDir or job_name column missing in DataFrames.\")\n",
    "    if not updated:\n",
    "        print(\"No updates were made to the finished containers file.\")\n",
    "\n",
    "slurm_metadata_path = os.path.join(scoped_results, \"task_metadata\", \"slurm-job-exporter\", \"slurm_job_id\")\n",
    "update_finished_containers_with_nfcore_task(slurm_metadata_path, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc9f3bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with Nextflow value nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with Nextflow value nf-NFCORE_SAREK_SAREK_MULTIQC\n",
      "Updated /usr/local/bin/scoped_results/task_energy_data/ebpf-mon/container_power/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-8HEIDLPLcFSgNV2onUEea8wK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-1AUOV7AhBGVUbCmee5WApTRX.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_CRAM_QC_MOSDEPTH_SAMTOOLS_SAMTOOLS_STATS_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-SX1AWI1RbvjBo0PJOwC1FAFw.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_MARKDUPLICATES_GATK4_MARKDUPLICATES_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-i3k55HVSqlStQlJ9rLveDORE.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_QUAL_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0X0tQJagkeWOAir2jS124FfK.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQC_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-qDilxwaxmY8uJ5TscM5fDPNc.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L1)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-cPB62cVKMj0A2W3ZgiyXeXAy.csv with Nextflow value nf-NFCORE_SAREK_PREPARE_INTERVALS_GATK4_INTERVALLISTTOBED_(genome)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-6NsMcpYNvIhqIRPUkkVmSPjV.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_SUMMARY_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-bQCEmlIiekPOOtkHpYmKBSn7.csv with Nextflow value nf-NFCORE_SAREK_SAREK_VCF_QC_BCFTOOLS_VCFTOOLS_VCFTOOLS_TSTV_COUNT_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-UY2XomSHbY5BM00lkqJ3KiSI.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_VARIANT_CALLING_GERMLINE_ALL_BAM_VARIANT_CALLING_SINGLE_STRELKA_STRELKA_SINGLE_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-TrD9qyudd3YDIKNfgNkKpu9H.csv with Nextflow value nf-NFCORE_SAREK_SAREK_FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON_BWAMEM1_MEM_(test-test_L2)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0mUZ0M8vpF30z1CEoXjCQQbH.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_BASERECALIBRATOR_GATK4_BASERECALIBRATOR_(test)\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-l4UOQ6vq023FfdVkhpq6uhFB.csv with Nextflow value nf-NFCORE_SAREK_SAREK_MULTIQC\n",
      "Updated /usr/local/bin/scoped_results/task_cpu_data/ebpf-mon/container_weighted_cycles/containers/nxf-0pUrbbt0IplTwbj4uE7h1Lv0.csv with Nextflow value nf-NFCORE_SAREK_SAREK_BAM_APPLYBQSR_GATK4_APPLYBQSR_(test)\n"
     ]
    }
   ],
   "source": [
    "def add_nextflow_to_all_task_csvs(results_dir, fin_containers_file, workdir_col='WorkDir', nextflow_col='Nextflow'):\n",
    "    \"\"\"\n",
    "    For every data source and metric, update each per-task CSV in 'containers' subfolders\n",
    "    with the correct Nextflow task value from the finished containers file.\n",
    "    \"\"\"\n",
    "    fin_df = pd.read_csv(fin_containers_file)\n",
    "    # Ensure WorkDir is string and stripped in fin_df\n",
    "    fin_df[workdir_col] = fin_df[workdir_col].astype(str).str.strip()\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    container_df = pd.read_csv(file_path)\n",
    "                    if workdir_col in container_df.columns:\n",
    "                        # Ensure WorkDir is string and stripped in container_df\n",
    "                        container_df[workdir_col] = container_df[workdir_col].astype(str).str.strip()\n",
    "                        workdir = container_df[workdir_col].iloc[0]\n",
    "                        match = fin_df[fin_df[workdir_col] == workdir]\n",
    "                        if not match.empty and nextflow_col in match.columns:\n",
    "                            nextflow_value = match[nextflow_col].values[0]\n",
    "                            container_df[nextflow_col] = nextflow_value\n",
    "                            container_df.to_csv(file_path, index=False)\n",
    "                            print(f\"Updated {file_path} with Nextflow value {nextflow_value}\")\n",
    "                        else:\n",
    "                            print(f\"No matching Nextflow value found for WorkDir {workdir} in {file_path}\") \n",
    "\n",
    "add_nextflow_to_all_task_csvs(scoped_results, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1ace",
   "metadata": {},
   "source": [
    "## Building the Kernel Matrix for Workflow Tasks\n",
    "\n",
    "We build an $N \\times N$ matrix $K_x$ where the $(i, j)$-th entry is the kernel evaluation $k_x(x_i, x_j)$,  \n",
    "with $x_i$ and $x_j$ being the temporal signatures for tasks $i$ and $j$.\n",
    "\n",
    "- **Each row and column** corresponds to a workflow task.\n",
    "- **Each entry** $K_x[i, j] = k(x_i, x_j)$ measures the similarity between tasks $i$ and $j$ using a kernel function.\n",
    "- We use the **Gaussian (RBF) kernel**, which measures similarity based on the Euclidean distance in feature space, scaled by a parameter $\\sigma$.\n",
    "- This kernel gives higher values when two tasks have similar temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0929880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nxf-0X0tQJagkeWOAir2jS124FfK': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 25483109423.0,\n",
      "                                                                                        'mean': 192675972059.03488,\n",
      "                                                                                        'peak_value': 198719087576.0,\n",
      "                                                                                        'variance': 1.0222524387866007e+21}}},\n",
      " 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1032268530333.0,\n",
      "                                                                                        'mean': 1032268530333.0,\n",
      "                                                                                        'peak_value': 1032268530333.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 0.0,\n",
      "                                                                                        'mean': 81623193039.92046,\n",
      "                                                                                        'peak_value': 749995124404.0,\n",
      "                                                                                        'variance': 1.6268045250826192e+22}}},\n",
      " 'nxf-1AUOV7AhBGVUbCmee5WApTRX': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 10099893263.0,\n",
      "                                                                                        'mean': 10099893263.0,\n",
      "                                                                                        'peak_value': 10099893263.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-2OomksOk8DQ5FccAo70dfVRP': {'temporal_signatures': {}},\n",
      " 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 100384532.0,\n",
      "                                                                                        'mean': 100384532.0,\n",
      "                                                                                        'peak_value': 100384532.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-8HEIDLPLcFSgNV2onUEea8wK': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 3385406987.0,\n",
      "                                                                                        'mean': 21827950682.107143,\n",
      "                                                                                        'peak_value': 22511007856.0,\n",
      "                                                                                        'variance': 1.2749086616447406e+19}}},\n",
      " 'nxf-9eBqec7AlOZ3GnoFtEtY14ny': {'temporal_signatures': {}},\n",
      " 'nxf-ChknE6ywWKGXejr2ww4QMS5U': {'temporal_signatures': {}},\n",
      " 'nxf-Qru0CcRgTuUzX8HRaHO9owtd': {'temporal_signatures': {}},\n",
      " 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 2498983465.0,\n",
      "                                                                                        'mean': 2057853096370.8164,\n",
      "                                                                                        'peak_value': 2652160935417.0,\n",
      "                                                                                        'variance': 1.147040387264574e+24}}},\n",
      " 'nxf-TrD9qyudd3YDIKNfgNkKpu9H': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1215164914769.0,\n",
      "                                                                                        'mean': 1215164914769.0,\n",
      "                                                                                        'peak_value': 1215164914769.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-UY2XomSHbY5BM00lkqJ3KiSI': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 18318826.0,\n",
      "                                                                                        'mean': 975265508.1348314,\n",
      "                                                                                        'peak_value': 28397848096.0,\n",
      "                                                                                        'variance': 2.6530589336617357e+19}}},\n",
      " 'nxf-bQCEmlIiekPOOtkHpYmKBSn7': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 8992514122.0,\n",
      "                                                                                        'mean': 8992514122.0,\n",
      "                                                                                        'peak_value': 8992514122.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-cPB62cVKMj0A2W3ZgiyXeXAy': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 61006529243.0,\n",
      "                                                                                        'mean': 1220750387110.952,\n",
      "                                                                                        'peak_value': 1264240781781.0,\n",
      "                                                                                        'variance': 5.105281221794935e+22}}},\n",
      " 'nxf-favxiZeim630IZ9J6BMtGABS': {'temporal_signatures': {}},\n",
      " 'nxf-i3k55HVSqlStQlJ9rLveDORE': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 101092567.0,\n",
      "                                                                                        'mean': 101092567.0,\n",
      "                                                                                        'peak_value': 101092567.0,\n",
      "                                                                                        'variance': 0.0}}},\n",
      " 'nxf-l4UOQ6vq023FfdVkhpq6uhFB': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 69202580.0,\n",
      "                                                                                        'mean': 20647296047.94017,\n",
      "                                                                                        'peak_value': 174880615261.0,\n",
      "                                                                                        'variance': 1.5746105714743446e+21}}},\n",
      " 'nxf-lDZYA22rlrG8QxKhhFiDw5KQ': {'temporal_signatures': {}},\n",
      " 'nxf-nIl5dATth0K0iWt19eAMXSrN': {'temporal_signatures': {}},\n",
      " 'nxf-qDilxwaxmY8uJ5TscM5fDPNc': {'temporal_signatures': {'container_weighted_cycles': {'lowest_value': 1232322940.0,\n",
      "                                                                                        'mean': 10042455993682.168,\n",
      "                                                                                        'peak_value': 10419001881335.0,\n",
      "                                                                                        'variance': 3.8270910104668226e+24}}},\n",
      " 'nxf-qo1gLwZ5KqnI1XeGT9KeMRUt': {'temporal_signatures': {}},\n",
      " 'nxf-r5ECchowA1ziKKWKd50nqfCS': {'temporal_signatures': {}}}\n"
     ]
    }
   ],
   "source": [
    "# TODO: If necessary deal with the numerical format of the signatures.\n",
    "def build_container_temporal_signatures_scoped_sources(results_dir, fin_containers_file):\n",
    "    \"\"\"\n",
    "    Build feature vectors for the scoped data sources and metrics by scanning every containers directory\n",
    "    under every metric for every data source. Returns a dictionary of container temporal signatures.\n",
    "    As the power consumption data of the workflow tasks will be used as labels to train models, it will be excluded from the temporal signatures.\n",
    "    Each container will have a 'temporal_signatures' dict with keys like 'source/metric' for every metric from the scoped data source(s).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fin_containers_file)\n",
    "    container_temporal_signatures = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        container_temporal_signatures[row['Name']] = {\n",
    "            'temporal_signatures': {}\n",
    "        }\n",
    "\n",
    "    # Feature vectors\n",
    "    for root, dirs, files in os.walk(results_dir):\n",
    "        if \"task_energy_data\" in root.split(os.sep):\n",
    "            continue\n",
    "        if os.path.basename(root) == \"containers\":\n",
    "            metric_name = os.path.basename(os.path.dirname(root))\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    ts_container_df = pd.read_csv(file_path)\n",
    "                    ts_container_df['timestamp'] = pd.to_datetime(ts_container_df['timestamp'], unit='ns')\n",
    "                    ts_container_df.set_index('timestamp', inplace=True)\n",
    "                    value_cols = [col for col in ts_container_df.columns if col.startswith('Value')]\n",
    "                    if not value_cols:\n",
    "                        print(f\"Skipping {file_path} as it does not contain 'value' column.\")\n",
    "                        continue\n",
    "                    resource_series = ts_container_df[value_cols[0]]  \n",
    "\n",
    "                    # Feature extraction\n",
    "                    peak_value = resource_series.max()\n",
    "                    lowest_value = resource_series.min()\n",
    "                    mean_value = resource_series.mean()\n",
    "                    median_value = resource_series.median()\n",
    "                    variance = resource_series.var()\n",
    "                    mean_val = resource_series.mean()\n",
    "                    if mean_val == 0:\n",
    "                        relative_variance = 0.0  \n",
    "                    else:\n",
    "                        relative_variance = (resource_series.var() - mean_val**2) / (mean_val**2)\n",
    "                    std_dev = resource_series.std()\n",
    "                    pattern_vector = resource_series.iloc[np.round(np.linspace(0, len(resource_series) - 1, 10)).astype(int)].to_numpy()\n",
    "\n",
    "                    # The server spec can come from the host benchmark in nextflow\n",
    "                    server_spec = {\n",
    "                        'GHz x Cores': \"\",\n",
    "                        'GFlops': \"\",\n",
    "                        'RAM': \"\",\n",
    "                        'IOPS': \"\",\n",
    "                        'Max Network Throughput': \"\",\n",
    "                    }\n",
    "\n",
    "                    feature_vector = { \n",
    "                        'peak_value': peak_value, 'lowest_value': lowest_value, 'mean': mean_value, \n",
    "                        'variance': variance\n",
    "                    }\n",
    "\n",
    "                    # feature_vector = { \n",
    "                    #     'peak_value': peak_value, 'lowest_value': lowest_value, 'mean': mean_value, 'median': median_value, \n",
    "                    #     'variance': variance,'relative_variance': relative_variance, 'std_dev':std_dev, \n",
    "                    #     'pattern_vector': pattern_vector, 'server_spec': server_spec\n",
    "                    # }\n",
    "                    \n",
    "                    container_name = os.path.splitext(file)[0]\n",
    "                    if container_name in container_temporal_signatures:\n",
    "                        if feature_vector is not None and feature_vector != {}:\n",
    "                            # Validation step to account for missing feature values\n",
    "                            expected_keys = ['peak_value', 'lowest_value', 'mean', 'variance']\n",
    "                            missing_values = [key for key in expected_keys if key not in feature_vector or feature_vector[key] is None]\n",
    "                            if missing_values:\n",
    "                                print(f\"Warning: Missing values in feature vector for {container_name} in {metric_name}: {missing_values}\")\n",
    "                            if 'pattern_vector' in feature_vector:\n",
    "                                if not isinstance(feature_vector['pattern_vector'],np.ndarray):\n",
    "                                    print(f\"WARNING: {container_name} {metric_name} pattern_vector shape: {feature_vector['pattern_vector'].shape}\")\n",
    "                            container_temporal_signatures[container_name]['temporal_signatures'][metric_name] = feature_vector\n",
    "    pprint.pprint(container_temporal_signatures)\n",
    "    return container_temporal_signatures\n",
    "\n",
    "container_temporal_signatures = build_container_temporal_signatures_scoped_sources(scoped_results, FIN_CONTAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a7accfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total containers with no signature for any metric: 9\n",
      "Remaining containers after cleaning: 14\n",
      "All metrics found: ['container_weighted_cycles']\n",
      "Keeping 14 containers with all metrics.\n",
      "['nxf-cPB62cVKMj0A2W3ZgiyXeXAy',\n",
      " 'nxf-8HEIDLPLcFSgNV2onUEea8wK',\n",
      " 'nxf-0X0tQJagkeWOAir2jS124FfK',\n",
      " 'nxf-TrD9qyudd3YDIKNfgNkKpu9H',\n",
      " 'nxf-qDilxwaxmY8uJ5TscM5fDPNc',\n",
      " 'nxf-SX1AWI1RbvjBo0PJOwC1FAFw',\n",
      " 'nxf-1AUOV7AhBGVUbCmee5WApTRX',\n",
      " 'nxf-0mUZ0M8vpF30z1CEoXjCQQbH',\n",
      " 'nxf-0pUrbbt0IplTwbj4uE7h1Lv0',\n",
      " 'nxf-UY2XomSHbY5BM00lkqJ3KiSI',\n",
      " 'nxf-bQCEmlIiekPOOtkHpYmKBSn7',\n",
      " 'nxf-6NsMcpYNvIhqIRPUkkVmSPjV',\n",
      " 'nxf-i3k55HVSqlStQlJ9rLveDORE',\n",
      " 'nxf-l4UOQ6vq023FfdVkhpq6uhFB']\n",
      "['container_weighted_cycles']\n",
      "['lowest_value', 'mean', 'peak_value', 'variance']\n"
     ]
    }
   ],
   "source": [
    "def cleanFeatureVectors(cleaned_container_temporal_signatures):\n",
    "    \"\"\"\n",
    "    Clean the feature vectors by removing containers that have no temporal signatures.\n",
    "    This function modifies the input dictionary in place.\n",
    "    \"\"\"\n",
    "\n",
    "    none_counter = 0\n",
    "    to_delete = []\n",
    "    for name, info in cleaned_container_temporal_signatures.items():\n",
    "        if not info['temporal_signatures']:\n",
    "            none_counter += 1\n",
    "            # print(f\"Container {name} has no temporal signatures. Will be deleted.\")\n",
    "            to_delete.append(name)\n",
    "    print(f\"Total containers with no signature for any metric: {none_counter}\")\n",
    "\n",
    "    for name in to_delete:\n",
    "        del cleaned_container_temporal_signatures[name]\n",
    "    # pprint.pprint(container_temporal_signatures)\n",
    "\n",
    "    print(f\"Remaining containers after cleaning: {len(cleaned_container_temporal_signatures)}\")\n",
    "\n",
    "    all_metrics = set()\n",
    "    for info in cleaned_container_temporal_signatures.values():\n",
    "        all_metrics.update(info['temporal_signatures'].keys())\n",
    "    all_metrics = sorted(all_metrics)\n",
    "    print(f\"All metrics found: {all_metrics}\")\n",
    "\n",
    "    all_feature_names = set()\n",
    "    for info in container_temporal_signatures.values():\n",
    "        for metric in info['temporal_signatures'].values():\n",
    "            all_feature_names.update([k for k in metric.keys()])\n",
    "    all_feature_names = sorted(all_feature_names)\n",
    "    \n",
    "    containers_with_all_metrics = []\n",
    "\n",
    "    for container, info in cleaned_container_temporal_signatures.items():\n",
    "        if set(info['temporal_signatures'].keys()) == set(all_metrics):\n",
    "            containers_with_all_metrics.append(container)\n",
    "    print(f\"Keeping {len(containers_with_all_metrics)} containers with all metrics.\")\n",
    "\n",
    "    pprint.pprint(containers_with_all_metrics)\n",
    "    pprint.pprint(all_metrics)\n",
    "    pprint.pprint(all_feature_names)\n",
    "    return containers_with_all_metrics, all_metrics, all_feature_names\n",
    "\n",
    "containers_with_all_metrics, all_metrics, all_feature_names = cleanFeatureVectors(container_temporal_signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93204eb0",
   "metadata": {},
   "source": [
    "## Feature Extraction for Container Metrics\n",
    "\n",
    "### Given\n",
    "\n",
    "- **Containers**  \n",
    "  `C = {c1, c2, , cn}`  \n",
    "  *Example:* `nxf-0X0tQJagkeWOAir2jS124FfK`, `nxf-0mUZ0M8vpF30z1CEoXjCQQbH`, \n",
    "\n",
    "- **Metrics**  \n",
    "  `M = {container_weighted_cycles}`\n",
    "\n",
    "- **Feature Table**\n",
    "\n",
    "| Container Name                | lowest_value        | mean               | peak_value         | variance           |\n",
    "|-------------------------------|---------------------|--------------------|--------------------|--------------------|\n",
    "| nxf-0X0tQJagkeWOAir2jS124FfK  | \\(2.55 \\times 10^{10}\\) | \\(1.93 \\times 10^{11}\\) | \\(1.99 \\times 10^{11}\\) | \\(1.02 \\times 10^{21}\\) |\n",
    "| nxf-0mUZ0M8vpF30z1CEoXjCQQbH  | \\(1.03 \\times 10^{12}\\) | \\(1.03 \\times 10^{12}\\) | \\(1.03 \\times 10^{12}\\) | \\(0\\)                  |\n",
    "\n",
    "- **Features per Metric**  \n",
    "  `F = {lowest_value, mean, peak_value, variance}`\n",
    "\n",
    "### Feature Vector\n",
    "\n",
    "For each container `c_i` and metric `m` in `M`, extract:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i =\n",
    "\\begin{bmatrix}\n",
    "\\text{lowest\\_value}(c_i, m) \\\\\n",
    "\\text{mean}(c_i, m) \\\\\n",
    "\\text{peak\\_value}(c_i, m) \\\\\n",
    "\\text{variance}(c_i, m)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Matrix form\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} \\\\\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3} & x_{2,4} \\\\\\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\vdots  \\\\\\\\\n",
    "x_{n,1} & x_{n,2} & x_{n,3} & x_{n,4}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3f1147d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14, 4)\n",
      "    lowest_value          mean    peak_value      variance\n",
      "0   6.100653e+10  1.220750e+12  1.264241e+12  5.105281e+22\n",
      "1   3.385407e+09  2.182795e+10  2.251101e+10  1.274909e+19\n",
      "2   2.548311e+10  1.926760e+11  1.987191e+11  1.022252e+21\n",
      "3   1.215165e+12  1.215165e+12  1.215165e+12  0.000000e+00\n",
      "4   1.232323e+09  1.004246e+13  1.041900e+13  3.827091e+24\n",
      "5   2.498983e+09  2.057853e+12  2.652161e+12  1.147040e+24\n",
      "6   1.009989e+10  1.009989e+10  1.009989e+10  0.000000e+00\n",
      "7   1.032269e+12  1.032269e+12  1.032269e+12  0.000000e+00\n",
      "8   0.000000e+00  8.162319e+10  7.499951e+11  1.626805e+22\n",
      "9   1.831883e+07  9.752655e+08  2.839785e+10  2.653059e+19\n",
      "10  8.992514e+09  8.992514e+09  8.992514e+09  0.000000e+00\n",
      "11  1.003845e+08  1.003845e+08  1.003845e+08  0.000000e+00\n",
      "12  1.010926e+08  1.010926e+08  1.010926e+08  0.000000e+00\n",
      "13  6.920258e+07  2.064730e+10  1.748806e+11  1.574611e+21\n"
     ]
    }
   ],
   "source": [
    "def buildFeatureMatriceInput(containers_with_all_metrics, all_metrics, all_feature_names):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the containers with all metrics.\n",
    "    Returns the feature matrix and the container names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Those containers who have all metrics (some are not caught by all exporters)\n",
    "    feature_matrix_x = []\n",
    "    container_names_x = []\n",
    "\n",
    "    for container in containers_with_all_metrics:\n",
    "        info = container_temporal_signatures[container]\n",
    "        # pprint.pprint(info['temporal_signatures'])\n",
    "        row = []\n",
    "        for metric in all_metrics:\n",
    "            feats = info['temporal_signatures'].get(metric)\n",
    "            # print(f\"Processing container: {container}, metric: {metric}\")\n",
    "            if feats:\n",
    "                for name in all_feature_names:\n",
    "                    value = feats.get(name)\n",
    "                    if isinstance(value, np.ndarray): # handles the pattern feature being a numpy array\n",
    "                        row.extend(value.tolist())\n",
    "                    else:\n",
    "                        row.append(value)\n",
    "        feature_matrix_x.append(row)\n",
    "        container_names_x.append(container)\n",
    "\n",
    "    # Convert into feature matrix K_x\n",
    "    feature_matrix_x = np.array(feature_matrix_x)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_x.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_x, columns=all_feature_names)\n",
    "    print(df)\n",
    "    return feature_matrix_x, container_names_x\n",
    "\n",
    "feature_matrix_x = buildFeatureMatriceInput(containers_with_all_metrics, all_metrics, all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add power values from one chosen data source to all nextflow files for each data source.\n",
    "# First just add the power values to fin_containers.\n",
    "def addPowerToFinContainers(fin_containers, containers_with_all_metrics, power_stats):\n",
    "    \"\"\"\n",
    "    Add power values to the finished containers file.\n",
    "    \"\"\"\n",
    "    fin_df = pd.read_csv(fin_containers)\n",
    "    power_stat_files = set(f[:-4] for f in os.listdir(power_stats) if f.endswith('.csv'))\n",
    "    # print(power_stat_files)\n",
    "\n",
    "    for container in containers_with_all_metrics:\n",
    "        # print(container)\n",
    "        if container in power_stat_files:\n",
    "            power_df = pd.read_csv(os.path.join(power_stats, f\"{container}.csv\"))\n",
    "            # print(power_df.head())\n",
    "            mean_power = power_df['Value (microjoules)'].mean() if 'Value (microjoules)' in power_df.columns else None\n",
    "            fin_df.loc[fin_df['Name'] == container, 'MeanPower'] = mean_power\n",
    "    fin_df.to_csv(fin_containers, index=False)\n",
    "    return fin_df\n",
    "\n",
    "fin_df = addPowerToFinContainers(FIN_CONTAINERS, containers_with_all_metrics, POWER_STATS)\n",
    "# pprint.pprint(fin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72877ef",
   "metadata": {},
   "source": [
    "## Feature Extraction for Container Runtime and Power\n",
    "\n",
    "### Given\n",
    "\n",
    "- **Containers**  \n",
    "  `C = {c1, c2, , cn}`  \n",
    "  *Example:* `nxf-0X0tQJagkeWOAir2jS124FfK`, `nxf-0mUZ0M8vpF30z1CEoXjCQQbH`, \n",
    "\n",
    "- **Metrics**  \n",
    "  `M = {runtime, power}`\n",
    "\n",
    "- **Feature Table**\n",
    "\n",
    "| Container Name                | runtime (s) | power (J)      |\n",
    "|-------------------------------|-------------|-----------------|\n",
    "| nxf-0X0tQJagkeWOAir2jS124FfK  | 123.4       | 1.23e+09        |\n",
    "| nxf-0mUZ0M8vpF30z1CEoXjCQQbH  | 98.7        | 2.34e+09        |\n",
    "\n",
    "- **Features per Container**  \n",
    "  `F = {runtime, power}`\n",
    "\n",
    "### Feature Vector\n",
    "\n",
    "For each container `c_i`, extract:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_i =\n",
    "\\begin{bmatrix}\n",
    "\\text{runtime}(c_i) \\\\\n",
    "\\text{power}(c_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Matrix form\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y_{1,1} & y_{1,2} \\\\\\\\\n",
    "y_{2,1} & y_{2,2} \\\\\\\\\n",
    "\\vdots  & \\vdots  \\\\\\\\\n",
    "y_{n,1} & y_{n,2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each row corresponds to a container, and the columns are:\n",
    "- `runtime`: execution time in seconds\n",
    "- `power`: mean power consumption in microjoules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "31ea0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14, 2)\n",
      "      runtime        power\n",
      "0    3.291482   683.055173\n",
      "1    3.624712    12.204298\n",
      "2    4.641757   107.747945\n",
      "3    2.615504   588.790631\n",
      "4    2.757030  1566.518561\n",
      "5   16.219254    80.176704\n",
      "6    1.906432     1.850064\n",
      "7    4.927431   189.087464\n",
      "8    4.828914    21.191534\n",
      "9    6.161708     0.297813\n",
      "10   1.252003     0.776517\n",
      "11   1.344221     0.008668\n",
      "12   1.494906     0.008729\n",
      "13  19.257875     1.580281\n"
     ]
    }
   ],
   "source": [
    "def buildFeatureMatriceOutput(fin_df):\n",
    "    \"\"\"\n",
    "    Build the feature matrices for the finished containers.\n",
    "    Returns the feature matrix and the container names only for containers with available power values.\n",
    "    \"\"\"\n",
    "    container_runtime_power = {}\n",
    "\n",
    "    fin_df['LifeTime_s'] = (\n",
    "        fin_df['LifeTime']\n",
    "        .str.extract(r'([0-9.]+)(ms|s)', expand=True)\n",
    "        .assign(\n",
    "            value=lambda x: x[0].astype(float),\n",
    "            seconds=lambda x: np.where(x[1] == 'ms', x['value'] / 1000, x['value'])\n",
    "        )['seconds']\n",
    "    )\n",
    "\n",
    "    for idx, row in fin_df.iterrows():\n",
    "        container_runtime_power[row['Name']] = {\n",
    "            'runtime': row['LifeTime_s'],\n",
    "            'power': row['MeanPower']\n",
    "        }\n",
    "        \n",
    "    feature_matrix_y = []\n",
    "    container_names_y = []\n",
    "\n",
    "    for container, info in container_runtime_power.items():\n",
    "        if container not in container_temporal_signatures:\n",
    "            continue\n",
    "        if pd.notna(info['runtime']) and pd.notna(info['power']):\n",
    "            feature_matrix_y.append([info['runtime'], info['power']])\n",
    "            container_names_y.append(container)\n",
    "            \n",
    "    # Transform feature matrix K_y into numpy array\n",
    "    feature_matrix_y = np.array(feature_matrix_y)\n",
    "    print(f\"Feature matrix shape: {feature_matrix_y.shape}\")\n",
    "    df = pd.DataFrame(feature_matrix_y, columns=['runtime', 'power'])\n",
    "    print(df)\n",
    "\n",
    "    return feature_matrix_y, container_names_y\n",
    "\n",
    "finished_containers_dfs_with_power = addPowerToFinContainers(FIN_CONTAINERS, containers_with_all_metrics,POWER_STATS) \n",
    "feature_matrix_y = buildFeatureMatriceOutput(finished_containers_dfs_with_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1d26ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X names: ['nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc']\n",
      "Y names: ['nxf-cPB62cVKMj0A2W3ZgiyXeXAy', 'nxf-8HEIDLPLcFSgNV2onUEea8wK', 'nxf-0X0tQJagkeWOAir2jS124FfK', 'nxf-TrD9qyudd3YDIKNfgNkKpu9H', 'nxf-qDilxwaxmY8uJ5TscM5fDPNc']\n",
      "Length X: 14\n",
      "Length Y: 14\n",
      "All X in Y: True\n",
      "All Y in X: True\n",
      "Order identical: True\n"
     ]
    }
   ],
   "source": [
    "# Debugging output to check if the container names in X and Y match and order is the same.\n",
    "container_names_x = feature_matrix_x[1]\n",
    "container_names_y = feature_matrix_y[1]\n",
    "print(\"X names:\", container_names_x[:5])\n",
    "print(\"Y names:\", container_names_y[:5])\n",
    "print(\"Length X:\", len(container_names_x))\n",
    "print(\"Length Y:\", len(container_names_y))\n",
    "print(\"All X in Y:\", all(name in container_names_y for name in container_names_x))\n",
    "print(\"All Y in X:\", all(name in container_names_x for name in container_names_y))\n",
    "print(\"Order identical:\", container_names_x == container_names_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bfe3e",
   "metadata": {},
   "source": [
    "## Z-Score Transformation and Standard Scaling performed on Feature and Label Matrices\n",
    "\n",
    "**Standard scaling** (also known as z-score normalization) is a technique used to standardize the features of a dataset so that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The standard score (z-score) for a value \\( x \\) is calculated as:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the original value,\n",
    "- \\( \\mu \\) is the mean of the training samples,\n",
    "- \\( \\sigma \\) is the standard deviation of the training samples.\n",
    "\n",
    "### How StandardScaler Works\n",
    "\n",
    "- **Centering**: Subtracts the mean value of each feature so that the feature is centered around zero.\n",
    "- **Scaling**: Divides each centered feature by its standard deviation so that the resulting distribution has unit variance.\n",
    "\n",
    "This transformation is performed **independently for each feature**.\n",
    "- Many machine learning algorithms assume that all features are centered around zero and have the same scale.\n",
    "- Features with larger scales can dominate the objective function and negatively impact model performance.\n",
    "- Standard scaling ensures that each feature contributes equally to the model.\n",
    "- StandardScaler is sensitive to outliers: extreme values can affect the mean and standard deviation, leading to less robust scaling.\n",
    "- For sparse data, one can disable mean centering to preserve sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ddfcb6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature matrix X shape: (14, 4)\n",
      "Scaled feature matrix Y shape: (14, 2)\n"
     ]
    }
   ],
   "source": [
    "def scaleFeatureMatrices(feature_matrix_x, feature_matrix_y):\n",
    "    \"\"\"\n",
    "    Scale the feature matrices using StandardScaler.\n",
    "    Returns the scaled feature matrices.\n",
    "    \"\"\"\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    scaled_x = scaler_x.fit_transform(feature_matrix_x)\n",
    "    scaled_y = scaler_y.fit_transform(feature_matrix_y)\n",
    "\n",
    "    print(f\"Scaled feature matrix X shape: {scaled_x.shape}\")\n",
    "    print(f\"Scaled feature matrix Y shape: {scaled_y.shape}\")\n",
    "    \n",
    "    return scaled_x, scaled_y, scaler_x, scaler_y\n",
    "\n",
    "scaled_feature_matrix_x, scaled_feature_matrix_y, scaler_x, scaler_y = scaleFeatureMatrices(feature_matrix_x[0], feature_matrix_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad0382",
   "metadata": {},
   "source": [
    "## Train-Test Split Procedure\n",
    "\n",
    "To evaluate machine learning models, it is common practice to split the available data into **training** and **testing** subsets. This ensures that model evaluation is performed on data not seen during training, providing an unbiased estimate of model performance.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The `train_test_split` function from scikit-learn is a utility that splits arrays or matrices into random train and test subsets. It shuffles and splits the data in a single call.\n",
    "\n",
    "**Parameters:**\n",
    "- **arrays**: Input data to split (e.g., numpy arrays, pandas DataFrames, lists). All arrays must have the same length.\n",
    "- **test_size**: Proportion (float between 0.0 and 1.0) or absolute number (int) of samples to include in the test split. Default is 0.25 if not specified.\n",
    "- **train_size**: Proportion or absolute number of samples to include in the train split. If None, set to the complement of `test_size`.\n",
    "- **random_state**: Controls the shuffling applied to the data before splitting. Setting an integer ensures reproducibility.\n",
    "- **shuffle**: Whether to shuffle the data before splitting (default: True).\n",
    "- **stratify**: If not None, data is split in a stratified fashion using this as class labels.\n",
    "\n",
    "**Returns:**\n",
    "- The function returns lists or arrays containing the train-test split of the inputs, preserving the input type (e.g., numpy array, pandas DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8a6f5e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (11, 4), Test set shape: (3, 4)\n",
      "y_test DataFrame:     runtime       power\n",
      "0  6.161708    0.297813\n",
      "1  1.344221    0.008668\n",
      "2  3.291482  683.055173\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "# Check the values of the test data set against the kcca predictions before z-score normalization.\n",
    "def splitFeatureMatrices(feature_matrix_x, feature_matrix_y, container_names_x, container_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = train_test_split(\n",
    "        feature_matrix_x, feature_matrix_y, container_names_x, container_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = splitFeatureMatrices(feature_matrix_x[0], feature_matrix_y[0], feature_matrix_x[1], feature_matrix_y[1])\n",
    "\n",
    "x_train_df = pd.DataFrame(X_train, columns=all_feature_names)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"X_train DataFrame:\", x_train_df)\n",
    "x_test_df = pd.DataFrame(X_test, columns=all_feature_names)\n",
    "# print(\"X_test DataFrame:\", x_test_df)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"y_train DataFrame:\", y_train_df)\n",
    "y_test_df = pd.DataFrame(y_test, columns=['runtime', 'power'])\n",
    "print(\"y_test DataFrame:\", y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1685ad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (11, 4), Test set shape: (3, 4)\n",
      "y_test DataFrame:     runtime     power\n",
      "0  0.160748 -0.542133\n",
      "1 -0.747207 -0.542809\n",
      "2 -0.380205  1.052769\n"
     ]
    }
   ],
   "source": [
    "def splitFeatureMatrices(feature_matrix_x, feature_matrix_y, container_names_x, container_names_y):\n",
    "    \"\"\"\n",
    "    Split the feature matrices into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = train_test_split(\n",
    "        feature_matrix_x, feature_matrix_y, container_names_x, container_names_y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_container_names_x, test_container_names_x, train_container_names_y, test_container_names_y = splitFeatureMatrices(scaled_feature_matrix_x, scaled_feature_matrix_y, feature_matrix_x[1], feature_matrix_y[1])\n",
    "\n",
    "x_train_df = pd.DataFrame(X_train, columns=all_feature_names)\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"X_train DataFrame:\", x_train_df)\n",
    "x_test_df = pd.DataFrame(X_test, columns=all_feature_names)\n",
    "# print(\"X_test DataFrame:\", x_test_df)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=['runtime', 'power'])\n",
    "# print(\"y_train DataFrame:\", y_train_df)\n",
    "y_test_df = pd.DataFrame(y_test, columns=['runtime', 'power'])\n",
    "print(\"y_test DataFrame:\", y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b3828f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_matrix_x std: 0.9999999999999999\n",
      "feature_matrix_y std: 1.0000000000000002\n",
      "X_train (features) std: 1.107782946169594\n",
      "Y_train (features) std: 1.0772242164070853\n",
      "X_test (features) std: 0.16666988881367265\n",
      "Y_test (features) std: 0.6136376991171003\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "# Check variance of original feature matrices before kernelization\n",
    "print(\"feature_matrix_x std:\", np.std(scaled_feature_matrix_x))\n",
    "print(\"feature_matrix_y std:\", np.std(scaled_feature_matrix_y))\n",
    "\n",
    "# After splitting\n",
    "print(\"X_train (features) std:\", np.std(X_train))\n",
    "print(\"Y_train (features) std:\", np.std(y_train))\n",
    "print(\"X_test (features) std:\", np.std(X_test))\n",
    "print(\"Y_test (features) std:\", np.std(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KCCA Model kernalizes the normalized input matrices itself.\n",
    "# def computeKernelMatrices(X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"\n",
    "#     Compute the RBF kernel matrices for train and test splits.\n",
    "#     Returns: K_x_train, K_x_test, K_y_train, K_y_test\n",
    "#     \"\"\"\n",
    "#     K_x_train = rbf_kernel(X_train, X_train)\n",
    "#     K_y_train = rbf_kernel(y_train, y_train)\n",
    "#     K_x_test = rbf_kernel(X_test, X_train)\n",
    "#     K_y_test = rbf_kernel(y_test, y_train)\n",
    "#     print(f\"K_x_train shape: {K_x_train.shape}, K_x_test shape: {K_x_test.shape}\")\n",
    "#     print(f\"K_y_train shape: {K_y_train.shape}, K_y_test shape: {K_y_test.shape}\")\n",
    "#     return K_x_train, K_x_test, K_y_train, K_y_test\n",
    "\n",
    "# K_x_train, K_x_test, K_y_train, K_y_test = computeKernelMatrices(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda0c12",
   "metadata": {},
   "source": [
    "## Kernel Canonical Correlation Analysis (KCCA) Overview\n",
    "\n",
    "The **KCCA algorithm** takes the kernel matrices \\( K_x \\) and \\( K_y \\) and solves a generalized eigenvector problem. This procedure finds subspaces in the linear space spanned by the eigenfunctions of the kernel functions such that projections onto these subspaces are **maximally correlated** [7].\n",
    "\n",
    "- We refer to these projections as the **resource usage projection** and the **metric projection**, respectively.\n",
    "- If the linear space associated with the Gaussian (RBF) kernel can be interpreted as clusters in the original feature space, then KCCA finds **correlated pairs of clusters** in the resource usage vector space and the performance/power vector space.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Compute kernel matrices** \\( K_x \\) and \\( K_y \\) for the resource and metric features.\n",
    "2. **Fit KCCA** using the training data kernel matrices.\n",
    "3. **Project data** into the maximally correlated subspaces for further analysis or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98851540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMCCA(kernel=&#x27;rbf&#x27;, n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMCCA</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_components',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_components&nbsp;</td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;rbf&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel_params',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel_params&nbsp;</td>\n",
       "            <td class=\"value\">{}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('regs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">regs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('signal_ranks',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">signal_ranks&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sval_thresh',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sval_thresh&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('diag_mode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">diag_mode&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;A&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('center',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">center&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('filter_params',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">filter_params&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multiview_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multiview_output&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pgso',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">pgso&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "KMCCA(kernel='rbf', n_components=2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmcca = KMCCA(kernel='rbf', n_components=2)\n",
    "kmcca.fit([X_train, y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a4b46",
   "metadata": {},
   "source": [
    "## Kernel Canonical Correlation Analysis (KMCCA)\n",
    "\n",
    "Traditional Canonical Correlation Analysis (CCA) aims to find useful projections of features in each view of data by computing a weighted sum. However, due to its linearity, CCA may not extract meaningful descriptors of complex data.\n",
    "\n",
    "Kernel MCCA (KMCCA) addresses this limitation by first projecting the data into a higher-dimensional feature space **before** performing CCA in that new space.\n",
    "\n",
    "### The Kernel Trick\n",
    "\n",
    "Kernels are effectively distance functions that compute inner products in the higher-dimensional feature space. This approach is known as the **kernel trick**. A kernel function \\( K \\) is used such that, for all data points \\( x, x' \\):\n",
    "\n",
    "\\[\n",
    "K(x, x') = \\langle \\phi(x), \\phi(x') \\rangle\n",
    "\\]\n",
    "\n",
    "where \\( \\phi \\) is the mapping to the feature space.\n",
    "\n",
    "### Kernel Matrix\n",
    "\n",
    "The **kernel matrix** \\( K \\) has entries \\( K_{ij} = K(x_i, x_j) \\) computed from the kernel function. Using the kernel trick, KMCCA solves for the **loadings of the kernel matrix** (denoted as `dual_vars_`) rather than directly solving for the original feature loadings.\n",
    "\n",
    "However, kernel matrices grow **quadratically** with the number of data points. Not only must \\( n^2 \\) elements be stored, but solving kernel eigenvalue problems becomes computationally expensive.\n",
    "\n",
    "### Cholesky Decomposition\n",
    "\n",
    "For a positive definite matrix \\( K \\), Cholesky decomposition expresses it as:\n",
    "\n",
    "\\[\n",
    "K = LL^\\top\n",
    "\\]\n",
    "\n",
    "where \\( L \\) is a lower triangular matrix.\n",
    "\n",
    "### Partial Gram-Schmidt Orthogonalization (PSGO)\n",
    "\n",
    "The **dual partial Gram-Schmidt orthogonalization (PSGO)** is equivalent to the **Incomplete Cholesky Decomposition (ICD)**, which seeks a **low-rank approximation** of \\( K \\), reducing computational cost.\n",
    "\n",
    "This approximation satisfies:\n",
    "\n",
    "\\[\n",
    "K \\approx GG^\\top\n",
    "\\]\n",
    "\n",
    "A PSGO tolerance yielding rank \\( r \\) leads to:\n",
    "- **Storage**: \\( \\mathcal{O}(nr) \\) instead of \\( \\mathcal{O}(n^2) \\)\n",
    "- **Computation**: \\( \\mathcal{O}(nr^2) \\) instead of \\( \\mathcal{O}(n^3) \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9fabaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project training and test data\n",
    "X_train_proj, Y_train_proj = kmcca.transform([X_train, y_train])\n",
    "X_test_proj, Y_test_proj = kmcca.transform([X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6a4361f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_proj shape: (11, 2)\n",
      "Y_train_proj shape: (11, 2)\n",
      "X_test_proj shape: (3, 2)\n",
      "Y_test_proj shape: (3, 2)\n",
      "X_train_proj mean/std: 3.658689512969266e-17 0.2132007163556105\n",
      "Y_train_proj mean/std: 2.7755575615628914e-17 0.21320071635561041\n",
      "First 3 rows of X_train_proj:\n",
      " [[-0.02583869 -0.11902035]\n",
      " [-0.31827816  0.57404992]\n",
      " [-0.03416744 -0.07056236]]\n",
      "First 3 rows of Y_train_proj:\n",
      " [[-0.02583869 -0.12117876]\n",
      " [-0.31827816  0.57402738]\n",
      " [-0.03416744 -0.07055146]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the projections\n",
    "print(\"X_train_proj shape:\", X_train_proj.shape)\n",
    "print(\"Y_train_proj shape:\", Y_train_proj.shape)\n",
    "print(\"X_test_proj shape:\", X_test_proj.shape)\n",
    "print(\"Y_test_proj shape:\", Y_test_proj.shape)\n",
    "\n",
    "print(\"X_train_proj mean/std:\", np.mean(X_train_proj), np.std(X_train_proj))\n",
    "print(\"Y_train_proj mean/std:\", np.mean(Y_train_proj), np.std(Y_train_proj))\n",
    "print(\"First 3 rows of X_train_proj:\\n\", X_train_proj[:3])\n",
    "print(\"First 3 rows of Y_train_proj:\\n\", Y_train_proj[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient between projections: 0.534\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "# Evaluate the correlation between the projections for test data\n",
    "corr, _ = pearsonr(X_test_proj.ravel(), Y_test_proj.ravel())\n",
    "print(f\"Pearson correlation coefficient between projections: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9062b3",
   "metadata": {},
   "source": [
    "## Predicting Power Consumption and Execution Time Using KCCA and Nearest Neighbors\n",
    "\n",
    "The **consumed power** and **execution time** for a one-to-one mapping of clusters to servers can be estimated using a KCCA model trained offline. The process is as follows:\n",
    "\n",
    "1. **Projection into Resource Subspace:**  \n",
    "   The input vector, which includes the temporal signature of the resource usage profile (and optionally server capacity), is projected into the resource subspace learned by KCCA.\n",
    "\n",
    "2. **Finding Nearest Neighbors:**  \n",
    "   For each test sample's resource projection (`X_test_proj`), find its *k* nearest neighbors among the training projections (`X_train_proj`).  \n",
    "   - This is typically done using Euclidean distance in the projected subspace.\n",
    "   - In our implementation, we use `k = 3`.\n",
    "\n",
    "3. **Inferring Metric Projections:**  \n",
    "   For each test sample, collect the metric projections (`Y_train_proj`) of its *k* nearest neighbors.\n",
    "\n",
    "4. **Weighted Sum for Prediction:**  \n",
    "   Compute a weighted sum of these metric projections, where the weight for each neighbor is the inverse of its distance to the test sample (closer neighbors have more influence).\n",
    "\n",
    "5. **Mapping Back to Original Metric Space:**  \n",
    "   The weighted sum gives an estimated metric projection for the test sample.  \n",
    "   - If your metrics were scaled, use the scaler's `inverse_transform` to convert the projection back to the original units (e.g., actual power and time).\n",
    "\n",
    "6. **Selecting the Optimal Point (Optional):**  \n",
    "   The optimal point of this iteration, with the minimum total power consumption, can be recorded for further analysis or scheduling.\n",
    "\n",
    "---\n",
    "- **KCCA** finds maximally correlated subspaces between resource usage and metrics, capturing nonlinear relationships.\n",
    "- By using nearest neighbors in the resource subspace, you leverage the learned relationship to predict metrics for new, unseen resource profiles.\n",
    "- The weighted sum ensures that predictions are more influenced by similar (closer) training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94b508",
   "metadata": {},
   "source": [
    "## Estimating Power and Execution Time via KCCA\n",
    "\n",
    "The consumed power and execution time for the **one-to-one mapping** of clusters to servers can be estimated using a **KCCA model** trained offline.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- The **input vector**  consisting of the **temporal signature** of the resource usage profile and the **server capacity**  is projected into the **resource subspace**.\n",
    "- The corresponding coordinates in the **metric subspace** are inferred using **k-nearest neighbors** (**\\( k = 3 \\)** in our implementation).\n",
    "- The **metric projection** is then mapped back to the original **metrics**:\n",
    "  - **Consumed Power**\n",
    "  - **Execution Time**\n",
    "\n",
    "A **weighted sum** of the metric projections from the \\( k \\) nearest neighbors is computed. The **weight** is defined as the **inverse of the distance** between projections in the subspace.\n",
    "\n",
    "The **optimal point**  the configuration with the **minimum total power consumption**  is recorded for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ba6b0",
   "metadata": {},
   "source": [
    "Then, temporal signature of the new cluster is updated from the consolidated workloads. Such consolidation iterations stop when the clusters cannot be merged anymore since merging will incur significant interference, and/or the degradation in application performance will be intolerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fc058598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "Predicted metrics for test data:\n",
      "    runtime       power\n",
      "0  5.169886  181.488593\n",
      "1  5.171704  180.503788\n",
      "2  5.006494  234.112882\n"
     ]
    }
   ],
   "source": [
    "# Starting point for unseen data used with KCCA model.\n",
    "def predictKCCAUnseen(X_train_proj, Y_train_proj, X_test_proj, scaler_y, k=3):\n",
    "    \"\"\"\n",
    "    Based on the projections of the trained Input features\n",
    "    this func uses the unsupervised nearest neighbours algorithm\n",
    "    to find the nearest points from the unseen points to the training data\n",
    "    points. When the k-nearest neighbours are found a weighted average\n",
    "    denotes the prediction for the unseen data.\n",
    "    This function assumes that the unseen data is already scaled and prepared.\n",
    "    \n",
    "    Args:\n",
    "        X_train_proj: Projected training resource features (n_train, n_components)\n",
    "        Y_train_proj: Projected training metric features (n_train, n_components)\n",
    "        X_test_proj: Projected test resource features (n_test, n_components)\n",
    "        scaler_y: Fitted StandardScaler for the metric space (for inverse_transform)\n",
    "        k: Number of nearest neighbors to use (default: 3)\n",
    "    Returns:\n",
    "        Y_pred: Predicted metrics (runtime, power) in original units for test data (n_test, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "    # Fit the model on the training projections\n",
    "    nn.fit(X_train_proj)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X_test_proj)\n",
    "    \n",
    "    Y_pred_proj = []\n",
    "    for dists, idxs in zip(distances, indices):\n",
    "        weights = 1 / (dists + 1e-8)  # Avoid division by zero\n",
    "        weights /= weights.sum()\n",
    "        y_pred = np.average(Y_train_proj[idxs], axis=0, weights=weights)\n",
    "        Y_pred_proj.append(y_pred)\n",
    "    Y_pred_proj = np.array(Y_pred_proj)\n",
    "    Y_pred = scaler_y.inverse_transform(Y_pred_proj)\n",
    "    print(Y_pred.shape)\n",
    "    return Y_pred\n",
    "\n",
    "# Call func on test data projections\n",
    "Y_pred = predictKCCAUnseen(X_train_proj, Y_train_proj, X_test_proj, scaler_y, k=3)\n",
    "df = pd.DataFrame(Y_pred, columns=['runtime', 'power'])\n",
    "print(\"Predicted metrics for test data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c2399",
   "metadata": {},
   "source": [
    "Actual test values I think the predictions are off and somewhat similar to each other due to lack of enough training data points.\n",
    "\n",
    "Training set shape: (11, 4), Test set shape: (3, 4)\n",
    "y_test DataFrame:     runtime       power\n",
    "0  6.161708    0.297813\n",
    "1  1.344221    0.008668\n",
    "2  3.291482  683.055173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e60d6",
   "metadata": {},
   "source": [
    "## Prediction Procedure Using k-Nearest Neighbors in Projected Space\n",
    "\n",
    "| Step | Purpose |\n",
    "|------|---------|\n",
    "| `nn.kneighbors(X_test_proj)` | Find \\(k\\) nearest neighbors for each test sample in the projected resource space |\n",
    "| Loop over test samples | For each test sample, perform the following steps: |\n",
    "| `weights = 1 / (dists + 1e-8)` | Compute inverse-distance weights |\n",
    "| `weights /= weights.sum()` | Normalize weights so they sum to 1 |\n",
    "| `np.average(Y_train_proj[idxs], axis=0, weights=weights)` | Compute the weighted average of neighbors' metric projections |\n",
    "| `Y_pred_proj.append(y_pred)` | Collect the prediction for this test sample |\n",
    "| `np.array(Y_pred_proj)` | Stack all predictions into a single matrix |\n",
    "| `scaler_y.inverse_transform(Y_pred_proj)` | Convert predictions back to original metric units |\n",
    "| `return Y_pred` | Output the final predictions |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharecomp-bB4WWry4-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
